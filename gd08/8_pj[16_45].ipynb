{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리\n",
    "- json 파싱\n",
    "- TFRecord 파일 생성 \n",
    "- Data Label로 만들기\n",
    "모델 구성\n",
    "- Hourglass Model\n",
    "- SimpleBaseline Model\n",
    "학습 엔진 구성\n",
    "모델 학습\n",
    "- Hourglass Model\n",
    "- SimpleBaseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "8.3.2\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(PIL.__version__)\n",
    "print(ray.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "MY_MODEL_PATH = os.path.join(PROJECT_PATH, 'my_models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[json 파싱]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# json 파일 구성 파악\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    # 좀 더 명확하게\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- __joints 순서__\n",
    "\n",
    "0 - 오른쪽 발목  \n",
    "1 - 오른쪽 무릎  \n",
    "2 - 오른쪽 엉덩이  \n",
    "3 - 왼쪽 엉덩이  \n",
    "4 - 왼쪽 무릎  \n",
    "5 - 왼쪽 발목  \n",
    "6 - 골반  \n",
    "7 - 가슴(흉부)  \n",
    "8 - 목  \n",
    "9 - 머리 위  \n",
    "10 - 오른쪽 손목  \n",
    "11 - 오른쪽 팔꿈치  \n",
    "12 - 오른쪽 어깨  \n",
    "13 - 왼쪽 어깨  \n",
    "14 - 왼쪽 팔꿈치  \n",
    "15 - 왼쪽 손목  \n",
    "    \n",
    "scale, center : 사람 몸의 크기와 중심점  \n",
    "scale은 200을 곱해야 온전한 크기가 됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation을 파싱\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "# parse_one_annotation() 테스트\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TFRecord 파일 생성]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 annotation -> TFRecord\n",
    "# TFRecord : tf.train.Example들의 합\n",
    "# 하나의 annotation을 tf.train.Example로 만들어주는 함수\n",
    "def generate_tfexample(anno):\n",
    "    \n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼마나 많은 TFRecord를 만들지 결정할 함수\n",
    "# 전체 데이터를 몇 개의 그룹으로 나눌지 결정\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chunkify() 테스트\n",
    "# test_chunks = chunkify([0] * 1000, 64)\n",
    "# print(test_chunks)\n",
    "# print(len(test_chunks))\n",
    "# print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 chunk를 TFRecord로 만드는 함수\n",
    "@ray.remote # 병렬 처리. ray 사용 시 함수 호출 문법이 약간 다름에 주의\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 -> TFRecord\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "# ray.init()\n",
    "\n",
    "# print('Start to parse annotations.')\n",
    "# if not os.path.exists(TFRECORD_PATH):\n",
    "#     os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "# with open(TRAIN_JSON) as train_json:\n",
    "#     train_annos = json.load(train_json)\n",
    "#     train_annotations = [\n",
    "#         parse_one_annotation(anno, IMAGE_PATH)\n",
    "#         for anno in train_annos\n",
    "#     ]\n",
    "#     print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "# with open(VALID_JSON) as val_json:\n",
    "#     val_annos = json.load(val_json)\n",
    "#     val_annotations = [\n",
    "#         parse_one_annotation(anno, IMAGE_PATH) \n",
    "#         for anno in val_annos\n",
    "#     ]\n",
    "#     print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "# print('Start to build TF Records.')\n",
    "# build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "# build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "# print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "#     len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Data Label로 만들기]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TFRecord로 저장된 데이터를 모델 학습에 필요한 데이터로 변환\n",
    "# # TFRecord : 직렬화된 데이터 << 만들 때 순서, 읽어올 때 순서 같아야 함. 데이터 형식 동일해야만.\n",
    "# def parse_tfexample(example):\n",
    "#     image_feature_description = {\n",
    "# #         'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "# #         'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "# #         'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "# #         'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "# #         'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "# #         'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "# #         'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "# #         'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "#         'image/height': tf.io.FixedLenFeature([], tf.float32),\n",
    "#         'image/width': tf.io.FixedLenFeature([], tf.float32),\n",
    "#         'image/depth': tf.io.FixedLenFeature([], tf.float32),\n",
    "#         'image/object/parts/x': tf.io.VarLenFeature(tf.float32),\n",
    "#         'image/object/parts/y': tf.io.VarLenFeature(tf.float32),\n",
    "#         'image/object/parts/v': tf.io.VarLenFeature(tf.float32),\n",
    "#         'image/object/center/x': tf.io.FixedLenFeature([], tf.float32),\n",
    "#         'image/object/center/y': tf.io.FixedLenFeature([], tf.float32),        \n",
    "        \n",
    "#         'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "#         'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "#         'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "#     }\n",
    "#     return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_roi(image, features, margin=0.2):\n",
    "#     img_shape = tf.shape(image)\n",
    "#     img_height = img_shape[0]\n",
    "#     img_width = img_shape[1]\n",
    "#     img_depth = img_shape[2]\n",
    "\n",
    "# #     keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "# #     keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "#     keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.float32)\n",
    "#     keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.float32)\n",
    "#     center_x = features['image/object/center/x']\n",
    "#     center_y = features['image/object/center/y']\n",
    "#     body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "#     # keypoint 중 유효한값(visible = 1) 만 사용\n",
    "#     masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "#     masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "#     # min, max 값\n",
    "#     keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "#     keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "#     keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "#     keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "#     # 높이 값을 이용해서 x, y 위치를 재조정. 박스를 정사각형으로 사용하기 위해 다음과 같이 설정.\n",
    "# #     xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "# #     xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "# #     ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "# #     ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "#     xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.float32)\n",
    "#     xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.float32)\n",
    "#     ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.float32)\n",
    "#     ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.float32)\n",
    "\n",
    "#     # 이미지 크기를 벗어나는 점을 재조정\n",
    "#     effective_xmin = xmin if xmin > 0 else 0\n",
    "#     effective_ymin = ymin if ymin > 0 else 0\n",
    "#     effective_xmax = xmax if xmax < img_width else img_width\n",
    "#     effective_ymax = ymax if ymax < img_height else img_height\n",
    "#     effective_height = effective_ymax - effective_ymin\n",
    "#     effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "#     image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "#     new_shape = tf.shape(image)\n",
    "#     new_height = new_shape[0]\n",
    "#     new_width = new_shape[1]\n",
    "\n",
    "#     effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "#     effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "#     return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경\n",
    "# def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "#     heatmap = tf.zeros((height, width))\n",
    "\n",
    "#     xmin = x0 - 3 * sigma\n",
    "#     ymin = y0 - 3 * sigma\n",
    "#     xmax = x0 + 3 * sigma\n",
    "#     ymax = y0 + 3 * sigma\n",
    "    \n",
    "#     if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "#         return heatmap\n",
    "\n",
    "#     size = 6 * sigma + 1\n",
    "#     x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "#     center_x = size // 2\n",
    "#     center_y = size // 2\n",
    "\n",
    "#     gaussian_patch = tf.cast(tf.math.exp(\n",
    "#         -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "#                              dtype=tf.float32)\n",
    "\n",
    "#     patch_xmin = tf.math.maximum(0, -xmin)\n",
    "#     patch_ymin = tf.math.maximum(0, -ymin)\n",
    "#     patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "#     patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "#     heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "#     heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "#     heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "#     heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "# #     indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "#     indices = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "#     updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "#     count = 0\n",
    "\n",
    "#     for j in tf.range(patch_ymin, patch_ymax):\n",
    "#         for i in tf.range(patch_xmin, patch_xmax):\n",
    "#             indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "#             updates = updates.write(count, gaussian_patch[j][i])\n",
    "#             count += 1\n",
    "\n",
    "#     heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "#     return heatmap\n",
    "\n",
    "# def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "#     v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "# #     x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "# #     y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "#     x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.float32)\n",
    "#     y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.float32)\n",
    "\n",
    "#     num_heatmap = heatmap_shape[2]\n",
    "#     heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "#     for i in range(num_heatmap):\n",
    "#         gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "#         heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "#     heatmaps = heatmap_array.stack()\n",
    "#     heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "#     return heatmaps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "#   정사각형으로 잘라서 사용.\n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        # min, max 값을 찾습니다.\n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Hourglass Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[SimpleBaseline Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deconv module\n",
    "# upconv1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "# bn1 = tf.keras.layers.BatchNormalization()\n",
    "# relu1 = tf.keras.layers.ReLU()\n",
    "# upconv2 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "# bn2 = tf.keras.layers.BatchNormalization()\n",
    "# relu2 = tf.keras.layers.ReLU()\n",
    "# upconv3 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "# bn3 = tf.keras.layers.BatchNormalization()\n",
    "# relu3 = tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deconv module 중복 제거\n",
    "# def _make_deconv_layer(num_deconv_layers):\n",
    "#     seq_model = tf.keras.models.Sequential()\n",
    "#     for i in range(num_deconv_layers):\n",
    "#         seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "#         seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "#         seq_model.add(tf.keras.layers.ReLU())\n",
    "#     return seq_model\n",
    "\n",
    "# upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(256, 256, 3))\n",
    "# x = resnet(inputs)\n",
    "# x = upconv(x)\n",
    "# out = final_layer(x)\n",
    "# simplebaseline_model = keras.Model(inputs, out)\n",
    "\n",
    "# simplebaseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')(inputs)\n",
    "    x = _make_deconv_layer(3)(x)\n",
    "    out = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = Simplebaseline(input_shape=(256, 256, 3))\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **학습 엔진 구성**   \n",
    "**- GPU가 여러 개인 환경**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 is_baseline):\n",
    "        \n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "    \n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "        self.is_baseline = is_baseline\n",
    "    \n",
    "    # learning rate를 점점 lr를 낮춘다.\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "    \n",
    "    # epoch 25,50,75일 때 lr를 줄인다.\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "        \n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "        if self.is_baseline:\n",
    "            loss = tf.math.reduce_mean(\n",
    "                tf.math.square(labels - outputs) * weights) * (\n",
    "                    1.0 / self.global_batch_size)\n",
    "        else:\n",
    "            for output in outputs:\n",
    "                loss += tf.math.reduce_mean(\n",
    "                    tf.math.square(labels - output) * weights) * (\n",
    "                        1.0 / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "    \n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        \n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            print('train time : {}'.format(time.time()-start_time))\n",
    "            \n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            \n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MY_MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "# dataset 생성\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "    \n",
    "    # tf.data.Dataset.list_files() : TFRecord 파일이 여러개 이므로\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋, 모델, 훈련용 객체 조립\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, is_baseline=False):\n",
    "    # MirroredStrategy() : 한 컴퓨터에 GPU가 여러 개인 경우 사용.\n",
    "    # 여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합\n",
    "    # 그 후 모델의 가중치를 업데이트\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        if is_baseline:\n",
    "            model = Simplebaseline(IMAGE_SHAPE)\n",
    "        else:\n",
    "            model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            is_baseline=is_baseline)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Hourglass Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourglass_model_file = train(epochs, learning_rate, num_heatmap, \n",
    "#                              batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[SimpleBaseline Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.607509553 epoch total loss 0.607509553\n",
      "Trained batch 2 batch loss 0.653562665 epoch total loss 0.630536079\n",
      "Trained batch 3 batch loss 0.547527313 epoch total loss 0.602866471\n",
      "Trained batch 4 batch loss 0.530192673 epoch total loss 0.584698\n",
      "Trained batch 5 batch loss 0.501303196 epoch total loss 0.568019032\n",
      "Trained batch 6 batch loss 0.45478195 epoch total loss 0.549146235\n",
      "Trained batch 7 batch loss 0.453841358 epoch total loss 0.535531223\n",
      "Trained batch 8 batch loss 0.451959908 epoch total loss 0.525084853\n",
      "Trained batch 9 batch loss 0.396058232 epoch total loss 0.510748565\n",
      "Trained batch 10 batch loss 0.407073319 epoch total loss 0.500381052\n",
      "Trained batch 11 batch loss 0.429308414 epoch total loss 0.493919879\n",
      "Trained batch 12 batch loss 0.435819089 epoch total loss 0.489078164\n",
      "Trained batch 13 batch loss 0.414327443 epoch total loss 0.483328134\n",
      "Trained batch 14 batch loss 0.38681829 epoch total loss 0.476434559\n",
      "Trained batch 15 batch loss 0.415375262 epoch total loss 0.472363949\n",
      "Trained batch 16 batch loss 0.388644189 epoch total loss 0.467131466\n",
      "Trained batch 17 batch loss 0.40504545 epoch total loss 0.46347934\n",
      "Trained batch 18 batch loss 0.393638849 epoch total loss 0.459599346\n",
      "Trained batch 19 batch loss 0.38819921 epoch total loss 0.455841422\n",
      "Trained batch 20 batch loss 0.393437147 epoch total loss 0.452721208\n",
      "Trained batch 21 batch loss 0.4145751 epoch total loss 0.450904757\n",
      "Trained batch 22 batch loss 0.419356 epoch total loss 0.449470729\n",
      "Trained batch 23 batch loss 0.426863581 epoch total loss 0.448487818\n",
      "Trained batch 24 batch loss 0.429060519 epoch total loss 0.447678357\n",
      "Trained batch 25 batch loss 0.405849367 epoch total loss 0.446005225\n",
      "Trained batch 26 batch loss 0.416099 epoch total loss 0.444854945\n",
      "Trained batch 27 batch loss 0.411402643 epoch total loss 0.443615973\n",
      "Trained batch 28 batch loss 0.396469951 epoch total loss 0.441932201\n",
      "Trained batch 29 batch loss 0.40880987 epoch total loss 0.440790057\n",
      "Trained batch 30 batch loss 0.404616952 epoch total loss 0.439584285\n",
      "Trained batch 31 batch loss 0.407963276 epoch total loss 0.438564241\n",
      "Trained batch 32 batch loss 0.414394379 epoch total loss 0.437808931\n",
      "Trained batch 33 batch loss 0.41090405 epoch total loss 0.436993629\n",
      "Trained batch 34 batch loss 0.403465 epoch total loss 0.4360075\n",
      "Trained batch 35 batch loss 0.402020484 epoch total loss 0.435036451\n",
      "Trained batch 36 batch loss 0.39983809 epoch total loss 0.434058726\n",
      "Trained batch 37 batch loss 0.392342806 epoch total loss 0.432931274\n",
      "Trained batch 38 batch loss 0.386736 epoch total loss 0.431715608\n",
      "Trained batch 39 batch loss 0.410370886 epoch total loss 0.431168348\n",
      "Trained batch 40 batch loss 0.378084064 epoch total loss 0.42984122\n",
      "Trained batch 41 batch loss 0.374836922 epoch total loss 0.428499639\n",
      "Trained batch 42 batch loss 0.369587183 epoch total loss 0.427096963\n",
      "Trained batch 43 batch loss 0.341167927 epoch total loss 0.425098598\n",
      "Trained batch 44 batch loss 0.395679206 epoch total loss 0.424429983\n",
      "Trained batch 45 batch loss 0.362671971 epoch total loss 0.423057586\n",
      "Trained batch 46 batch loss 0.359440327 epoch total loss 0.421674609\n",
      "Trained batch 47 batch loss 0.395467222 epoch total loss 0.421117\n",
      "Trained batch 48 batch loss 0.386301398 epoch total loss 0.420391679\n",
      "Trained batch 49 batch loss 0.395506769 epoch total loss 0.419883817\n",
      "Trained batch 50 batch loss 0.381557047 epoch total loss 0.419117272\n",
      "Trained batch 51 batch loss 0.372804612 epoch total loss 0.418209195\n",
      "Trained batch 52 batch loss 0.404042631 epoch total loss 0.417936772\n",
      "Trained batch 53 batch loss 0.403206974 epoch total loss 0.417658865\n",
      "Trained batch 54 batch loss 0.407395303 epoch total loss 0.417468786\n",
      "Trained batch 55 batch loss 0.394667327 epoch total loss 0.417054206\n",
      "Trained batch 56 batch loss 0.37770316 epoch total loss 0.416351497\n",
      "Trained batch 57 batch loss 0.364645243 epoch total loss 0.415444344\n",
      "Trained batch 58 batch loss 0.376087219 epoch total loss 0.414765775\n",
      "Trained batch 59 batch loss 0.372244626 epoch total loss 0.414045066\n",
      "Trained batch 60 batch loss 0.36322391 epoch total loss 0.413198054\n",
      "Trained batch 61 batch loss 0.380909592 epoch total loss 0.412668735\n",
      "Trained batch 62 batch loss 0.379342973 epoch total loss 0.41213122\n",
      "Trained batch 63 batch loss 0.37972638 epoch total loss 0.411616862\n",
      "Trained batch 64 batch loss 0.375928819 epoch total loss 0.411059231\n",
      "Trained batch 65 batch loss 0.384073585 epoch total loss 0.410644054\n",
      "Trained batch 66 batch loss 0.371102661 epoch total loss 0.410044968\n",
      "Trained batch 67 batch loss 0.371207505 epoch total loss 0.409465313\n",
      "Trained batch 68 batch loss 0.395095855 epoch total loss 0.409253985\n",
      "Trained batch 69 batch loss 0.381329834 epoch total loss 0.408849299\n",
      "Trained batch 70 batch loss 0.352765709 epoch total loss 0.408048123\n",
      "Trained batch 71 batch loss 0.35473603 epoch total loss 0.407297254\n",
      "Trained batch 72 batch loss 0.372627974 epoch total loss 0.406815708\n",
      "Trained batch 73 batch loss 0.356841445 epoch total loss 0.406131148\n",
      "Trained batch 74 batch loss 0.362135708 epoch total loss 0.405536592\n",
      "Trained batch 75 batch loss 0.346837699 epoch total loss 0.404753953\n",
      "Trained batch 76 batch loss 0.362003893 epoch total loss 0.404191434\n",
      "Trained batch 77 batch loss 0.397974968 epoch total loss 0.4041107\n",
      "Trained batch 78 batch loss 0.393081427 epoch total loss 0.403969288\n",
      "Trained batch 79 batch loss 0.370374858 epoch total loss 0.403544039\n",
      "Trained batch 80 batch loss 0.363242269 epoch total loss 0.40304026\n",
      "Trained batch 81 batch loss 0.344329119 epoch total loss 0.402315438\n",
      "Trained batch 82 batch loss 0.373788029 epoch total loss 0.401967525\n",
      "Trained batch 83 batch loss 0.392719448 epoch total loss 0.401856124\n",
      "Trained batch 84 batch loss 0.366742224 epoch total loss 0.401438087\n",
      "Trained batch 85 batch loss 0.386214197 epoch total loss 0.401258975\n",
      "Trained batch 86 batch loss 0.37388882 epoch total loss 0.400940746\n",
      "Trained batch 87 batch loss 0.370791942 epoch total loss 0.400594205\n",
      "Trained batch 88 batch loss 0.398076475 epoch total loss 0.400565594\n",
      "Trained batch 89 batch loss 0.37052846 epoch total loss 0.400228083\n",
      "Trained batch 90 batch loss 0.368433833 epoch total loss 0.399874836\n",
      "Trained batch 91 batch loss 0.388568461 epoch total loss 0.39975059\n",
      "Trained batch 92 batch loss 0.345936805 epoch total loss 0.39916566\n",
      "Trained batch 93 batch loss 0.357482791 epoch total loss 0.398717463\n",
      "Trained batch 94 batch loss 0.352560639 epoch total loss 0.39822644\n",
      "Trained batch 95 batch loss 0.36911425 epoch total loss 0.397919983\n",
      "Trained batch 96 batch loss 0.369146287 epoch total loss 0.397620231\n",
      "Trained batch 97 batch loss 0.369516224 epoch total loss 0.397330493\n",
      "Trained batch 98 batch loss 0.389970213 epoch total loss 0.397255361\n",
      "Trained batch 99 batch loss 0.379371077 epoch total loss 0.397074729\n",
      "Trained batch 100 batch loss 0.376968 epoch total loss 0.396873653\n",
      "Trained batch 101 batch loss 0.391064018 epoch total loss 0.396816134\n",
      "Trained batch 102 batch loss 0.359376699 epoch total loss 0.396449059\n",
      "Trained batch 103 batch loss 0.370907336 epoch total loss 0.396201074\n",
      "Trained batch 104 batch loss 0.362027436 epoch total loss 0.395872474\n",
      "Trained batch 105 batch loss 0.373618454 epoch total loss 0.395660549\n",
      "Trained batch 106 batch loss 0.365953237 epoch total loss 0.395380259\n",
      "Trained batch 107 batch loss 0.414598584 epoch total loss 0.395559907\n",
      "Trained batch 108 batch loss 0.379827797 epoch total loss 0.395414233\n",
      "Trained batch 109 batch loss 0.375420153 epoch total loss 0.3952308\n",
      "Trained batch 110 batch loss 0.357312173 epoch total loss 0.394886076\n",
      "Trained batch 111 batch loss 0.372047365 epoch total loss 0.394680321\n",
      "Trained batch 112 batch loss 0.354486644 epoch total loss 0.394321471\n",
      "Trained batch 113 batch loss 0.324686289 epoch total loss 0.393705249\n",
      "Trained batch 114 batch loss 0.374287784 epoch total loss 0.393534899\n",
      "Trained batch 115 batch loss 0.381391555 epoch total loss 0.393429339\n",
      "Trained batch 116 batch loss 0.408021927 epoch total loss 0.393555135\n",
      "Trained batch 117 batch loss 0.385309577 epoch total loss 0.393484682\n",
      "Trained batch 118 batch loss 0.386525691 epoch total loss 0.393425703\n",
      "Trained batch 119 batch loss 0.387998581 epoch total loss 0.393380105\n",
      "Trained batch 120 batch loss 0.38188374 epoch total loss 0.393284321\n",
      "Trained batch 121 batch loss 0.376128286 epoch total loss 0.393142551\n",
      "Trained batch 122 batch loss 0.385272235 epoch total loss 0.393078029\n",
      "Trained batch 123 batch loss 0.365438133 epoch total loss 0.39285332\n",
      "Trained batch 124 batch loss 0.386635721 epoch total loss 0.392803162\n",
      "Trained batch 125 batch loss 0.366416663 epoch total loss 0.392592072\n",
      "Trained batch 126 batch loss 0.375977665 epoch total loss 0.392460197\n",
      "Trained batch 127 batch loss 0.368878603 epoch total loss 0.392274499\n",
      "Trained batch 128 batch loss 0.369994193 epoch total loss 0.392100453\n",
      "Trained batch 129 batch loss 0.379099309 epoch total loss 0.391999692\n",
      "Trained batch 130 batch loss 0.356643885 epoch total loss 0.391727716\n",
      "Trained batch 131 batch loss 0.387298733 epoch total loss 0.39169389\n",
      "Trained batch 132 batch loss 0.365370721 epoch total loss 0.391494483\n",
      "Trained batch 133 batch loss 0.321326256 epoch total loss 0.390966922\n",
      "Trained batch 134 batch loss 0.311558396 epoch total loss 0.390374303\n",
      "Trained batch 135 batch loss 0.307581961 epoch total loss 0.389761031\n",
      "Trained batch 136 batch loss 0.356209517 epoch total loss 0.389514327\n",
      "Trained batch 137 batch loss 0.38495636 epoch total loss 0.389481068\n",
      "Trained batch 138 batch loss 0.402068973 epoch total loss 0.389572293\n",
      "Trained batch 139 batch loss 0.406250358 epoch total loss 0.389692277\n",
      "Trained batch 140 batch loss 0.375790805 epoch total loss 0.389592975\n",
      "Trained batch 141 batch loss 0.375151217 epoch total loss 0.389490545\n",
      "Trained batch 142 batch loss 0.402032614 epoch total loss 0.389578849\n",
      "Trained batch 143 batch loss 0.371253729 epoch total loss 0.389450699\n",
      "Trained batch 144 batch loss 0.382390499 epoch total loss 0.389401674\n",
      "Trained batch 145 batch loss 0.400660813 epoch total loss 0.389479339\n",
      "Trained batch 146 batch loss 0.380326301 epoch total loss 0.389416635\n",
      "Trained batch 147 batch loss 0.407466739 epoch total loss 0.389539421\n",
      "Trained batch 148 batch loss 0.367563963 epoch total loss 0.389390945\n",
      "Trained batch 149 batch loss 0.380797327 epoch total loss 0.389333278\n",
      "Trained batch 150 batch loss 0.34791404 epoch total loss 0.389057159\n",
      "Trained batch 151 batch loss 0.343174726 epoch total loss 0.388753295\n",
      "Trained batch 152 batch loss 0.359243214 epoch total loss 0.388559133\n",
      "Trained batch 153 batch loss 0.396169066 epoch total loss 0.388608873\n",
      "Trained batch 154 batch loss 0.398981333 epoch total loss 0.388676226\n",
      "Trained batch 155 batch loss 0.38245225 epoch total loss 0.388636082\n",
      "Trained batch 156 batch loss 0.340077043 epoch total loss 0.388324797\n",
      "Trained batch 157 batch loss 0.360203534 epoch total loss 0.388145685\n",
      "Trained batch 158 batch loss 0.372241288 epoch total loss 0.388045043\n",
      "Trained batch 159 batch loss 0.343240082 epoch total loss 0.387763232\n",
      "Trained batch 160 batch loss 0.377041399 epoch total loss 0.387696207\n",
      "Trained batch 161 batch loss 0.380229324 epoch total loss 0.387649834\n",
      "Trained batch 162 batch loss 0.377070338 epoch total loss 0.387584537\n",
      "Trained batch 163 batch loss 0.350382 epoch total loss 0.387356311\n",
      "Trained batch 164 batch loss 0.355232358 epoch total loss 0.38716045\n",
      "Trained batch 165 batch loss 0.379510105 epoch total loss 0.387114078\n",
      "Trained batch 166 batch loss 0.372424394 epoch total loss 0.387025595\n",
      "Trained batch 167 batch loss 0.370586425 epoch total loss 0.386927158\n",
      "Trained batch 168 batch loss 0.368371367 epoch total loss 0.38681671\n",
      "Trained batch 169 batch loss 0.363773108 epoch total loss 0.386680335\n",
      "Trained batch 170 batch loss 0.354253262 epoch total loss 0.3864896\n",
      "Trained batch 171 batch loss 0.346927822 epoch total loss 0.386258274\n",
      "Trained batch 172 batch loss 0.351764 epoch total loss 0.386057705\n",
      "Trained batch 173 batch loss 0.372264773 epoch total loss 0.385977954\n",
      "Trained batch 174 batch loss 0.345740676 epoch total loss 0.385746717\n",
      "Trained batch 175 batch loss 0.349698871 epoch total loss 0.385540724\n",
      "Trained batch 176 batch loss 0.350469917 epoch total loss 0.385341465\n",
      "Trained batch 177 batch loss 0.365319967 epoch total loss 0.385228336\n",
      "Trained batch 178 batch loss 0.356508 epoch total loss 0.385067\n",
      "Trained batch 179 batch loss 0.335493833 epoch total loss 0.384790033\n",
      "Trained batch 180 batch loss 0.344874382 epoch total loss 0.384568274\n",
      "Trained batch 181 batch loss 0.347377211 epoch total loss 0.384362787\n",
      "Trained batch 182 batch loss 0.342098236 epoch total loss 0.384130538\n",
      "Trained batch 183 batch loss 0.346249431 epoch total loss 0.38392356\n",
      "Trained batch 184 batch loss 0.363030016 epoch total loss 0.38381\n",
      "Trained batch 185 batch loss 0.345228076 epoch total loss 0.383601457\n",
      "Trained batch 186 batch loss 0.3344661 epoch total loss 0.383337289\n",
      "Trained batch 187 batch loss 0.369611233 epoch total loss 0.383263886\n",
      "Trained batch 188 batch loss 0.362823844 epoch total loss 0.383155167\n",
      "Trained batch 189 batch loss 0.366936922 epoch total loss 0.383069366\n",
      "Trained batch 190 batch loss 0.378422678 epoch total loss 0.383044928\n",
      "Trained batch 191 batch loss 0.350998551 epoch total loss 0.382877141\n",
      "Trained batch 192 batch loss 0.35893032 epoch total loss 0.382752419\n",
      "Trained batch 193 batch loss 0.342426866 epoch total loss 0.382543504\n",
      "Trained batch 194 batch loss 0.355115175 epoch total loss 0.382402122\n",
      "Trained batch 195 batch loss 0.344507962 epoch total loss 0.382207781\n",
      "Trained batch 196 batch loss 0.352175802 epoch total loss 0.382054538\n",
      "Trained batch 197 batch loss 0.378891528 epoch total loss 0.382038474\n",
      "Trained batch 198 batch loss 0.404970467 epoch total loss 0.382154286\n",
      "Trained batch 199 batch loss 0.39183414 epoch total loss 0.382202923\n",
      "Trained batch 200 batch loss 0.379646897 epoch total loss 0.382190138\n",
      "Trained batch 201 batch loss 0.37779057 epoch total loss 0.382168263\n",
      "Trained batch 202 batch loss 0.36892271 epoch total loss 0.382102668\n",
      "Trained batch 203 batch loss 0.343744159 epoch total loss 0.381913692\n",
      "Trained batch 204 batch loss 0.363129735 epoch total loss 0.381821603\n",
      "Trained batch 205 batch loss 0.373943627 epoch total loss 0.381783187\n",
      "Trained batch 206 batch loss 0.362980425 epoch total loss 0.381691933\n",
      "Trained batch 207 batch loss 0.358729422 epoch total loss 0.381581\n",
      "Trained batch 208 batch loss 0.36122191 epoch total loss 0.381483108\n",
      "Trained batch 209 batch loss 0.349648714 epoch total loss 0.381330788\n",
      "Trained batch 210 batch loss 0.344126016 epoch total loss 0.381153613\n",
      "Trained batch 211 batch loss 0.384272099 epoch total loss 0.381168395\n",
      "Trained batch 212 batch loss 0.390141249 epoch total loss 0.381210715\n",
      "Trained batch 213 batch loss 0.411405087 epoch total loss 0.381352484\n",
      "Trained batch 214 batch loss 0.376675 epoch total loss 0.381330639\n",
      "Trained batch 215 batch loss 0.371043175 epoch total loss 0.381282777\n",
      "Trained batch 216 batch loss 0.394827396 epoch total loss 0.38134551\n",
      "Trained batch 217 batch loss 0.383641 epoch total loss 0.38135609\n",
      "Trained batch 218 batch loss 0.347696841 epoch total loss 0.381201684\n",
      "Trained batch 219 batch loss 0.398084432 epoch total loss 0.381278783\n",
      "Trained batch 220 batch loss 0.353449881 epoch total loss 0.381152272\n",
      "Trained batch 221 batch loss 0.378382653 epoch total loss 0.381139725\n",
      "Trained batch 222 batch loss 0.363425404 epoch total loss 0.381059945\n",
      "Trained batch 223 batch loss 0.350264579 epoch total loss 0.380921841\n",
      "Trained batch 224 batch loss 0.358725816 epoch total loss 0.380822748\n",
      "Trained batch 225 batch loss 0.335894585 epoch total loss 0.380623072\n",
      "Trained batch 226 batch loss 0.33170709 epoch total loss 0.380406648\n",
      "Trained batch 227 batch loss 0.339230776 epoch total loss 0.380225271\n",
      "Trained batch 228 batch loss 0.350628227 epoch total loss 0.380095452\n",
      "Trained batch 229 batch loss 0.338281304 epoch total loss 0.379912853\n",
      "Trained batch 230 batch loss 0.347779393 epoch total loss 0.37977314\n",
      "Trained batch 231 batch loss 0.332077235 epoch total loss 0.379566669\n",
      "Trained batch 232 batch loss 0.326128095 epoch total loss 0.379336327\n",
      "Trained batch 233 batch loss 0.338442087 epoch total loss 0.379160792\n",
      "Trained batch 234 batch loss 0.349950522 epoch total loss 0.37903598\n",
      "Trained batch 235 batch loss 0.364379346 epoch total loss 0.378973603\n",
      "Trained batch 236 batch loss 0.364573538 epoch total loss 0.378912598\n",
      "Trained batch 237 batch loss 0.369810373 epoch total loss 0.378874183\n",
      "Trained batch 238 batch loss 0.365690678 epoch total loss 0.37881881\n",
      "Trained batch 239 batch loss 0.312834531 epoch total loss 0.378542721\n",
      "Trained batch 240 batch loss 0.353044093 epoch total loss 0.378436476\n",
      "Trained batch 241 batch loss 0.370391488 epoch total loss 0.378403097\n",
      "Trained batch 242 batch loss 0.381661028 epoch total loss 0.378416538\n",
      "Trained batch 243 batch loss 0.35463 epoch total loss 0.378318667\n",
      "Trained batch 244 batch loss 0.347469836 epoch total loss 0.378192246\n",
      "Trained batch 245 batch loss 0.361442745 epoch total loss 0.378123879\n",
      "Trained batch 246 batch loss 0.358211637 epoch total loss 0.378042936\n",
      "Trained batch 247 batch loss 0.380970269 epoch total loss 0.378054827\n",
      "Trained batch 248 batch loss 0.371452242 epoch total loss 0.378028184\n",
      "Trained batch 249 batch loss 0.354318678 epoch total loss 0.377932966\n",
      "Trained batch 250 batch loss 0.361429572 epoch total loss 0.377866954\n",
      "Trained batch 251 batch loss 0.361388206 epoch total loss 0.377801299\n",
      "Trained batch 252 batch loss 0.352565527 epoch total loss 0.377701133\n",
      "Trained batch 253 batch loss 0.335927486 epoch total loss 0.377536029\n",
      "Trained batch 254 batch loss 0.367361337 epoch total loss 0.377495974\n",
      "Trained batch 255 batch loss 0.396208286 epoch total loss 0.377569377\n",
      "Trained batch 256 batch loss 0.363801122 epoch total loss 0.377515584\n",
      "Trained batch 257 batch loss 0.342060894 epoch total loss 0.377377629\n",
      "Trained batch 258 batch loss 0.338901639 epoch total loss 0.377228528\n",
      "Trained batch 259 batch loss 0.338191926 epoch total loss 0.377077788\n",
      "Trained batch 260 batch loss 0.366611868 epoch total loss 0.377037555\n",
      "Trained batch 261 batch loss 0.367668211 epoch total loss 0.377001643\n",
      "Trained batch 262 batch loss 0.379546 epoch total loss 0.377011359\n",
      "Trained batch 263 batch loss 0.36764878 epoch total loss 0.376975745\n",
      "Trained batch 264 batch loss 0.362155825 epoch total loss 0.376919597\n",
      "Trained batch 265 batch loss 0.373409212 epoch total loss 0.376906335\n",
      "Trained batch 266 batch loss 0.384463876 epoch total loss 0.376934737\n",
      "Trained batch 267 batch loss 0.350811452 epoch total loss 0.376836926\n",
      "Trained batch 268 batch loss 0.37716648 epoch total loss 0.376838148\n",
      "Trained batch 269 batch loss 0.355588615 epoch total loss 0.376759171\n",
      "Trained batch 270 batch loss 0.372255564 epoch total loss 0.376742482\n",
      "Trained batch 271 batch loss 0.369986326 epoch total loss 0.376717538\n",
      "Trained batch 272 batch loss 0.331814826 epoch total loss 0.376552463\n",
      "Trained batch 273 batch loss 0.329846263 epoch total loss 0.376381397\n",
      "Trained batch 274 batch loss 0.348367393 epoch total loss 0.376279145\n",
      "Trained batch 275 batch loss 0.346810281 epoch total loss 0.376171976\n",
      "Trained batch 276 batch loss 0.346234769 epoch total loss 0.376063526\n",
      "Trained batch 277 batch loss 0.357256591 epoch total loss 0.375995636\n",
      "Trained batch 278 batch loss 0.341458231 epoch total loss 0.37587139\n",
      "Trained batch 279 batch loss 0.341826469 epoch total loss 0.375749379\n",
      "Trained batch 280 batch loss 0.332569778 epoch total loss 0.375595182\n",
      "Trained batch 281 batch loss 0.344183862 epoch total loss 0.375483394\n",
      "Trained batch 282 batch loss 0.337425143 epoch total loss 0.375348449\n",
      "Trained batch 283 batch loss 0.339887589 epoch total loss 0.37522313\n",
      "Trained batch 284 batch loss 0.351442814 epoch total loss 0.375139385\n",
      "Trained batch 285 batch loss 0.341727614 epoch total loss 0.375022173\n",
      "Trained batch 286 batch loss 0.318082273 epoch total loss 0.374823093\n",
      "Trained batch 287 batch loss 0.340032548 epoch total loss 0.374701858\n",
      "Trained batch 288 batch loss 0.360438854 epoch total loss 0.374652326\n",
      "Trained batch 289 batch loss 0.363089591 epoch total loss 0.374612331\n",
      "Trained batch 290 batch loss 0.349958748 epoch total loss 0.374527305\n",
      "Trained batch 291 batch loss 0.328257442 epoch total loss 0.37436831\n",
      "Trained batch 292 batch loss 0.359991461 epoch total loss 0.374319077\n",
      "Trained batch 293 batch loss 0.354632944 epoch total loss 0.374251872\n",
      "Trained batch 294 batch loss 0.339734733 epoch total loss 0.374134481\n",
      "Trained batch 295 batch loss 0.30956 epoch total loss 0.373915583\n",
      "Trained batch 296 batch loss 0.334273517 epoch total loss 0.373781681\n",
      "Trained batch 297 batch loss 0.360802889 epoch total loss 0.373737961\n",
      "Trained batch 298 batch loss 0.349621117 epoch total loss 0.373657048\n",
      "Trained batch 299 batch loss 0.364151627 epoch total loss 0.373625249\n",
      "Trained batch 300 batch loss 0.326143801 epoch total loss 0.373466969\n",
      "Trained batch 301 batch loss 0.361593187 epoch total loss 0.37342754\n",
      "Trained batch 302 batch loss 0.370620102 epoch total loss 0.373418242\n",
      "Trained batch 303 batch loss 0.342691392 epoch total loss 0.373316824\n",
      "Trained batch 304 batch loss 0.346915573 epoch total loss 0.37322998\n",
      "Trained batch 305 batch loss 0.358583897 epoch total loss 0.373181939\n",
      "Trained batch 306 batch loss 0.33560425 epoch total loss 0.373059154\n",
      "Trained batch 307 batch loss 0.340535194 epoch total loss 0.372953206\n",
      "Trained batch 308 batch loss 0.339790046 epoch total loss 0.372845531\n",
      "Trained batch 309 batch loss 0.346654743 epoch total loss 0.372760773\n",
      "Trained batch 310 batch loss 0.341212839 epoch total loss 0.372659\n",
      "Trained batch 311 batch loss 0.328366876 epoch total loss 0.372516602\n",
      "Trained batch 312 batch loss 0.328134596 epoch total loss 0.372374326\n",
      "Trained batch 313 batch loss 0.34921959 epoch total loss 0.372300357\n",
      "Trained batch 314 batch loss 0.37744078 epoch total loss 0.372316748\n",
      "Trained batch 315 batch loss 0.347300708 epoch total loss 0.372237325\n",
      "Trained batch 316 batch loss 0.376292259 epoch total loss 0.37225014\n",
      "Trained batch 317 batch loss 0.348209858 epoch total loss 0.372174323\n",
      "Trained batch 318 batch loss 0.370047539 epoch total loss 0.372167617\n",
      "Trained batch 319 batch loss 0.370330393 epoch total loss 0.372161865\n",
      "Trained batch 320 batch loss 0.368067294 epoch total loss 0.37214905\n",
      "Trained batch 321 batch loss 0.351146549 epoch total loss 0.372083634\n",
      "Trained batch 322 batch loss 0.349648654 epoch total loss 0.372013956\n",
      "Trained batch 323 batch loss 0.343721449 epoch total loss 0.371926337\n",
      "Trained batch 324 batch loss 0.31795907 epoch total loss 0.371759802\n",
      "Trained batch 325 batch loss 0.342709899 epoch total loss 0.371670425\n",
      "Trained batch 326 batch loss 0.309360474 epoch total loss 0.371479273\n",
      "Trained batch 327 batch loss 0.317677736 epoch total loss 0.371314734\n",
      "Trained batch 328 batch loss 0.329592198 epoch total loss 0.371187538\n",
      "Trained batch 329 batch loss 0.339581937 epoch total loss 0.371091485\n",
      "Trained batch 330 batch loss 0.343364894 epoch total loss 0.371007472\n",
      "Trained batch 331 batch loss 0.344594032 epoch total loss 0.370927691\n",
      "Trained batch 332 batch loss 0.37996763 epoch total loss 0.370954901\n",
      "Trained batch 333 batch loss 0.369426429 epoch total loss 0.370950311\n",
      "Trained batch 334 batch loss 0.373251736 epoch total loss 0.370957196\n",
      "Trained batch 335 batch loss 0.371819615 epoch total loss 0.370959759\n",
      "Trained batch 336 batch loss 0.370614082 epoch total loss 0.370958745\n",
      "Trained batch 337 batch loss 0.347792268 epoch total loss 0.37089\n",
      "Trained batch 338 batch loss 0.351103365 epoch total loss 0.37083146\n",
      "Trained batch 339 batch loss 0.375818074 epoch total loss 0.370846152\n",
      "Trained batch 340 batch loss 0.355760634 epoch total loss 0.370801777\n",
      "Trained batch 341 batch loss 0.337152272 epoch total loss 0.370703101\n",
      "Trained batch 342 batch loss 0.336202204 epoch total loss 0.37060222\n",
      "Trained batch 343 batch loss 0.348669767 epoch total loss 0.370538294\n",
      "Trained batch 344 batch loss 0.332099 epoch total loss 0.370426565\n",
      "Trained batch 345 batch loss 0.344903052 epoch total loss 0.370352566\n",
      "Trained batch 346 batch loss 0.347217262 epoch total loss 0.37028569\n",
      "Trained batch 347 batch loss 0.339033842 epoch total loss 0.370195627\n",
      "Trained batch 348 batch loss 0.320860922 epoch total loss 0.370053858\n",
      "Trained batch 349 batch loss 0.335042477 epoch total loss 0.369953543\n",
      "Trained batch 350 batch loss 0.341963887 epoch total loss 0.369873583\n",
      "Trained batch 351 batch loss 0.346512765 epoch total loss 0.369807\n",
      "Trained batch 352 batch loss 0.352211773 epoch total loss 0.369757056\n",
      "Trained batch 353 batch loss 0.332798332 epoch total loss 0.369652331\n",
      "Trained batch 354 batch loss 0.370767206 epoch total loss 0.36965549\n",
      "Trained batch 355 batch loss 0.368163049 epoch total loss 0.369651288\n",
      "Trained batch 356 batch loss 0.361244828 epoch total loss 0.369627714\n",
      "Trained batch 357 batch loss 0.360860884 epoch total loss 0.369603127\n",
      "Trained batch 358 batch loss 0.36667788 epoch total loss 0.369595\n",
      "Trained batch 359 batch loss 0.360049367 epoch total loss 0.369568378\n",
      "Trained batch 360 batch loss 0.352874458 epoch total loss 0.369522\n",
      "Trained batch 361 batch loss 0.371774793 epoch total loss 0.369528264\n",
      "Trained batch 362 batch loss 0.375349343 epoch total loss 0.369544357\n",
      "Trained batch 363 batch loss 0.347366035 epoch total loss 0.369483262\n",
      "Trained batch 364 batch loss 0.361793041 epoch total loss 0.369462103\n",
      "Trained batch 365 batch loss 0.379359156 epoch total loss 0.369489223\n",
      "Trained batch 366 batch loss 0.37940675 epoch total loss 0.369516343\n",
      "Trained batch 367 batch loss 0.355754316 epoch total loss 0.369478852\n",
      "Trained batch 368 batch loss 0.353336751 epoch total loss 0.369434983\n",
      "Trained batch 369 batch loss 0.355688483 epoch total loss 0.3693977\n",
      "Trained batch 370 batch loss 0.365237713 epoch total loss 0.369386464\n",
      "Trained batch 371 batch loss 0.354123086 epoch total loss 0.369345337\n",
      "Trained batch 372 batch loss 0.358085901 epoch total loss 0.369315088\n",
      "Trained batch 373 batch loss 0.366378844 epoch total loss 0.36930719\n",
      "Trained batch 374 batch loss 0.361134171 epoch total loss 0.369285345\n",
      "Trained batch 375 batch loss 0.35152033 epoch total loss 0.369237959\n",
      "Trained batch 376 batch loss 0.357455313 epoch total loss 0.369206607\n",
      "Trained batch 377 batch loss 0.351670623 epoch total loss 0.369160086\n",
      "Trained batch 378 batch loss 0.346980631 epoch total loss 0.369101435\n",
      "Trained batch 379 batch loss 0.339507669 epoch total loss 0.369023353\n",
      "Trained batch 380 batch loss 0.340833753 epoch total loss 0.368949175\n",
      "Trained batch 381 batch loss 0.337988973 epoch total loss 0.368867904\n",
      "Trained batch 382 batch loss 0.342442483 epoch total loss 0.368798703\n",
      "Trained batch 383 batch loss 0.322963536 epoch total loss 0.368679047\n",
      "Trained batch 384 batch loss 0.329372793 epoch total loss 0.368576676\n",
      "Trained batch 385 batch loss 0.332016617 epoch total loss 0.368481725\n",
      "Trained batch 386 batch loss 0.351604432 epoch total loss 0.368438\n",
      "Trained batch 387 batch loss 0.356104642 epoch total loss 0.368406147\n",
      "Trained batch 388 batch loss 0.345712095 epoch total loss 0.368347675\n",
      "Trained batch 389 batch loss 0.356858671 epoch total loss 0.368318141\n",
      "Trained batch 390 batch loss 0.353577852 epoch total loss 0.368280351\n",
      "Trained batch 391 batch loss 0.34035942 epoch total loss 0.368208945\n",
      "Trained batch 392 batch loss 0.33668223 epoch total loss 0.368128538\n",
      "Trained batch 393 batch loss 0.360300392 epoch total loss 0.36810863\n",
      "Trained batch 394 batch loss 0.372282773 epoch total loss 0.36811921\n",
      "Trained batch 395 batch loss 0.374092758 epoch total loss 0.36813435\n",
      "Trained batch 396 batch loss 0.376226306 epoch total loss 0.368154764\n",
      "Trained batch 397 batch loss 0.402904928 epoch total loss 0.368242323\n",
      "Trained batch 398 batch loss 0.348607481 epoch total loss 0.368192971\n",
      "Trained batch 399 batch loss 0.323225439 epoch total loss 0.368080258\n",
      "Trained batch 400 batch loss 0.324992865 epoch total loss 0.367972553\n",
      "Trained batch 401 batch loss 0.359805554 epoch total loss 0.367952198\n",
      "Trained batch 402 batch loss 0.335700721 epoch total loss 0.36787194\n",
      "Trained batch 403 batch loss 0.340685427 epoch total loss 0.367804468\n",
      "Trained batch 404 batch loss 0.34034726 epoch total loss 0.367736518\n",
      "Trained batch 405 batch loss 0.345487982 epoch total loss 0.367681593\n",
      "Trained batch 406 batch loss 0.331500232 epoch total loss 0.367592454\n",
      "Trained batch 407 batch loss 0.325715363 epoch total loss 0.367489576\n",
      "Trained batch 408 batch loss 0.333289713 epoch total loss 0.367405713\n",
      "Trained batch 409 batch loss 0.357145697 epoch total loss 0.367380649\n",
      "Trained batch 410 batch loss 0.327825457 epoch total loss 0.367284149\n",
      "Trained batch 411 batch loss 0.338568 epoch total loss 0.367214262\n",
      "Trained batch 412 batch loss 0.341562539 epoch total loss 0.367152\n",
      "Trained batch 413 batch loss 0.376138151 epoch total loss 0.367173791\n",
      "Trained batch 414 batch loss 0.340007126 epoch total loss 0.367108196\n",
      "Trained batch 415 batch loss 0.342240065 epoch total loss 0.367048264\n",
      "Trained batch 416 batch loss 0.366130292 epoch total loss 0.367046058\n",
      "Trained batch 417 batch loss 0.343080729 epoch total loss 0.366988599\n",
      "Trained batch 418 batch loss 0.312368155 epoch total loss 0.366857916\n",
      "Trained batch 419 batch loss 0.342985868 epoch total loss 0.366800934\n",
      "Trained batch 420 batch loss 0.335137159 epoch total loss 0.366725564\n",
      "Trained batch 421 batch loss 0.338748336 epoch total loss 0.366659105\n",
      "Trained batch 422 batch loss 0.347207606 epoch total loss 0.36661303\n",
      "Trained batch 423 batch loss 0.348992765 epoch total loss 0.366571367\n",
      "Trained batch 424 batch loss 0.337682724 epoch total loss 0.366503239\n",
      "Trained batch 425 batch loss 0.325469613 epoch total loss 0.366406679\n",
      "Trained batch 426 batch loss 0.331892967 epoch total loss 0.366325676\n",
      "Trained batch 427 batch loss 0.356083274 epoch total loss 0.366301656\n",
      "Trained batch 428 batch loss 0.335482657 epoch total loss 0.366229653\n",
      "Trained batch 429 batch loss 0.357694089 epoch total loss 0.366209775\n",
      "Trained batch 430 batch loss 0.343104869 epoch total loss 0.366156042\n",
      "Trained batch 431 batch loss 0.314587951 epoch total loss 0.366036385\n",
      "Trained batch 432 batch loss 0.30892694 epoch total loss 0.365904212\n",
      "Trained batch 433 batch loss 0.340455741 epoch total loss 0.365845442\n",
      "Trained batch 434 batch loss 0.38510552 epoch total loss 0.365889788\n",
      "Trained batch 435 batch loss 0.340735227 epoch total loss 0.365831971\n",
      "Trained batch 436 batch loss 0.325195938 epoch total loss 0.36573875\n",
      "Trained batch 437 batch loss 0.357805431 epoch total loss 0.3657206\n",
      "Trained batch 438 batch loss 0.339564323 epoch total loss 0.365660876\n",
      "Trained batch 439 batch loss 0.313654512 epoch total loss 0.365542442\n",
      "Trained batch 440 batch loss 0.329313397 epoch total loss 0.365460098\n",
      "Trained batch 441 batch loss 0.341033369 epoch total loss 0.365404725\n",
      "Trained batch 442 batch loss 0.327555269 epoch total loss 0.365319103\n",
      "Trained batch 443 batch loss 0.306525826 epoch total loss 0.365186363\n",
      "Trained batch 444 batch loss 0.308989644 epoch total loss 0.365059793\n",
      "Trained batch 445 batch loss 0.305743873 epoch total loss 0.364926487\n",
      "Trained batch 446 batch loss 0.337807894 epoch total loss 0.36486569\n",
      "Trained batch 447 batch loss 0.325790524 epoch total loss 0.36477828\n",
      "Trained batch 448 batch loss 0.320382357 epoch total loss 0.364679188\n",
      "Trained batch 449 batch loss 0.326017261 epoch total loss 0.364593089\n",
      "Trained batch 450 batch loss 0.334455341 epoch total loss 0.364526123\n",
      "Trained batch 451 batch loss 0.359594047 epoch total loss 0.364515185\n",
      "Trained batch 452 batch loss 0.367468655 epoch total loss 0.364521712\n",
      "Trained batch 453 batch loss 0.366685688 epoch total loss 0.36452648\n",
      "Trained batch 454 batch loss 0.366935402 epoch total loss 0.364531755\n",
      "Trained batch 455 batch loss 0.341216832 epoch total loss 0.364480525\n",
      "Trained batch 456 batch loss 0.3655608 epoch total loss 0.36448288\n",
      "Trained batch 457 batch loss 0.340201825 epoch total loss 0.364429742\n",
      "Trained batch 458 batch loss 0.358256638 epoch total loss 0.364416271\n",
      "Trained batch 459 batch loss 0.31333 epoch total loss 0.36430496\n",
      "Trained batch 460 batch loss 0.343532771 epoch total loss 0.364259809\n",
      "Trained batch 461 batch loss 0.318784356 epoch total loss 0.364161164\n",
      "Trained batch 462 batch loss 0.314885557 epoch total loss 0.364054501\n",
      "Trained batch 463 batch loss 0.313791573 epoch total loss 0.363945961\n",
      "Trained batch 464 batch loss 0.301439017 epoch total loss 0.363811225\n",
      "Trained batch 465 batch loss 0.291993529 epoch total loss 0.363656789\n",
      "Trained batch 466 batch loss 0.31297034 epoch total loss 0.363548\n",
      "Trained batch 467 batch loss 0.288812071 epoch total loss 0.363388\n",
      "Trained batch 468 batch loss 0.31434828 epoch total loss 0.363283217\n",
      "Trained batch 469 batch loss 0.316694289 epoch total loss 0.363183886\n",
      "Trained batch 470 batch loss 0.31535697 epoch total loss 0.363082111\n",
      "Trained batch 471 batch loss 0.342521459 epoch total loss 0.36303845\n",
      "Trained batch 472 batch loss 0.324559122 epoch total loss 0.362956911\n",
      "Trained batch 473 batch loss 0.305441439 epoch total loss 0.362835288\n",
      "Trained batch 474 batch loss 0.345169604 epoch total loss 0.362798035\n",
      "Trained batch 475 batch loss 0.357424498 epoch total loss 0.36278671\n",
      "Trained batch 476 batch loss 0.355136365 epoch total loss 0.362770617\n",
      "Trained batch 477 batch loss 0.360673398 epoch total loss 0.362766236\n",
      "Trained batch 478 batch loss 0.305072337 epoch total loss 0.362645507\n",
      "Trained batch 479 batch loss 0.343565077 epoch total loss 0.362605691\n",
      "Trained batch 480 batch loss 0.345014781 epoch total loss 0.362569034\n",
      "Trained batch 481 batch loss 0.348316312 epoch total loss 0.362539411\n",
      "Trained batch 482 batch loss 0.367180288 epoch total loss 0.362549037\n",
      "Trained batch 483 batch loss 0.389068723 epoch total loss 0.362603962\n",
      "Trained batch 484 batch loss 0.346739858 epoch total loss 0.36257118\n",
      "Trained batch 485 batch loss 0.340734482 epoch total loss 0.362526149\n",
      "Trained batch 486 batch loss 0.337631166 epoch total loss 0.362474918\n",
      "Trained batch 487 batch loss 0.351375163 epoch total loss 0.362452149\n",
      "Trained batch 488 batch loss 0.330338359 epoch total loss 0.362386316\n",
      "Trained batch 489 batch loss 0.345382154 epoch total loss 0.362351567\n",
      "Trained batch 490 batch loss 0.370225549 epoch total loss 0.36236763\n",
      "Trained batch 491 batch loss 0.362210959 epoch total loss 0.362367302\n",
      "Trained batch 492 batch loss 0.39264974 epoch total loss 0.362428874\n",
      "Trained batch 493 batch loss 0.383693248 epoch total loss 0.362472\n",
      "Trained batch 494 batch loss 0.388133794 epoch total loss 0.362523973\n",
      "Trained batch 495 batch loss 0.383855879 epoch total loss 0.362567037\n",
      "Trained batch 496 batch loss 0.360827804 epoch total loss 0.362563521\n",
      "Trained batch 497 batch loss 0.365389585 epoch total loss 0.362569213\n",
      "Trained batch 498 batch loss 0.360958189 epoch total loss 0.362566\n",
      "Trained batch 499 batch loss 0.34252736 epoch total loss 0.362525821\n",
      "Trained batch 500 batch loss 0.375221401 epoch total loss 0.362551242\n",
      "Trained batch 501 batch loss 0.352773488 epoch total loss 0.362531722\n",
      "Trained batch 502 batch loss 0.340785623 epoch total loss 0.362488389\n",
      "Trained batch 503 batch loss 0.348121673 epoch total loss 0.362459868\n",
      "Trained batch 504 batch loss 0.336629599 epoch total loss 0.362408578\n",
      "Trained batch 505 batch loss 0.346038699 epoch total loss 0.362376183\n",
      "Trained batch 506 batch loss 0.324299932 epoch total loss 0.362300932\n",
      "Trained batch 507 batch loss 0.333763897 epoch total loss 0.362244636\n",
      "Trained batch 508 batch loss 0.331977636 epoch total loss 0.362185061\n",
      "Trained batch 509 batch loss 0.366940081 epoch total loss 0.362194389\n",
      "Trained batch 510 batch loss 0.356654525 epoch total loss 0.362183541\n",
      "Trained batch 511 batch loss 0.34950152 epoch total loss 0.362158716\n",
      "Trained batch 512 batch loss 0.371370047 epoch total loss 0.362176716\n",
      "Trained batch 513 batch loss 0.378056288 epoch total loss 0.362207651\n",
      "Trained batch 514 batch loss 0.337138921 epoch total loss 0.362158895\n",
      "Trained batch 515 batch loss 0.335518301 epoch total loss 0.362107188\n",
      "Trained batch 516 batch loss 0.317826301 epoch total loss 0.362021357\n",
      "Trained batch 517 batch loss 0.345122784 epoch total loss 0.361988693\n",
      "Trained batch 518 batch loss 0.301268816 epoch total loss 0.361871451\n",
      "Trained batch 519 batch loss 0.315602601 epoch total loss 0.361782312\n",
      "Trained batch 520 batch loss 0.346889257 epoch total loss 0.361753672\n",
      "Trained batch 521 batch loss 0.316575348 epoch total loss 0.361666948\n",
      "Trained batch 522 batch loss 0.336380124 epoch total loss 0.361618519\n",
      "Trained batch 523 batch loss 0.363371 epoch total loss 0.361621857\n",
      "Trained batch 524 batch loss 0.330961287 epoch total loss 0.361563355\n",
      "Trained batch 525 batch loss 0.34007597 epoch total loss 0.361522406\n",
      "Trained batch 526 batch loss 0.339625597 epoch total loss 0.361480802\n",
      "Trained batch 527 batch loss 0.347812951 epoch total loss 0.361454844\n",
      "Trained batch 528 batch loss 0.361085296 epoch total loss 0.361454159\n",
      "Trained batch 529 batch loss 0.347913444 epoch total loss 0.361428559\n",
      "Trained batch 530 batch loss 0.362268299 epoch total loss 0.361430168\n",
      "Trained batch 531 batch loss 0.321543574 epoch total loss 0.361355036\n",
      "Trained batch 532 batch loss 0.3497549 epoch total loss 0.361333251\n",
      "Trained batch 533 batch loss 0.326260716 epoch total loss 0.361267447\n",
      "Trained batch 534 batch loss 0.353567183 epoch total loss 0.361253023\n",
      "Trained batch 535 batch loss 0.347214282 epoch total loss 0.361226797\n",
      "Trained batch 536 batch loss 0.340348542 epoch total loss 0.361187845\n",
      "Trained batch 537 batch loss 0.328520149 epoch total loss 0.361127019\n",
      "Trained batch 538 batch loss 0.34160471 epoch total loss 0.36109072\n",
      "Trained batch 539 batch loss 0.320885777 epoch total loss 0.361016124\n",
      "Trained batch 540 batch loss 0.359928787 epoch total loss 0.361014098\n",
      "Trained batch 541 batch loss 0.345360965 epoch total loss 0.36098519\n",
      "Trained batch 542 batch loss 0.31286633 epoch total loss 0.360896409\n",
      "Trained batch 543 batch loss 0.330837667 epoch total loss 0.360841066\n",
      "Trained batch 544 batch loss 0.337223232 epoch total loss 0.360797644\n",
      "Trained batch 545 batch loss 0.359471172 epoch total loss 0.3607952\n",
      "Trained batch 546 batch loss 0.32936731 epoch total loss 0.360737622\n",
      "Trained batch 547 batch loss 0.331790686 epoch total loss 0.360684693\n",
      "Trained batch 548 batch loss 0.351002663 epoch total loss 0.36066702\n",
      "Trained batch 549 batch loss 0.318280816 epoch total loss 0.360589802\n",
      "Trained batch 550 batch loss 0.360584646 epoch total loss 0.360589802\n",
      "Trained batch 551 batch loss 0.36599791 epoch total loss 0.360599607\n",
      "Trained batch 552 batch loss 0.28401947 epoch total loss 0.360460848\n",
      "Trained batch 553 batch loss 0.280082077 epoch total loss 0.360315502\n",
      "Trained batch 554 batch loss 0.312136412 epoch total loss 0.360228539\n",
      "Trained batch 555 batch loss 0.318265945 epoch total loss 0.36015293\n",
      "Trained batch 556 batch loss 0.317400515 epoch total loss 0.36007604\n",
      "Trained batch 557 batch loss 0.313857615 epoch total loss 0.359993041\n",
      "Trained batch 558 batch loss 0.325616837 epoch total loss 0.359931439\n",
      "Trained batch 559 batch loss 0.354498118 epoch total loss 0.359921724\n",
      "Trained batch 560 batch loss 0.345316172 epoch total loss 0.359895647\n",
      "Trained batch 561 batch loss 0.334053874 epoch total loss 0.359849602\n",
      "Trained batch 562 batch loss 0.362220705 epoch total loss 0.359853804\n",
      "Trained batch 563 batch loss 0.389658749 epoch total loss 0.359906763\n",
      "Trained batch 564 batch loss 0.356366277 epoch total loss 0.359900475\n",
      "Trained batch 565 batch loss 0.356700629 epoch total loss 0.359894812\n",
      "Trained batch 566 batch loss 0.345567286 epoch total loss 0.35986951\n",
      "Trained batch 567 batch loss 0.324501961 epoch total loss 0.359807134\n",
      "Trained batch 568 batch loss 0.298703372 epoch total loss 0.359699577\n",
      "Trained batch 569 batch loss 0.303742886 epoch total loss 0.359601229\n",
      "Trained batch 570 batch loss 0.311840743 epoch total loss 0.359517455\n",
      "Trained batch 571 batch loss 0.341635942 epoch total loss 0.359486103\n",
      "Trained batch 572 batch loss 0.357849479 epoch total loss 0.359483242\n",
      "Trained batch 573 batch loss 0.323269576 epoch total loss 0.359420061\n",
      "Trained batch 574 batch loss 0.319770873 epoch total loss 0.359351\n",
      "Trained batch 575 batch loss 0.349320531 epoch total loss 0.359333545\n",
      "Trained batch 576 batch loss 0.355821252 epoch total loss 0.359327435\n",
      "Trained batch 577 batch loss 0.349520594 epoch total loss 0.359310448\n",
      "Trained batch 578 batch loss 0.334940553 epoch total loss 0.359268308\n",
      "Trained batch 579 batch loss 0.330406934 epoch total loss 0.359218448\n",
      "Trained batch 580 batch loss 0.322006702 epoch total loss 0.359154314\n",
      "Trained batch 581 batch loss 0.322190017 epoch total loss 0.359090686\n",
      "Trained batch 582 batch loss 0.31638357 epoch total loss 0.359017313\n",
      "Trained batch 583 batch loss 0.34179455 epoch total loss 0.358987778\n",
      "Trained batch 584 batch loss 0.303348094 epoch total loss 0.3588925\n",
      "Trained batch 585 batch loss 0.312260866 epoch total loss 0.358812779\n",
      "Trained batch 586 batch loss 0.367801666 epoch total loss 0.358828098\n",
      "Trained batch 587 batch loss 0.339038908 epoch total loss 0.358794391\n",
      "Trained batch 588 batch loss 0.339819849 epoch total loss 0.358762115\n",
      "Trained batch 589 batch loss 0.345714271 epoch total loss 0.358739972\n",
      "Trained batch 590 batch loss 0.357952297 epoch total loss 0.358738631\n",
      "Trained batch 591 batch loss 0.342340291 epoch total loss 0.358710885\n",
      "Trained batch 592 batch loss 0.329763442 epoch total loss 0.358661979\n",
      "Trained batch 593 batch loss 0.333521843 epoch total loss 0.358619601\n",
      "Trained batch 594 batch loss 0.352806151 epoch total loss 0.358609825\n",
      "Trained batch 595 batch loss 0.33116281 epoch total loss 0.358563691\n",
      "Trained batch 596 batch loss 0.348815978 epoch total loss 0.35854733\n",
      "Trained batch 597 batch loss 0.321996748 epoch total loss 0.358486116\n",
      "Trained batch 598 batch loss 0.331860393 epoch total loss 0.358441591\n",
      "Trained batch 599 batch loss 0.319420338 epoch total loss 0.358376443\n",
      "Trained batch 600 batch loss 0.317966759 epoch total loss 0.35830909\n",
      "Trained batch 601 batch loss 0.318499804 epoch total loss 0.35824284\n",
      "Trained batch 602 batch loss 0.303923696 epoch total loss 0.358152628\n",
      "Trained batch 603 batch loss 0.308464885 epoch total loss 0.358070225\n",
      "Trained batch 604 batch loss 0.32813108 epoch total loss 0.358020663\n",
      "Trained batch 605 batch loss 0.305252 epoch total loss 0.357933432\n",
      "Trained batch 606 batch loss 0.283851475 epoch total loss 0.357811183\n",
      "Trained batch 607 batch loss 0.312397778 epoch total loss 0.357736349\n",
      "Trained batch 608 batch loss 0.259968191 epoch total loss 0.357575536\n",
      "Trained batch 609 batch loss 0.250519633 epoch total loss 0.357399762\n",
      "Trained batch 610 batch loss 0.29903996 epoch total loss 0.357304096\n",
      "Trained batch 611 batch loss 0.331439525 epoch total loss 0.357261747\n",
      "Trained batch 612 batch loss 0.405605346 epoch total loss 0.357340753\n",
      "Trained batch 613 batch loss 0.395059079 epoch total loss 0.357402295\n",
      "Trained batch 614 batch loss 0.316875845 epoch total loss 0.357336283\n",
      "Trained batch 615 batch loss 0.317686975 epoch total loss 0.35727182\n",
      "Trained batch 616 batch loss 0.337366819 epoch total loss 0.357239515\n",
      "Trained batch 617 batch loss 0.334294289 epoch total loss 0.357202321\n",
      "Trained batch 618 batch loss 0.346112221 epoch total loss 0.35718438\n",
      "Trained batch 619 batch loss 0.331507325 epoch total loss 0.357142895\n",
      "Trained batch 620 batch loss 0.339117765 epoch total loss 0.357113808\n",
      "Trained batch 621 batch loss 0.357659578 epoch total loss 0.357114702\n",
      "Trained batch 622 batch loss 0.333354324 epoch total loss 0.357076526\n",
      "Trained batch 623 batch loss 0.333094478 epoch total loss 0.357038021\n",
      "Trained batch 624 batch loss 0.317847461 epoch total loss 0.356975228\n",
      "Trained batch 625 batch loss 0.323479265 epoch total loss 0.356921643\n",
      "Trained batch 626 batch loss 0.338601798 epoch total loss 0.356892377\n",
      "Trained batch 627 batch loss 0.329906255 epoch total loss 0.356849343\n",
      "Trained batch 628 batch loss 0.316871047 epoch total loss 0.356785685\n",
      "Trained batch 629 batch loss 0.282404661 epoch total loss 0.356667429\n",
      "Trained batch 630 batch loss 0.271221936 epoch total loss 0.356531799\n",
      "Trained batch 631 batch loss 0.281881899 epoch total loss 0.356413484\n",
      "Trained batch 632 batch loss 0.316212773 epoch total loss 0.356349885\n",
      "Trained batch 633 batch loss 0.294985592 epoch total loss 0.356252939\n",
      "Trained batch 634 batch loss 0.318340182 epoch total loss 0.356193125\n",
      "Trained batch 635 batch loss 0.323564976 epoch total loss 0.356141746\n",
      "Trained batch 636 batch loss 0.310969621 epoch total loss 0.356070727\n",
      "Trained batch 637 batch loss 0.344991952 epoch total loss 0.356053323\n",
      "Trained batch 638 batch loss 0.328231096 epoch total loss 0.356009722\n",
      "Trained batch 639 batch loss 0.330264747 epoch total loss 0.355969429\n",
      "Trained batch 640 batch loss 0.356850058 epoch total loss 0.3559708\n",
      "Trained batch 641 batch loss 0.299971879 epoch total loss 0.355883449\n",
      "Trained batch 642 batch loss 0.303615212 epoch total loss 0.355802029\n",
      "Trained batch 643 batch loss 0.31370163 epoch total loss 0.355736583\n",
      "Trained batch 644 batch loss 0.338719696 epoch total loss 0.355710149\n",
      "Trained batch 645 batch loss 0.365199983 epoch total loss 0.355724871\n",
      "Trained batch 646 batch loss 0.379406035 epoch total loss 0.355761528\n",
      "Trained batch 647 batch loss 0.369125128 epoch total loss 0.355782181\n",
      "Trained batch 648 batch loss 0.310146272 epoch total loss 0.355711758\n",
      "Trained batch 649 batch loss 0.326863438 epoch total loss 0.355667293\n",
      "Trained batch 650 batch loss 0.33006835 epoch total loss 0.355627924\n",
      "Trained batch 651 batch loss 0.317936569 epoch total loss 0.355570018\n",
      "Trained batch 652 batch loss 0.360143125 epoch total loss 0.355577022\n",
      "Trained batch 653 batch loss 0.338878304 epoch total loss 0.355551451\n",
      "Trained batch 654 batch loss 0.340536892 epoch total loss 0.355528474\n",
      "Trained batch 655 batch loss 0.313479304 epoch total loss 0.35546428\n",
      "Trained batch 656 batch loss 0.31968233 epoch total loss 0.355409741\n",
      "Trained batch 657 batch loss 0.33124131 epoch total loss 0.355372936\n",
      "Trained batch 658 batch loss 0.332736522 epoch total loss 0.355338544\n",
      "Trained batch 659 batch loss 0.307793409 epoch total loss 0.355266392\n",
      "Trained batch 660 batch loss 0.342236191 epoch total loss 0.355246663\n",
      "Trained batch 661 batch loss 0.319022208 epoch total loss 0.355191857\n",
      "Trained batch 662 batch loss 0.329716176 epoch total loss 0.355153352\n",
      "Trained batch 663 batch loss 0.331770271 epoch total loss 0.355118096\n",
      "Trained batch 664 batch loss 0.317790389 epoch total loss 0.355061889\n",
      "Trained batch 665 batch loss 0.344457239 epoch total loss 0.355045944\n",
      "Trained batch 666 batch loss 0.333272606 epoch total loss 0.355013222\n",
      "Trained batch 667 batch loss 0.373756289 epoch total loss 0.355041325\n",
      "Trained batch 668 batch loss 0.343894899 epoch total loss 0.355024636\n",
      "Trained batch 669 batch loss 0.324406385 epoch total loss 0.354978859\n",
      "Trained batch 670 batch loss 0.334185928 epoch total loss 0.354947805\n",
      "Trained batch 671 batch loss 0.357907474 epoch total loss 0.354952216\n",
      "Trained batch 672 batch loss 0.340685964 epoch total loss 0.354931\n",
      "Trained batch 673 batch loss 0.379155815 epoch total loss 0.354966968\n",
      "Trained batch 674 batch loss 0.369994253 epoch total loss 0.35498929\n",
      "Trained batch 675 batch loss 0.352687776 epoch total loss 0.354985863\n",
      "Trained batch 676 batch loss 0.326202542 epoch total loss 0.354943305\n",
      "Trained batch 677 batch loss 0.31768316 epoch total loss 0.35488826\n",
      "Trained batch 678 batch loss 0.323951185 epoch total loss 0.354842633\n",
      "Trained batch 679 batch loss 0.330454886 epoch total loss 0.354806721\n",
      "Trained batch 680 batch loss 0.325930089 epoch total loss 0.354764253\n",
      "Trained batch 681 batch loss 0.33140415 epoch total loss 0.35472995\n",
      "Trained batch 682 batch loss 0.34073782 epoch total loss 0.354709446\n",
      "Trained batch 683 batch loss 0.369352281 epoch total loss 0.354730874\n",
      "Trained batch 684 batch loss 0.355844796 epoch total loss 0.354732513\n",
      "Trained batch 685 batch loss 0.347561866 epoch total loss 0.354722053\n",
      "Trained batch 686 batch loss 0.333491713 epoch total loss 0.354691118\n",
      "Trained batch 687 batch loss 0.318889081 epoch total loss 0.354639\n",
      "Trained batch 688 batch loss 0.332312167 epoch total loss 0.354606539\n",
      "Trained batch 689 batch loss 0.33442682 epoch total loss 0.354577243\n",
      "Trained batch 690 batch loss 0.323646098 epoch total loss 0.354532421\n",
      "Trained batch 691 batch loss 0.321475089 epoch total loss 0.354484558\n",
      "Trained batch 692 batch loss 0.305406064 epoch total loss 0.354413658\n",
      "Trained batch 693 batch loss 0.305895358 epoch total loss 0.354343623\n",
      "Trained batch 694 batch loss 0.336799949 epoch total loss 0.354318351\n",
      "Trained batch 695 batch loss 0.313057333 epoch total loss 0.354259\n",
      "Trained batch 696 batch loss 0.326933771 epoch total loss 0.354219735\n",
      "Trained batch 697 batch loss 0.322584093 epoch total loss 0.354174346\n",
      "Trained batch 698 batch loss 0.310524911 epoch total loss 0.35411182\n",
      "Trained batch 699 batch loss 0.317486584 epoch total loss 0.354059428\n",
      "Trained batch 700 batch loss 0.323043495 epoch total loss 0.354015142\n",
      "Trained batch 701 batch loss 0.316000283 epoch total loss 0.353960901\n",
      "Trained batch 702 batch loss 0.295431077 epoch total loss 0.353877515\n",
      "Trained batch 703 batch loss 0.289809674 epoch total loss 0.353786379\n",
      "Trained batch 704 batch loss 0.266885668 epoch total loss 0.353662938\n",
      "Trained batch 705 batch loss 0.306282282 epoch total loss 0.353595763\n",
      "Trained batch 706 batch loss 0.346717656 epoch total loss 0.353586\n",
      "Trained batch 707 batch loss 0.333299339 epoch total loss 0.353557289\n",
      "Trained batch 708 batch loss 0.340581477 epoch total loss 0.35353896\n",
      "Trained batch 709 batch loss 0.315311074 epoch total loss 0.353485048\n",
      "Trained batch 710 batch loss 0.310363501 epoch total loss 0.353424311\n",
      "Trained batch 711 batch loss 0.295289427 epoch total loss 0.353342533\n",
      "Trained batch 712 batch loss 0.299317062 epoch total loss 0.353266656\n",
      "Trained batch 713 batch loss 0.316300571 epoch total loss 0.3532148\n",
      "Trained batch 714 batch loss 0.323988259 epoch total loss 0.353173882\n",
      "Trained batch 715 batch loss 0.333356619 epoch total loss 0.353146166\n",
      "Trained batch 716 batch loss 0.305974126 epoch total loss 0.353080273\n",
      "Trained batch 717 batch loss 0.330415606 epoch total loss 0.353048682\n",
      "Trained batch 718 batch loss 0.357192338 epoch total loss 0.353054434\n",
      "Trained batch 719 batch loss 0.341038167 epoch total loss 0.353037715\n",
      "Trained batch 720 batch loss 0.341414303 epoch total loss 0.353021592\n",
      "Trained batch 721 batch loss 0.323590904 epoch total loss 0.352980763\n",
      "Trained batch 722 batch loss 0.314847142 epoch total loss 0.352927953\n",
      "Trained batch 723 batch loss 0.296343654 epoch total loss 0.352849692\n",
      "Trained batch 724 batch loss 0.324176461 epoch total loss 0.352810085\n",
      "Trained batch 725 batch loss 0.32970956 epoch total loss 0.352778226\n",
      "Trained batch 726 batch loss 0.328662 epoch total loss 0.352745\n",
      "Trained batch 727 batch loss 0.303051502 epoch total loss 0.35267663\n",
      "Trained batch 728 batch loss 0.325266093 epoch total loss 0.35263896\n",
      "Trained batch 729 batch loss 0.314081579 epoch total loss 0.352586061\n",
      "Trained batch 730 batch loss 0.313584208 epoch total loss 0.352532655\n",
      "Trained batch 731 batch loss 0.316440523 epoch total loss 0.352483302\n",
      "Trained batch 732 batch loss 0.313354701 epoch total loss 0.352429837\n",
      "Trained batch 733 batch loss 0.329906642 epoch total loss 0.352399081\n",
      "Trained batch 734 batch loss 0.319309771 epoch total loss 0.352354\n",
      "Trained batch 735 batch loss 0.308361053 epoch total loss 0.352294147\n",
      "Trained batch 736 batch loss 0.339237213 epoch total loss 0.352276385\n",
      "Trained batch 737 batch loss 0.314381719 epoch total loss 0.352224976\n",
      "Trained batch 738 batch loss 0.345983624 epoch total loss 0.352216512\n",
      "Trained batch 739 batch loss 0.349967033 epoch total loss 0.352213472\n",
      "Trained batch 740 batch loss 0.345160693 epoch total loss 0.352203935\n",
      "Trained batch 741 batch loss 0.334499925 epoch total loss 0.352180064\n",
      "Trained batch 742 batch loss 0.377288163 epoch total loss 0.352213889\n",
      "Trained batch 743 batch loss 0.38200444 epoch total loss 0.352254\n",
      "Trained batch 744 batch loss 0.34883076 epoch total loss 0.352249384\n",
      "Trained batch 745 batch loss 0.327077329 epoch total loss 0.352215618\n",
      "Trained batch 746 batch loss 0.304659188 epoch total loss 0.352151871\n",
      "Trained batch 747 batch loss 0.279128611 epoch total loss 0.352054089\n",
      "Trained batch 748 batch loss 0.309537947 epoch total loss 0.351997256\n",
      "Trained batch 749 batch loss 0.28938356 epoch total loss 0.351913691\n",
      "Trained batch 750 batch loss 0.282172263 epoch total loss 0.351820678\n",
      "Trained batch 751 batch loss 0.274316 epoch total loss 0.351717472\n",
      "Trained batch 752 batch loss 0.262528539 epoch total loss 0.351598889\n",
      "Trained batch 753 batch loss 0.290238559 epoch total loss 0.351517439\n",
      "Trained batch 754 batch loss 0.304523379 epoch total loss 0.351455122\n",
      "Trained batch 755 batch loss 0.325336576 epoch total loss 0.351420552\n",
      "Trained batch 756 batch loss 0.356146306 epoch total loss 0.35142678\n",
      "Trained batch 757 batch loss 0.356389016 epoch total loss 0.351433337\n",
      "Trained batch 758 batch loss 0.341103464 epoch total loss 0.351419687\n",
      "Trained batch 759 batch loss 0.34441632 epoch total loss 0.351410478\n",
      "Trained batch 760 batch loss 0.335415125 epoch total loss 0.351389438\n",
      "Trained batch 761 batch loss 0.336348981 epoch total loss 0.351369649\n",
      "Trained batch 762 batch loss 0.356600255 epoch total loss 0.351376504\n",
      "Trained batch 763 batch loss 0.338049293 epoch total loss 0.35135904\n",
      "Trained batch 764 batch loss 0.345716923 epoch total loss 0.351351619\n",
      "Trained batch 765 batch loss 0.361742675 epoch total loss 0.351365238\n",
      "Trained batch 766 batch loss 0.334769 epoch total loss 0.351343572\n",
      "Trained batch 767 batch loss 0.321214467 epoch total loss 0.351304322\n",
      "Trained batch 768 batch loss 0.333684266 epoch total loss 0.351281375\n",
      "Trained batch 769 batch loss 0.36966306 epoch total loss 0.351305276\n",
      "Trained batch 770 batch loss 0.377812743 epoch total loss 0.351339668\n",
      "Trained batch 771 batch loss 0.343145162 epoch total loss 0.351329058\n",
      "Trained batch 772 batch loss 0.33827734 epoch total loss 0.35131216\n",
      "Trained batch 773 batch loss 0.323540062 epoch total loss 0.351276249\n",
      "Trained batch 774 batch loss 0.332896411 epoch total loss 0.351252466\n",
      "Trained batch 775 batch loss 0.353128493 epoch total loss 0.35125488\n",
      "Trained batch 776 batch loss 0.36968106 epoch total loss 0.351278633\n",
      "Trained batch 777 batch loss 0.357567787 epoch total loss 0.351286739\n",
      "Trained batch 778 batch loss 0.359371543 epoch total loss 0.35129714\n",
      "Trained batch 779 batch loss 0.302236944 epoch total loss 0.351234168\n",
      "Trained batch 780 batch loss 0.305948377 epoch total loss 0.351176113\n",
      "Trained batch 781 batch loss 0.311052918 epoch total loss 0.351124734\n",
      "Trained batch 782 batch loss 0.328471184 epoch total loss 0.351095766\n",
      "Trained batch 783 batch loss 0.344045162 epoch total loss 0.351086766\n",
      "Trained batch 784 batch loss 0.316322625 epoch total loss 0.35104242\n",
      "Trained batch 785 batch loss 0.335271865 epoch total loss 0.351022333\n",
      "Trained batch 786 batch loss 0.353787482 epoch total loss 0.35102585\n",
      "Trained batch 787 batch loss 0.323124707 epoch total loss 0.350990385\n",
      "Trained batch 788 batch loss 0.325920194 epoch total loss 0.350958586\n",
      "Trained batch 789 batch loss 0.324760228 epoch total loss 0.350925386\n",
      "Trained batch 790 batch loss 0.32352531 epoch total loss 0.350890696\n",
      "Trained batch 791 batch loss 0.313233465 epoch total loss 0.350843072\n",
      "Trained batch 792 batch loss 0.314937294 epoch total loss 0.350797743\n",
      "Trained batch 793 batch loss 0.34314549 epoch total loss 0.350788087\n",
      "Trained batch 794 batch loss 0.345107645 epoch total loss 0.350780934\n",
      "Trained batch 795 batch loss 0.379850417 epoch total loss 0.350817502\n",
      "Trained batch 796 batch loss 0.372564644 epoch total loss 0.3508448\n",
      "Trained batch 797 batch loss 0.374305218 epoch total loss 0.350874215\n",
      "Trained batch 798 batch loss 0.338059902 epoch total loss 0.350858182\n",
      "Trained batch 799 batch loss 0.28847456 epoch total loss 0.350780129\n",
      "Trained batch 800 batch loss 0.291580796 epoch total loss 0.35070613\n",
      "Trained batch 801 batch loss 0.26656431 epoch total loss 0.350601107\n",
      "Trained batch 802 batch loss 0.276595712 epoch total loss 0.350508809\n",
      "Trained batch 803 batch loss 0.31828624 epoch total loss 0.350468695\n",
      "Trained batch 804 batch loss 0.36435318 epoch total loss 0.350485951\n",
      "Trained batch 805 batch loss 0.324312121 epoch total loss 0.350453436\n",
      "Trained batch 806 batch loss 0.288453907 epoch total loss 0.350376517\n",
      "Trained batch 807 batch loss 0.31714195 epoch total loss 0.35033533\n",
      "Trained batch 808 batch loss 0.295799494 epoch total loss 0.350267857\n",
      "Trained batch 809 batch loss 0.326374501 epoch total loss 0.350238323\n",
      "Trained batch 810 batch loss 0.305706352 epoch total loss 0.350183338\n",
      "Trained batch 811 batch loss 0.314050376 epoch total loss 0.350138783\n",
      "Trained batch 812 batch loss 0.329439223 epoch total loss 0.350113302\n",
      "Trained batch 813 batch loss 0.325083733 epoch total loss 0.350082487\n",
      "Trained batch 814 batch loss 0.331218541 epoch total loss 0.350059301\n",
      "Trained batch 815 batch loss 0.362650603 epoch total loss 0.350074738\n",
      "Trained batch 816 batch loss 0.390652418 epoch total loss 0.350124478\n",
      "Trained batch 817 batch loss 0.370555758 epoch total loss 0.350149453\n",
      "Trained batch 818 batch loss 0.390056878 epoch total loss 0.350198239\n",
      "Trained batch 819 batch loss 0.341076225 epoch total loss 0.350187093\n",
      "Trained batch 820 batch loss 0.29531759 epoch total loss 0.350120187\n",
      "Trained batch 821 batch loss 0.335179359 epoch total loss 0.350101978\n",
      "Trained batch 822 batch loss 0.374834 epoch total loss 0.350132078\n",
      "Trained batch 823 batch loss 0.344496667 epoch total loss 0.350125194\n",
      "Trained batch 824 batch loss 0.344613791 epoch total loss 0.350118518\n",
      "Trained batch 825 batch loss 0.289403111 epoch total loss 0.350044906\n",
      "Trained batch 826 batch loss 0.298981607 epoch total loss 0.349983096\n",
      "Trained batch 827 batch loss 0.347749889 epoch total loss 0.349980384\n",
      "Trained batch 828 batch loss 0.32037282 epoch total loss 0.349944621\n",
      "Trained batch 829 batch loss 0.351215661 epoch total loss 0.349946171\n",
      "Trained batch 830 batch loss 0.344239354 epoch total loss 0.349939287\n",
      "Trained batch 831 batch loss 0.392033756 epoch total loss 0.349989951\n",
      "Trained batch 832 batch loss 0.364146352 epoch total loss 0.350006938\n",
      "Trained batch 833 batch loss 0.330001831 epoch total loss 0.349982917\n",
      "Trained batch 834 batch loss 0.360210657 epoch total loss 0.349995166\n",
      "Trained batch 835 batch loss 0.362943143 epoch total loss 0.350010663\n",
      "Trained batch 836 batch loss 0.340002388 epoch total loss 0.349998683\n",
      "Trained batch 837 batch loss 0.325957626 epoch total loss 0.349969983\n",
      "Trained batch 838 batch loss 0.31871137 epoch total loss 0.3499327\n",
      "Trained batch 839 batch loss 0.268859416 epoch total loss 0.349836051\n",
      "Trained batch 840 batch loss 0.300015569 epoch total loss 0.349776745\n",
      "Trained batch 841 batch loss 0.333737552 epoch total loss 0.349757671\n",
      "Trained batch 842 batch loss 0.317985088 epoch total loss 0.349719942\n",
      "Trained batch 843 batch loss 0.327987283 epoch total loss 0.349694163\n",
      "Trained batch 844 batch loss 0.372123957 epoch total loss 0.349720746\n",
      "Trained batch 845 batch loss 0.33739391 epoch total loss 0.349706173\n",
      "Trained batch 846 batch loss 0.306612134 epoch total loss 0.349655211\n",
      "Trained batch 847 batch loss 0.344591379 epoch total loss 0.349649251\n",
      "Trained batch 848 batch loss 0.316945612 epoch total loss 0.349610716\n",
      "Trained batch 849 batch loss 0.318989336 epoch total loss 0.349574655\n",
      "Trained batch 850 batch loss 0.32185784 epoch total loss 0.349542052\n",
      "Trained batch 851 batch loss 0.3409684 epoch total loss 0.349531978\n",
      "Trained batch 852 batch loss 0.353902459 epoch total loss 0.349537134\n",
      "Trained batch 853 batch loss 0.338037819 epoch total loss 0.349523664\n",
      "Trained batch 854 batch loss 0.337547779 epoch total loss 0.349509627\n",
      "Trained batch 855 batch loss 0.335945576 epoch total loss 0.349493772\n",
      "Trained batch 856 batch loss 0.345975578 epoch total loss 0.349489659\n",
      "Trained batch 857 batch loss 0.326286316 epoch total loss 0.349462599\n",
      "Trained batch 858 batch loss 0.32629928 epoch total loss 0.349435598\n",
      "Trained batch 859 batch loss 0.333983511 epoch total loss 0.349417597\n",
      "Trained batch 860 batch loss 0.314079612 epoch total loss 0.349376529\n",
      "Trained batch 861 batch loss 0.290240854 epoch total loss 0.349307835\n",
      "Trained batch 862 batch loss 0.307904333 epoch total loss 0.349259794\n",
      "Trained batch 863 batch loss 0.317970455 epoch total loss 0.349223524\n",
      "Trained batch 864 batch loss 0.32100141 epoch total loss 0.349190891\n",
      "Trained batch 865 batch loss 0.323061258 epoch total loss 0.349160671\n",
      "Trained batch 866 batch loss 0.302330792 epoch total loss 0.34910661\n",
      "Trained batch 867 batch loss 0.32128948 epoch total loss 0.349074513\n",
      "Trained batch 868 batch loss 0.310144037 epoch total loss 0.34902969\n",
      "Trained batch 869 batch loss 0.314066678 epoch total loss 0.348989427\n",
      "Trained batch 870 batch loss 0.308501124 epoch total loss 0.348942906\n",
      "Trained batch 871 batch loss 0.312667459 epoch total loss 0.348901242\n",
      "Trained batch 872 batch loss 0.316315532 epoch total loss 0.34886387\n",
      "Trained batch 873 batch loss 0.324247956 epoch total loss 0.348835677\n",
      "Trained batch 874 batch loss 0.350099027 epoch total loss 0.348837107\n",
      "Trained batch 875 batch loss 0.356491923 epoch total loss 0.348845869\n",
      "Trained batch 876 batch loss 0.34304139 epoch total loss 0.348839253\n",
      "Trained batch 877 batch loss 0.335274607 epoch total loss 0.348823786\n",
      "Trained batch 878 batch loss 0.328951567 epoch total loss 0.348801136\n",
      "Trained batch 879 batch loss 0.323367983 epoch total loss 0.348772198\n",
      "Trained batch 880 batch loss 0.27093479 epoch total loss 0.348683745\n",
      "Trained batch 881 batch loss 0.287103981 epoch total loss 0.348613858\n",
      "Trained batch 882 batch loss 0.331888676 epoch total loss 0.348594904\n",
      "Trained batch 883 batch loss 0.319751263 epoch total loss 0.348562241\n",
      "Trained batch 884 batch loss 0.356277585 epoch total loss 0.348570973\n",
      "Trained batch 885 batch loss 0.320498139 epoch total loss 0.348539263\n",
      "Trained batch 886 batch loss 0.311136693 epoch total loss 0.348497033\n",
      "Trained batch 887 batch loss 0.335017264 epoch total loss 0.348481834\n",
      "Trained batch 888 batch loss 0.314222485 epoch total loss 0.34844324\n",
      "Trained batch 889 batch loss 0.292884707 epoch total loss 0.348380744\n",
      "Trained batch 890 batch loss 0.306393206 epoch total loss 0.348333567\n",
      "Trained batch 891 batch loss 0.321332693 epoch total loss 0.348303258\n",
      "Trained batch 892 batch loss 0.288716137 epoch total loss 0.348236471\n",
      "Trained batch 893 batch loss 0.324522495 epoch total loss 0.348209918\n",
      "Trained batch 894 batch loss 0.340224564 epoch total loss 0.348200947\n",
      "Trained batch 895 batch loss 0.356445044 epoch total loss 0.348210156\n",
      "Trained batch 896 batch loss 0.330540925 epoch total loss 0.348190457\n",
      "Trained batch 897 batch loss 0.307778329 epoch total loss 0.348145396\n",
      "Trained batch 898 batch loss 0.285672575 epoch total loss 0.348075807\n",
      "Trained batch 899 batch loss 0.289109081 epoch total loss 0.348010242\n",
      "Trained batch 900 batch loss 0.309596658 epoch total loss 0.347967565\n",
      "Trained batch 901 batch loss 0.331353307 epoch total loss 0.347949117\n",
      "Trained batch 902 batch loss 0.346604466 epoch total loss 0.347947657\n",
      "Trained batch 903 batch loss 0.353611737 epoch total loss 0.347953916\n",
      "Trained batch 904 batch loss 0.357437104 epoch total loss 0.347964406\n",
      "Trained batch 905 batch loss 0.327473909 epoch total loss 0.347941756\n",
      "Trained batch 906 batch loss 0.325507 epoch total loss 0.347917\n",
      "Trained batch 907 batch loss 0.337232023 epoch total loss 0.347905189\n",
      "Trained batch 908 batch loss 0.30297491 epoch total loss 0.347855717\n",
      "Trained batch 909 batch loss 0.323413551 epoch total loss 0.347828835\n",
      "Trained batch 910 batch loss 0.326044083 epoch total loss 0.347804904\n",
      "Trained batch 911 batch loss 0.326564133 epoch total loss 0.347781599\n",
      "Trained batch 912 batch loss 0.324935496 epoch total loss 0.347756535\n",
      "Trained batch 913 batch loss 0.301655 epoch total loss 0.34770605\n",
      "Trained batch 914 batch loss 0.284208298 epoch total loss 0.34763658\n",
      "Trained batch 915 batch loss 0.275962532 epoch total loss 0.34755826\n",
      "Trained batch 916 batch loss 0.328104526 epoch total loss 0.347537\n",
      "Trained batch 917 batch loss 0.29783532 epoch total loss 0.3474828\n",
      "Trained batch 918 batch loss 0.349288344 epoch total loss 0.347484738\n",
      "Trained batch 919 batch loss 0.34375605 epoch total loss 0.347480685\n",
      "Trained batch 920 batch loss 0.321707129 epoch total loss 0.34745267\n",
      "Trained batch 921 batch loss 0.32915473 epoch total loss 0.347432822\n",
      "Trained batch 922 batch loss 0.31762445 epoch total loss 0.347400486\n",
      "Trained batch 923 batch loss 0.314126253 epoch total loss 0.347364426\n",
      "Trained batch 924 batch loss 0.339058429 epoch total loss 0.347355425\n",
      "Trained batch 925 batch loss 0.323162556 epoch total loss 0.347329259\n",
      "Trained batch 926 batch loss 0.276878834 epoch total loss 0.347253203\n",
      "Trained batch 927 batch loss 0.247698516 epoch total loss 0.347145826\n",
      "Trained batch 928 batch loss 0.267337233 epoch total loss 0.347059816\n",
      "Trained batch 929 batch loss 0.319474906 epoch total loss 0.347030133\n",
      "Trained batch 930 batch loss 0.374530882 epoch total loss 0.347059727\n",
      "Trained batch 931 batch loss 0.379814148 epoch total loss 0.347094893\n",
      "Trained batch 932 batch loss 0.361560553 epoch total loss 0.34711045\n",
      "Trained batch 933 batch loss 0.342115909 epoch total loss 0.347105056\n",
      "Trained batch 934 batch loss 0.342199534 epoch total loss 0.347099811\n",
      "Trained batch 935 batch loss 0.305481404 epoch total loss 0.347055286\n",
      "Trained batch 936 batch loss 0.305312872 epoch total loss 0.347010672\n",
      "Trained batch 937 batch loss 0.311094165 epoch total loss 0.346972346\n",
      "Trained batch 938 batch loss 0.315087676 epoch total loss 0.346938372\n",
      "Trained batch 939 batch loss 0.310958713 epoch total loss 0.346900046\n",
      "Trained batch 940 batch loss 0.307569206 epoch total loss 0.346858174\n",
      "Trained batch 941 batch loss 0.282647 epoch total loss 0.346789956\n",
      "Trained batch 942 batch loss 0.329185843 epoch total loss 0.34677127\n",
      "Trained batch 943 batch loss 0.334873796 epoch total loss 0.346758664\n",
      "Trained batch 944 batch loss 0.341657877 epoch total loss 0.34675324\n",
      "Trained batch 945 batch loss 0.356954247 epoch total loss 0.346764028\n",
      "Trained batch 946 batch loss 0.305514038 epoch total loss 0.346720427\n",
      "Trained batch 947 batch loss 0.319418132 epoch total loss 0.346691608\n",
      "Trained batch 948 batch loss 0.307894349 epoch total loss 0.34665069\n",
      "Trained batch 949 batch loss 0.344559789 epoch total loss 0.346648484\n",
      "Trained batch 950 batch loss 0.330278873 epoch total loss 0.346631289\n",
      "Trained batch 951 batch loss 0.332009 epoch total loss 0.346615881\n",
      "Trained batch 952 batch loss 0.333912909 epoch total loss 0.346602559\n",
      "Trained batch 953 batch loss 0.307409555 epoch total loss 0.346561432\n",
      "Trained batch 954 batch loss 0.315062523 epoch total loss 0.346528411\n",
      "Trained batch 955 batch loss 0.316019416 epoch total loss 0.346496463\n",
      "Trained batch 956 batch loss 0.31429556 epoch total loss 0.346462786\n",
      "Trained batch 957 batch loss 0.339771748 epoch total loss 0.346455783\n",
      "Trained batch 958 batch loss 0.328001678 epoch total loss 0.34643653\n",
      "Trained batch 959 batch loss 0.319271535 epoch total loss 0.346408218\n",
      "Trained batch 960 batch loss 0.337018728 epoch total loss 0.346398413\n",
      "Trained batch 961 batch loss 0.339530051 epoch total loss 0.34639129\n",
      "Trained batch 962 batch loss 0.310450137 epoch total loss 0.346353918\n",
      "Trained batch 963 batch loss 0.310783505 epoch total loss 0.346317\n",
      "Trained batch 964 batch loss 0.316390574 epoch total loss 0.346285939\n",
      "Trained batch 965 batch loss 0.343247294 epoch total loss 0.34628281\n",
      "Trained batch 966 batch loss 0.331352443 epoch total loss 0.346267343\n",
      "Trained batch 967 batch loss 0.351609647 epoch total loss 0.346272886\n",
      "Trained batch 968 batch loss 0.348007679 epoch total loss 0.346274704\n",
      "Trained batch 969 batch loss 0.346951783 epoch total loss 0.346275389\n",
      "Trained batch 970 batch loss 0.34034586 epoch total loss 0.34626928\n",
      "Trained batch 971 batch loss 0.31538716 epoch total loss 0.346237481\n",
      "Trained batch 972 batch loss 0.326378316 epoch total loss 0.346217066\n",
      "Trained batch 973 batch loss 0.310622483 epoch total loss 0.346180469\n",
      "Trained batch 974 batch loss 0.315117896 epoch total loss 0.34614858\n",
      "Trained batch 975 batch loss 0.30185625 epoch total loss 0.346103132\n",
      "Trained batch 976 batch loss 0.332820535 epoch total loss 0.346089542\n",
      "Trained batch 977 batch loss 0.312125981 epoch total loss 0.346054792\n",
      "Trained batch 978 batch loss 0.289815873 epoch total loss 0.345997274\n",
      "Trained batch 979 batch loss 0.3023009 epoch total loss 0.34595266\n",
      "Trained batch 980 batch loss 0.322318614 epoch total loss 0.34592855\n",
      "Trained batch 981 batch loss 0.324591815 epoch total loss 0.345906794\n",
      "Trained batch 982 batch loss 0.333988607 epoch total loss 0.345894665\n",
      "Trained batch 983 batch loss 0.396672428 epoch total loss 0.345946312\n",
      "Trained batch 984 batch loss 0.401225597 epoch total loss 0.34600246\n",
      "Trained batch 985 batch loss 0.371650159 epoch total loss 0.346028507\n",
      "Trained batch 986 batch loss 0.329179347 epoch total loss 0.34601143\n",
      "Trained batch 987 batch loss 0.322500885 epoch total loss 0.345987618\n",
      "Trained batch 988 batch loss 0.347036272 epoch total loss 0.345988691\n",
      "Trained batch 989 batch loss 0.33908388 epoch total loss 0.345981687\n",
      "Trained batch 990 batch loss 0.343457431 epoch total loss 0.345979154\n",
      "Trained batch 991 batch loss 0.301518768 epoch total loss 0.345934272\n",
      "Trained batch 992 batch loss 0.332381874 epoch total loss 0.345920593\n",
      "Trained batch 993 batch loss 0.33017236 epoch total loss 0.345904738\n",
      "Trained batch 994 batch loss 0.345210135 epoch total loss 0.345904052\n",
      "Trained batch 995 batch loss 0.322879285 epoch total loss 0.345880896\n",
      "Trained batch 996 batch loss 0.322213 epoch total loss 0.345857114\n",
      "Trained batch 997 batch loss 0.326875329 epoch total loss 0.34583807\n",
      "Trained batch 998 batch loss 0.340326637 epoch total loss 0.345832556\n",
      "Trained batch 999 batch loss 0.349335968 epoch total loss 0.345836073\n",
      "Trained batch 1000 batch loss 0.334454268 epoch total loss 0.345824689\n",
      "Trained batch 1001 batch loss 0.343362391 epoch total loss 0.345822215\n",
      "Trained batch 1002 batch loss 0.323297888 epoch total loss 0.345799744\n",
      "Trained batch 1003 batch loss 0.352437615 epoch total loss 0.34580636\n",
      "Trained batch 1004 batch loss 0.317925721 epoch total loss 0.345778584\n",
      "Trained batch 1005 batch loss 0.327292651 epoch total loss 0.345760226\n",
      "Trained batch 1006 batch loss 0.307079405 epoch total loss 0.345721751\n",
      "Trained batch 1007 batch loss 0.287482709 epoch total loss 0.345663905\n",
      "Trained batch 1008 batch loss 0.29981041 epoch total loss 0.345618427\n",
      "Trained batch 1009 batch loss 0.278941154 epoch total loss 0.345552325\n",
      "Trained batch 1010 batch loss 0.305965841 epoch total loss 0.345513135\n",
      "Trained batch 1011 batch loss 0.261735648 epoch total loss 0.345430285\n",
      "Trained batch 1012 batch loss 0.251954168 epoch total loss 0.345337898\n",
      "Trained batch 1013 batch loss 0.255244583 epoch total loss 0.345248967\n",
      "Trained batch 1014 batch loss 0.269313753 epoch total loss 0.345174104\n",
      "Trained batch 1015 batch loss 0.325540066 epoch total loss 0.345154732\n",
      "Trained batch 1016 batch loss 0.329627603 epoch total loss 0.345139444\n",
      "Trained batch 1017 batch loss 0.326430023 epoch total loss 0.345121026\n",
      "Trained batch 1018 batch loss 0.293419898 epoch total loss 0.345070273\n",
      "Trained batch 1019 batch loss 0.320065022 epoch total loss 0.345045716\n",
      "Trained batch 1020 batch loss 0.350793153 epoch total loss 0.345051378\n",
      "Trained batch 1021 batch loss 0.341192245 epoch total loss 0.345047593\n",
      "Trained batch 1022 batch loss 0.320629239 epoch total loss 0.345023662\n",
      "Trained batch 1023 batch loss 0.317239404 epoch total loss 0.344996512\n",
      "Trained batch 1024 batch loss 0.364620477 epoch total loss 0.345015675\n",
      "Trained batch 1025 batch loss 0.354494333 epoch total loss 0.345024914\n",
      "Trained batch 1026 batch loss 0.332989573 epoch total loss 0.345013171\n",
      "Trained batch 1027 batch loss 0.323858321 epoch total loss 0.344992578\n",
      "Trained batch 1028 batch loss 0.310238242 epoch total loss 0.344958782\n",
      "Trained batch 1029 batch loss 0.333429903 epoch total loss 0.344947577\n",
      "Trained batch 1030 batch loss 0.355287194 epoch total loss 0.34495762\n",
      "Trained batch 1031 batch loss 0.343356907 epoch total loss 0.34495604\n",
      "Trained batch 1032 batch loss 0.369982898 epoch total loss 0.344980299\n",
      "Trained batch 1033 batch loss 0.383534193 epoch total loss 0.345017642\n",
      "Trained batch 1034 batch loss 0.35174042 epoch total loss 0.345024139\n",
      "Trained batch 1035 batch loss 0.35229671 epoch total loss 0.345031172\n",
      "Trained batch 1036 batch loss 0.336614698 epoch total loss 0.345023036\n",
      "Trained batch 1037 batch loss 0.344181955 epoch total loss 0.345022231\n",
      "Trained batch 1038 batch loss 0.376987547 epoch total loss 0.345053017\n",
      "Trained batch 1039 batch loss 0.359067202 epoch total loss 0.345066518\n",
      "Trained batch 1040 batch loss 0.306370795 epoch total loss 0.345029294\n",
      "Trained batch 1041 batch loss 0.305955648 epoch total loss 0.344991773\n",
      "Trained batch 1042 batch loss 0.289196908 epoch total loss 0.344938219\n",
      "Trained batch 1043 batch loss 0.319402 epoch total loss 0.344913721\n",
      "Trained batch 1044 batch loss 0.321211934 epoch total loss 0.344891\n",
      "Trained batch 1045 batch loss 0.352609426 epoch total loss 0.344898403\n",
      "Trained batch 1046 batch loss 0.32669884 epoch total loss 0.344881\n",
      "Trained batch 1047 batch loss 0.329415411 epoch total loss 0.344866216\n",
      "Trained batch 1048 batch loss 0.325263351 epoch total loss 0.3448475\n",
      "Trained batch 1049 batch loss 0.33823204 epoch total loss 0.344841182\n",
      "Trained batch 1050 batch loss 0.316367298 epoch total loss 0.344814062\n",
      "Trained batch 1051 batch loss 0.322838962 epoch total loss 0.344793171\n",
      "Trained batch 1052 batch loss 0.304430842 epoch total loss 0.344754815\n",
      "Trained batch 1053 batch loss 0.298793018 epoch total loss 0.344711185\n",
      "Trained batch 1054 batch loss 0.305939019 epoch total loss 0.344674379\n",
      "Trained batch 1055 batch loss 0.314947307 epoch total loss 0.344646215\n",
      "Trained batch 1056 batch loss 0.373333275 epoch total loss 0.344673365\n",
      "Trained batch 1057 batch loss 0.348831177 epoch total loss 0.344677299\n",
      "Trained batch 1058 batch loss 0.318502754 epoch total loss 0.344652563\n",
      "Trained batch 1059 batch loss 0.304823726 epoch total loss 0.344614953\n",
      "Trained batch 1060 batch loss 0.275383919 epoch total loss 0.344549656\n",
      "Trained batch 1061 batch loss 0.274757534 epoch total loss 0.344483852\n",
      "Trained batch 1062 batch loss 0.300217479 epoch total loss 0.344442189\n",
      "Trained batch 1063 batch loss 0.297229499 epoch total loss 0.344397783\n",
      "Trained batch 1064 batch loss 0.28317669 epoch total loss 0.344340235\n",
      "Trained batch 1065 batch loss 0.302410245 epoch total loss 0.344300866\n",
      "Trained batch 1066 batch loss 0.291279078 epoch total loss 0.344251126\n",
      "Trained batch 1067 batch loss 0.289579153 epoch total loss 0.344199896\n",
      "Trained batch 1068 batch loss 0.307557583 epoch total loss 0.344165593\n",
      "Trained batch 1069 batch loss 0.287356794 epoch total loss 0.344112456\n",
      "Trained batch 1070 batch loss 0.312388778 epoch total loss 0.344082773\n",
      "Trained batch 1071 batch loss 0.294175774 epoch total loss 0.344036192\n",
      "Trained batch 1072 batch loss 0.306631386 epoch total loss 0.344001323\n",
      "Trained batch 1073 batch loss 0.304930776 epoch total loss 0.343964905\n",
      "Trained batch 1074 batch loss 0.292639583 epoch total loss 0.343917102\n",
      "Trained batch 1075 batch loss 0.296865523 epoch total loss 0.343873352\n",
      "Trained batch 1076 batch loss 0.331441402 epoch total loss 0.343861789\n",
      "Trained batch 1077 batch loss 0.356669754 epoch total loss 0.34387368\n",
      "Trained batch 1078 batch loss 0.335751176 epoch total loss 0.34386614\n",
      "Trained batch 1079 batch loss 0.348340571 epoch total loss 0.343870282\n",
      "Trained batch 1080 batch loss 0.349491537 epoch total loss 0.343875498\n",
      "Trained batch 1081 batch loss 0.375670582 epoch total loss 0.343904912\n",
      "Trained batch 1082 batch loss 0.329298705 epoch total loss 0.343891382\n",
      "Trained batch 1083 batch loss 0.32598269 epoch total loss 0.343874872\n",
      "Trained batch 1084 batch loss 0.339878 epoch total loss 0.343871176\n",
      "Trained batch 1085 batch loss 0.320615172 epoch total loss 0.343849748\n",
      "Trained batch 1086 batch loss 0.319826931 epoch total loss 0.343827605\n",
      "Trained batch 1087 batch loss 0.334234208 epoch total loss 0.343818784\n",
      "Trained batch 1088 batch loss 0.331430674 epoch total loss 0.343807399\n",
      "Trained batch 1089 batch loss 0.300714 epoch total loss 0.343767822\n",
      "Trained batch 1090 batch loss 0.31895861 epoch total loss 0.343745083\n",
      "Trained batch 1091 batch loss 0.329621911 epoch total loss 0.343732119\n",
      "Trained batch 1092 batch loss 0.339065343 epoch total loss 0.343727827\n",
      "Trained batch 1093 batch loss 0.305955529 epoch total loss 0.343693286\n",
      "Trained batch 1094 batch loss 0.330773771 epoch total loss 0.343681484\n",
      "Trained batch 1095 batch loss 0.338510633 epoch total loss 0.343676746\n",
      "Trained batch 1096 batch loss 0.314769983 epoch total loss 0.343650371\n",
      "Trained batch 1097 batch loss 0.325441241 epoch total loss 0.343633771\n",
      "Trained batch 1098 batch loss 0.332122684 epoch total loss 0.343623281\n",
      "Trained batch 1099 batch loss 0.324813187 epoch total loss 0.343606174\n",
      "Trained batch 1100 batch loss 0.343535841 epoch total loss 0.343606085\n",
      "Trained batch 1101 batch loss 0.342301488 epoch total loss 0.343604922\n",
      "Trained batch 1102 batch loss 0.311198771 epoch total loss 0.343575507\n",
      "Trained batch 1103 batch loss 0.325028 epoch total loss 0.343558699\n",
      "Trained batch 1104 batch loss 0.319131017 epoch total loss 0.343536556\n",
      "Trained batch 1105 batch loss 0.327717662 epoch total loss 0.343522251\n",
      "Trained batch 1106 batch loss 0.315334558 epoch total loss 0.34349677\n",
      "Trained batch 1107 batch loss 0.311286867 epoch total loss 0.343467683\n",
      "Trained batch 1108 batch loss 0.313237667 epoch total loss 0.343440384\n",
      "Trained batch 1109 batch loss 0.310827 epoch total loss 0.343410969\n",
      "Trained batch 1110 batch loss 0.302241504 epoch total loss 0.343373895\n",
      "Trained batch 1111 batch loss 0.309015661 epoch total loss 0.34334296\n",
      "Trained batch 1112 batch loss 0.314969242 epoch total loss 0.343317449\n",
      "Trained batch 1113 batch loss 0.290363699 epoch total loss 0.343269885\n",
      "Trained batch 1114 batch loss 0.293014646 epoch total loss 0.343224794\n",
      "Trained batch 1115 batch loss 0.315280795 epoch total loss 0.34319973\n",
      "Trained batch 1116 batch loss 0.29210481 epoch total loss 0.343153954\n",
      "Trained batch 1117 batch loss 0.317917049 epoch total loss 0.343131363\n",
      "Trained batch 1118 batch loss 0.292091668 epoch total loss 0.343085706\n",
      "Trained batch 1119 batch loss 0.270818889 epoch total loss 0.343021125\n",
      "Trained batch 1120 batch loss 0.318776786 epoch total loss 0.342999488\n",
      "Trained batch 1121 batch loss 0.301286042 epoch total loss 0.342962295\n",
      "Trained batch 1122 batch loss 0.308974832 epoch total loss 0.342932\n",
      "Trained batch 1123 batch loss 0.326839566 epoch total loss 0.342917651\n",
      "Trained batch 1124 batch loss 0.340115577 epoch total loss 0.342915177\n",
      "Trained batch 1125 batch loss 0.351789057 epoch total loss 0.342923045\n",
      "Trained batch 1126 batch loss 0.345462352 epoch total loss 0.34292528\n",
      "Trained batch 1127 batch loss 0.32083 epoch total loss 0.3429057\n",
      "Trained batch 1128 batch loss 0.323074967 epoch total loss 0.342888117\n",
      "Trained batch 1129 batch loss 0.3520208 epoch total loss 0.342896223\n",
      "Trained batch 1130 batch loss 0.322332621 epoch total loss 0.342878\n",
      "Trained batch 1131 batch loss 0.324648738 epoch total loss 0.342861891\n",
      "Trained batch 1132 batch loss 0.337859839 epoch total loss 0.34285748\n",
      "Trained batch 1133 batch loss 0.278100878 epoch total loss 0.342800319\n",
      "Trained batch 1134 batch loss 0.28460151 epoch total loss 0.342749\n",
      "Trained batch 1135 batch loss 0.31507805 epoch total loss 0.342724621\n",
      "Trained batch 1136 batch loss 0.345671326 epoch total loss 0.342727214\n",
      "Trained batch 1137 batch loss 0.34669733 epoch total loss 0.342730701\n",
      "Trained batch 1138 batch loss 0.333564222 epoch total loss 0.342722654\n",
      "Trained batch 1139 batch loss 0.371823788 epoch total loss 0.342748195\n",
      "Trained batch 1140 batch loss 0.339001924 epoch total loss 0.342744887\n",
      "Trained batch 1141 batch loss 0.342047036 epoch total loss 0.342744291\n",
      "Trained batch 1142 batch loss 0.338079423 epoch total loss 0.342740208\n",
      "Trained batch 1143 batch loss 0.351019025 epoch total loss 0.34274742\n",
      "Trained batch 1144 batch loss 0.318635404 epoch total loss 0.34272635\n",
      "Trained batch 1145 batch loss 0.342738152 epoch total loss 0.34272638\n",
      "Trained batch 1146 batch loss 0.337521225 epoch total loss 0.34272182\n",
      "Trained batch 1147 batch loss 0.325893 epoch total loss 0.342707157\n",
      "Trained batch 1148 batch loss 0.317261428 epoch total loss 0.342684984\n",
      "Trained batch 1149 batch loss 0.320700765 epoch total loss 0.342665881\n",
      "Trained batch 1150 batch loss 0.301657557 epoch total loss 0.342630208\n",
      "Trained batch 1151 batch loss 0.293831468 epoch total loss 0.342587799\n",
      "Trained batch 1152 batch loss 0.311383545 epoch total loss 0.342560709\n",
      "Trained batch 1153 batch loss 0.297850072 epoch total loss 0.342521936\n",
      "Trained batch 1154 batch loss 0.319583118 epoch total loss 0.342502058\n",
      "Trained batch 1155 batch loss 0.330666721 epoch total loss 0.342491806\n",
      "Trained batch 1156 batch loss 0.315543175 epoch total loss 0.3424685\n",
      "Trained batch 1157 batch loss 0.327880412 epoch total loss 0.342455894\n",
      "Trained batch 1158 batch loss 0.334791183 epoch total loss 0.342449248\n",
      "Trained batch 1159 batch loss 0.323638856 epoch total loss 0.342433035\n",
      "Trained batch 1160 batch loss 0.336572528 epoch total loss 0.342427969\n",
      "Trained batch 1161 batch loss 0.347393572 epoch total loss 0.342432261\n",
      "Trained batch 1162 batch loss 0.297394544 epoch total loss 0.342393488\n",
      "Trained batch 1163 batch loss 0.285877019 epoch total loss 0.34234491\n",
      "Trained batch 1164 batch loss 0.3263385 epoch total loss 0.342331141\n",
      "Trained batch 1165 batch loss 0.340380847 epoch total loss 0.342329472\n",
      "Trained batch 1166 batch loss 0.331037343 epoch total loss 0.342319787\n",
      "Trained batch 1167 batch loss 0.342119 epoch total loss 0.342319608\n",
      "Trained batch 1168 batch loss 0.32532391 epoch total loss 0.342305064\n",
      "Trained batch 1169 batch loss 0.30631125 epoch total loss 0.342274278\n",
      "Trained batch 1170 batch loss 0.313156188 epoch total loss 0.342249393\n",
      "Trained batch 1171 batch loss 0.28871119 epoch total loss 0.342203677\n",
      "Trained batch 1172 batch loss 0.307182223 epoch total loss 0.342173785\n",
      "Trained batch 1173 batch loss 0.310854048 epoch total loss 0.342147082\n",
      "Trained batch 1174 batch loss 0.324974835 epoch total loss 0.342132479\n",
      "Trained batch 1175 batch loss 0.340023935 epoch total loss 0.342130661\n",
      "Trained batch 1176 batch loss 0.33059153 epoch total loss 0.342120856\n",
      "Trained batch 1177 batch loss 0.356820464 epoch total loss 0.342133343\n",
      "Trained batch 1178 batch loss 0.321864694 epoch total loss 0.342116147\n",
      "Trained batch 1179 batch loss 0.296018064 epoch total loss 0.342077047\n",
      "Trained batch 1180 batch loss 0.293454081 epoch total loss 0.34203583\n",
      "Trained batch 1181 batch loss 0.314816266 epoch total loss 0.342012793\n",
      "Trained batch 1182 batch loss 0.318711519 epoch total loss 0.341993093\n",
      "Trained batch 1183 batch loss 0.305287778 epoch total loss 0.341962069\n",
      "Trained batch 1184 batch loss 0.361809522 epoch total loss 0.341978848\n",
      "Trained batch 1185 batch loss 0.34460631 epoch total loss 0.341981053\n",
      "Trained batch 1186 batch loss 0.333421528 epoch total loss 0.341973871\n",
      "Trained batch 1187 batch loss 0.34354 epoch total loss 0.341975182\n",
      "Trained batch 1188 batch loss 0.351904482 epoch total loss 0.341983527\n",
      "Trained batch 1189 batch loss 0.356318414 epoch total loss 0.341995597\n",
      "Trained batch 1190 batch loss 0.342885524 epoch total loss 0.341996342\n",
      "Trained batch 1191 batch loss 0.362462401 epoch total loss 0.342013508\n",
      "Trained batch 1192 batch loss 0.334874451 epoch total loss 0.342007518\n",
      "Trained batch 1193 batch loss 0.34439683 epoch total loss 0.342009515\n",
      "Trained batch 1194 batch loss 0.334288895 epoch total loss 0.342003047\n",
      "Trained batch 1195 batch loss 0.324395895 epoch total loss 0.341988325\n",
      "Trained batch 1196 batch loss 0.305928946 epoch total loss 0.341958195\n",
      "Trained batch 1197 batch loss 0.30416432 epoch total loss 0.341926605\n",
      "Trained batch 1198 batch loss 0.284446269 epoch total loss 0.341878653\n",
      "Trained batch 1199 batch loss 0.299989462 epoch total loss 0.341843694\n",
      "Trained batch 1200 batch loss 0.337607384 epoch total loss 0.341840178\n",
      "Trained batch 1201 batch loss 0.354635954 epoch total loss 0.341850847\n",
      "Trained batch 1202 batch loss 0.351256549 epoch total loss 0.341858685\n",
      "Trained batch 1203 batch loss 0.384618789 epoch total loss 0.341894209\n",
      "Trained batch 1204 batch loss 0.363215059 epoch total loss 0.341911912\n",
      "Trained batch 1205 batch loss 0.34697026 epoch total loss 0.341916144\n",
      "Trained batch 1206 batch loss 0.322459638 epoch total loss 0.3419\n",
      "Trained batch 1207 batch loss 0.302485198 epoch total loss 0.341867328\n",
      "Trained batch 1208 batch loss 0.309604228 epoch total loss 0.341840625\n",
      "Trained batch 1209 batch loss 0.295517683 epoch total loss 0.341802329\n",
      "Trained batch 1210 batch loss 0.325253934 epoch total loss 0.34178865\n",
      "Trained batch 1211 batch loss 0.332189441 epoch total loss 0.341780722\n",
      "Trained batch 1212 batch loss 0.329266608 epoch total loss 0.341770381\n",
      "Trained batch 1213 batch loss 0.309173167 epoch total loss 0.341743499\n",
      "Trained batch 1214 batch loss 0.314949453 epoch total loss 0.341721445\n",
      "Trained batch 1215 batch loss 0.346567363 epoch total loss 0.341725409\n",
      "Trained batch 1216 batch loss 0.308897078 epoch total loss 0.341698408\n",
      "Trained batch 1217 batch loss 0.321236402 epoch total loss 0.3416816\n",
      "Trained batch 1218 batch loss 0.348134637 epoch total loss 0.341686904\n",
      "Trained batch 1219 batch loss 0.313382208 epoch total loss 0.341663688\n",
      "Trained batch 1220 batch loss 0.330646753 epoch total loss 0.341654658\n",
      "Trained batch 1221 batch loss 0.335106 epoch total loss 0.341649324\n",
      "Trained batch 1222 batch loss 0.31897974 epoch total loss 0.341630757\n",
      "Trained batch 1223 batch loss 0.294835865 epoch total loss 0.341592491\n",
      "Trained batch 1224 batch loss 0.312460661 epoch total loss 0.341568679\n",
      "Trained batch 1225 batch loss 0.31125319 epoch total loss 0.341543943\n",
      "Trained batch 1226 batch loss 0.356905282 epoch total loss 0.34155646\n",
      "Trained batch 1227 batch loss 0.345874876 epoch total loss 0.34156\n",
      "Trained batch 1228 batch loss 0.316525847 epoch total loss 0.341539621\n",
      "Trained batch 1229 batch loss 0.308969557 epoch total loss 0.341513097\n",
      "Trained batch 1230 batch loss 0.315843135 epoch total loss 0.341492236\n",
      "Trained batch 1231 batch loss 0.324730366 epoch total loss 0.341478646\n",
      "Trained batch 1232 batch loss 0.277534902 epoch total loss 0.34142673\n",
      "Trained batch 1233 batch loss 0.314610064 epoch total loss 0.341404974\n",
      "Trained batch 1234 batch loss 0.338025898 epoch total loss 0.341402233\n",
      "Trained batch 1235 batch loss 0.347436905 epoch total loss 0.34140712\n",
      "Trained batch 1236 batch loss 0.350238621 epoch total loss 0.341414273\n",
      "Trained batch 1237 batch loss 0.364997089 epoch total loss 0.341433316\n",
      "Trained batch 1238 batch loss 0.317119747 epoch total loss 0.341413677\n",
      "Trained batch 1239 batch loss 0.339425266 epoch total loss 0.341412067\n",
      "Trained batch 1240 batch loss 0.349093527 epoch total loss 0.341418266\n",
      "Trained batch 1241 batch loss 0.356643587 epoch total loss 0.341430515\n",
      "Trained batch 1242 batch loss 0.337975472 epoch total loss 0.341427743\n",
      "Trained batch 1243 batch loss 0.29936716 epoch total loss 0.341393918\n",
      "Trained batch 1244 batch loss 0.322842389 epoch total loss 0.341379\n",
      "Trained batch 1245 batch loss 0.340881854 epoch total loss 0.341378599\n",
      "Trained batch 1246 batch loss 0.310222477 epoch total loss 0.341353595\n",
      "Trained batch 1247 batch loss 0.328740478 epoch total loss 0.341343462\n",
      "Trained batch 1248 batch loss 0.332324475 epoch total loss 0.34133625\n",
      "Trained batch 1249 batch loss 0.333712488 epoch total loss 0.341330141\n",
      "Trained batch 1250 batch loss 0.321674287 epoch total loss 0.341314435\n",
      "Trained batch 1251 batch loss 0.342448324 epoch total loss 0.341315329\n",
      "Trained batch 1252 batch loss 0.338073403 epoch total loss 0.341312736\n",
      "Trained batch 1253 batch loss 0.282019734 epoch total loss 0.34126541\n",
      "Trained batch 1254 batch loss 0.298672199 epoch total loss 0.341231436\n",
      "Trained batch 1255 batch loss 0.316688687 epoch total loss 0.341211885\n",
      "Trained batch 1256 batch loss 0.311202884 epoch total loss 0.341187984\n",
      "Trained batch 1257 batch loss 0.320775628 epoch total loss 0.341171741\n",
      "Trained batch 1258 batch loss 0.358871222 epoch total loss 0.341185808\n",
      "Trained batch 1259 batch loss 0.338253498 epoch total loss 0.341183484\n",
      "Trained batch 1260 batch loss 0.300553799 epoch total loss 0.341151237\n",
      "Trained batch 1261 batch loss 0.288407266 epoch total loss 0.341109425\n",
      "Trained batch 1262 batch loss 0.281698108 epoch total loss 0.341062337\n",
      "Trained batch 1263 batch loss 0.328478634 epoch total loss 0.341052383\n",
      "Trained batch 1264 batch loss 0.351326644 epoch total loss 0.341060519\n",
      "Trained batch 1265 batch loss 0.345123321 epoch total loss 0.341063738\n",
      "Trained batch 1266 batch loss 0.325850368 epoch total loss 0.341051698\n",
      "Trained batch 1267 batch loss 0.329573214 epoch total loss 0.341042638\n",
      "Trained batch 1268 batch loss 0.322786242 epoch total loss 0.341028243\n",
      "Trained batch 1269 batch loss 0.324919701 epoch total loss 0.341015548\n",
      "Trained batch 1270 batch loss 0.336620659 epoch total loss 0.341012061\n",
      "Trained batch 1271 batch loss 0.324148417 epoch total loss 0.340998799\n",
      "Trained batch 1272 batch loss 0.290430903 epoch total loss 0.340959042\n",
      "Trained batch 1273 batch loss 0.323075294 epoch total loss 0.340945\n",
      "Trained batch 1274 batch loss 0.31773144 epoch total loss 0.340926796\n",
      "Trained batch 1275 batch loss 0.319258153 epoch total loss 0.340909779\n",
      "Trained batch 1276 batch loss 0.313884914 epoch total loss 0.34088859\n",
      "Trained batch 1277 batch loss 0.329246581 epoch total loss 0.34087947\n",
      "Trained batch 1278 batch loss 0.345617 epoch total loss 0.340883195\n",
      "Trained batch 1279 batch loss 0.311211705 epoch total loss 0.340859979\n",
      "Trained batch 1280 batch loss 0.305253625 epoch total loss 0.340832174\n",
      "Trained batch 1281 batch loss 0.359475553 epoch total loss 0.340846747\n",
      "Trained batch 1282 batch loss 0.355178058 epoch total loss 0.340857893\n",
      "Trained batch 1283 batch loss 0.333488226 epoch total loss 0.340852171\n",
      "Trained batch 1284 batch loss 0.357346743 epoch total loss 0.340865016\n",
      "Trained batch 1285 batch loss 0.327250957 epoch total loss 0.340854406\n",
      "Trained batch 1286 batch loss 0.325630695 epoch total loss 0.340842575\n",
      "Trained batch 1287 batch loss 0.325180352 epoch total loss 0.340830415\n",
      "Trained batch 1288 batch loss 0.31827274 epoch total loss 0.340812892\n",
      "Trained batch 1289 batch loss 0.323961496 epoch total loss 0.340799838\n",
      "Trained batch 1290 batch loss 0.324630857 epoch total loss 0.340787321\n",
      "Trained batch 1291 batch loss 0.31995523 epoch total loss 0.340771168\n",
      "Trained batch 1292 batch loss 0.314385176 epoch total loss 0.340750754\n",
      "Trained batch 1293 batch loss 0.311101019 epoch total loss 0.340727806\n",
      "Trained batch 1294 batch loss 0.305278122 epoch total loss 0.340700418\n",
      "Trained batch 1295 batch loss 0.280265927 epoch total loss 0.340653747\n",
      "Trained batch 1296 batch loss 0.287127733 epoch total loss 0.340612471\n",
      "Trained batch 1297 batch loss 0.29592824 epoch total loss 0.34057802\n",
      "Trained batch 1298 batch loss 0.340966523 epoch total loss 0.340578318\n",
      "Trained batch 1299 batch loss 0.349273354 epoch total loss 0.340585\n",
      "Trained batch 1300 batch loss 0.365683347 epoch total loss 0.340604305\n",
      "Trained batch 1301 batch loss 0.342872411 epoch total loss 0.340606064\n",
      "Trained batch 1302 batch loss 0.327545464 epoch total loss 0.34059602\n",
      "Trained batch 1303 batch loss 0.324063778 epoch total loss 0.340583324\n",
      "Trained batch 1304 batch loss 0.303839207 epoch total loss 0.340555161\n",
      "Trained batch 1305 batch loss 0.347610176 epoch total loss 0.340560555\n",
      "Trained batch 1306 batch loss 0.327144086 epoch total loss 0.340550274\n",
      "Trained batch 1307 batch loss 0.284933776 epoch total loss 0.340507746\n",
      "Trained batch 1308 batch loss 0.333384842 epoch total loss 0.340502292\n",
      "Trained batch 1309 batch loss 0.348849177 epoch total loss 0.34050867\n",
      "Trained batch 1310 batch loss 0.340261966 epoch total loss 0.340508461\n",
      "Trained batch 1311 batch loss 0.288807154 epoch total loss 0.340469033\n",
      "Trained batch 1312 batch loss 0.294109106 epoch total loss 0.340433717\n",
      "Trained batch 1313 batch loss 0.290511787 epoch total loss 0.340395659\n",
      "Trained batch 1314 batch loss 0.290960759 epoch total loss 0.340358049\n",
      "Trained batch 1315 batch loss 0.281392336 epoch total loss 0.340313196\n",
      "Trained batch 1316 batch loss 0.270371705 epoch total loss 0.340260088\n",
      "Trained batch 1317 batch loss 0.295480251 epoch total loss 0.340226054\n",
      "Trained batch 1318 batch loss 0.309240073 epoch total loss 0.34020254\n",
      "Trained batch 1319 batch loss 0.302033067 epoch total loss 0.340173602\n",
      "Trained batch 1320 batch loss 0.334189832 epoch total loss 0.340169072\n",
      "Trained batch 1321 batch loss 0.317284465 epoch total loss 0.340151757\n",
      "Trained batch 1322 batch loss 0.325481892 epoch total loss 0.340140671\n",
      "Trained batch 1323 batch loss 0.311132729 epoch total loss 0.340118736\n",
      "Trained batch 1324 batch loss 0.284707606 epoch total loss 0.340076864\n",
      "Trained batch 1325 batch loss 0.32994476 epoch total loss 0.340069234\n",
      "Trained batch 1326 batch loss 0.313917398 epoch total loss 0.340049505\n",
      "Trained batch 1327 batch loss 0.321501285 epoch total loss 0.340035528\n",
      "Trained batch 1328 batch loss 0.325865656 epoch total loss 0.340024859\n",
      "Trained batch 1329 batch loss 0.308446765 epoch total loss 0.340001076\n",
      "Trained batch 1330 batch loss 0.313427478 epoch total loss 0.339981109\n",
      "Trained batch 1331 batch loss 0.319995254 epoch total loss 0.339966089\n",
      "Trained batch 1332 batch loss 0.331633687 epoch total loss 0.33995983\n",
      "Trained batch 1333 batch loss 0.340507746 epoch total loss 0.339960247\n",
      "Trained batch 1334 batch loss 0.323682427 epoch total loss 0.339948058\n",
      "Trained batch 1335 batch loss 0.302852631 epoch total loss 0.339920253\n",
      "Trained batch 1336 batch loss 0.309360147 epoch total loss 0.339897394\n",
      "Trained batch 1337 batch loss 0.318226 epoch total loss 0.339881182\n",
      "Trained batch 1338 batch loss 0.345845878 epoch total loss 0.339885652\n",
      "Trained batch 1339 batch loss 0.319155395 epoch total loss 0.339870155\n",
      "Trained batch 1340 batch loss 0.354252815 epoch total loss 0.339880884\n",
      "Trained batch 1341 batch loss 0.332122624 epoch total loss 0.339875102\n",
      "Trained batch 1342 batch loss 0.339140058 epoch total loss 0.339874566\n",
      "Trained batch 1343 batch loss 0.335284829 epoch total loss 0.339871168\n",
      "Trained batch 1344 batch loss 0.323388 epoch total loss 0.33985889\n",
      "Trained batch 1345 batch loss 0.310042083 epoch total loss 0.339836717\n",
      "Trained batch 1346 batch loss 0.319136262 epoch total loss 0.339821339\n",
      "Trained batch 1347 batch loss 0.310641676 epoch total loss 0.339799672\n",
      "Trained batch 1348 batch loss 0.32677415 epoch total loss 0.339790016\n",
      "Trained batch 1349 batch loss 0.326815665 epoch total loss 0.33978039\n",
      "Trained batch 1350 batch loss 0.299282551 epoch total loss 0.339750379\n",
      "Trained batch 1351 batch loss 0.303382456 epoch total loss 0.339723468\n",
      "Trained batch 1352 batch loss 0.311673 epoch total loss 0.339702725\n",
      "Trained batch 1353 batch loss 0.287947595 epoch total loss 0.339664459\n",
      "Trained batch 1354 batch loss 0.315651655 epoch total loss 0.339646727\n",
      "Trained batch 1355 batch loss 0.315318555 epoch total loss 0.339628756\n",
      "Trained batch 1356 batch loss 0.307202697 epoch total loss 0.339604825\n",
      "Trained batch 1357 batch loss 0.298842072 epoch total loss 0.339574784\n",
      "Trained batch 1358 batch loss 0.285465062 epoch total loss 0.339534938\n",
      "Trained batch 1359 batch loss 0.325930566 epoch total loss 0.339524925\n",
      "Trained batch 1360 batch loss 0.314840674 epoch total loss 0.339506775\n",
      "Trained batch 1361 batch loss 0.331698626 epoch total loss 0.339501053\n",
      "Trained batch 1362 batch loss 0.323099762 epoch total loss 0.339488983\n",
      "Trained batch 1363 batch loss 0.299516767 epoch total loss 0.339459658\n",
      "Trained batch 1364 batch loss 0.261335313 epoch total loss 0.339402378\n",
      "Trained batch 1365 batch loss 0.253539294 epoch total loss 0.339339495\n",
      "Trained batch 1366 batch loss 0.288571328 epoch total loss 0.339302331\n",
      "Trained batch 1367 batch loss 0.329852581 epoch total loss 0.339295417\n",
      "Trained batch 1368 batch loss 0.36513266 epoch total loss 0.339314312\n",
      "Trained batch 1369 batch loss 0.32377106 epoch total loss 0.339302957\n",
      "Trained batch 1370 batch loss 0.280706197 epoch total loss 0.339260161\n",
      "Trained batch 1371 batch loss 0.300387025 epoch total loss 0.339231819\n",
      "Trained batch 1372 batch loss 0.310985476 epoch total loss 0.339211226\n",
      "Trained batch 1373 batch loss 0.275074869 epoch total loss 0.339164525\n",
      "Trained batch 1374 batch loss 0.32597369 epoch total loss 0.339154929\n",
      "Trained batch 1375 batch loss 0.309150755 epoch total loss 0.339133114\n",
      "Trained batch 1376 batch loss 0.319798738 epoch total loss 0.339119047\n",
      "Trained batch 1377 batch loss 0.332033396 epoch total loss 0.339113891\n",
      "Trained batch 1378 batch loss 0.305190176 epoch total loss 0.339089274\n",
      "Trained batch 1379 batch loss 0.309604436 epoch total loss 0.339067876\n",
      "Trained batch 1380 batch loss 0.305366725 epoch total loss 0.339043468\n",
      "Trained batch 1381 batch loss 0.302321 epoch total loss 0.339016855\n",
      "Trained batch 1382 batch loss 0.291601747 epoch total loss 0.338982552\n",
      "Trained batch 1383 batch loss 0.31595248 epoch total loss 0.338965893\n",
      "Trained batch 1384 batch loss 0.326188147 epoch total loss 0.338956654\n",
      "Trained batch 1385 batch loss 0.334037 epoch total loss 0.338953108\n",
      "Trained batch 1386 batch loss 0.300899714 epoch total loss 0.33892566\n",
      "Trained batch 1387 batch loss 0.281891108 epoch total loss 0.338884562\n",
      "Trained batch 1388 batch loss 0.281282306 epoch total loss 0.338843048\n",
      "Epoch 1 train loss 0.33884304761886597\n",
      "train time : 695.7866554260254\n",
      "Validated batch 1 batch loss 0.303896338\n",
      "Validated batch 2 batch loss 0.33051452\n",
      "Validated batch 3 batch loss 0.329513669\n",
      "Validated batch 4 batch loss 0.308888823\n",
      "Validated batch 5 batch loss 0.340491235\n",
      "Validated batch 6 batch loss 0.357219756\n",
      "Validated batch 7 batch loss 0.299950838\n",
      "Validated batch 8 batch loss 0.333964229\n",
      "Validated batch 9 batch loss 0.314182699\n",
      "Validated batch 10 batch loss 0.355810851\n",
      "Validated batch 11 batch loss 0.32017085\n",
      "Validated batch 12 batch loss 0.276929498\n",
      "Validated batch 13 batch loss 0.289231032\n",
      "Validated batch 14 batch loss 0.315271437\n",
      "Validated batch 15 batch loss 0.30521971\n",
      "Validated batch 16 batch loss 0.327014118\n",
      "Validated batch 17 batch loss 0.311053693\n",
      "Validated batch 18 batch loss 0.309132636\n",
      "Validated batch 19 batch loss 0.332578897\n",
      "Validated batch 20 batch loss 0.355101287\n",
      "Validated batch 21 batch loss 0.340415239\n",
      "Validated batch 22 batch loss 0.331068307\n",
      "Validated batch 23 batch loss 0.305727482\n",
      "Validated batch 24 batch loss 0.318999469\n",
      "Validated batch 25 batch loss 0.313623875\n",
      "Validated batch 26 batch loss 0.304019392\n",
      "Validated batch 27 batch loss 0.306447923\n",
      "Validated batch 28 batch loss 0.309852242\n",
      "Validated batch 29 batch loss 0.341189027\n",
      "Validated batch 30 batch loss 0.321852058\n",
      "Validated batch 31 batch loss 0.296913\n",
      "Validated batch 32 batch loss 0.311475\n",
      "Validated batch 33 batch loss 0.32456845\n",
      "Validated batch 34 batch loss 0.321611464\n",
      "Validated batch 35 batch loss 0.309158713\n",
      "Validated batch 36 batch loss 0.318426579\n",
      "Validated batch 37 batch loss 0.33072\n",
      "Validated batch 38 batch loss 0.352638811\n",
      "Validated batch 39 batch loss 0.351448476\n",
      "Validated batch 40 batch loss 0.322986722\n",
      "Validated batch 41 batch loss 0.351594\n",
      "Validated batch 42 batch loss 0.286370426\n",
      "Validated batch 43 batch loss 0.31537503\n",
      "Validated batch 44 batch loss 0.294528365\n",
      "Validated batch 45 batch loss 0.328970939\n",
      "Validated batch 46 batch loss 0.362209022\n",
      "Validated batch 47 batch loss 0.32498911\n",
      "Validated batch 48 batch loss 0.332184374\n",
      "Validated batch 49 batch loss 0.306198955\n",
      "Validated batch 50 batch loss 0.34573549\n",
      "Validated batch 51 batch loss 0.33047992\n",
      "Validated batch 52 batch loss 0.330562353\n",
      "Validated batch 53 batch loss 0.344869226\n",
      "Validated batch 54 batch loss 0.339554071\n",
      "Validated batch 55 batch loss 0.346841455\n",
      "Validated batch 56 batch loss 0.319895804\n",
      "Validated batch 57 batch loss 0.347743273\n",
      "Validated batch 58 batch loss 0.338994771\n",
      "Validated batch 59 batch loss 0.32589674\n",
      "Validated batch 60 batch loss 0.357849091\n",
      "Validated batch 61 batch loss 0.33949405\n",
      "Validated batch 62 batch loss 0.336675465\n",
      "Validated batch 63 batch loss 0.380143076\n",
      "Validated batch 64 batch loss 0.29030925\n",
      "Validated batch 65 batch loss 0.343477637\n",
      "Validated batch 66 batch loss 0.292970955\n",
      "Validated batch 67 batch loss 0.315350354\n",
      "Validated batch 68 batch loss 0.359771073\n",
      "Validated batch 69 batch loss 0.314858049\n",
      "Validated batch 70 batch loss 0.318538636\n",
      "Validated batch 71 batch loss 0.312882483\n",
      "Validated batch 72 batch loss 0.324663073\n",
      "Validated batch 73 batch loss 0.309938252\n",
      "Validated batch 74 batch loss 0.320711493\n",
      "Validated batch 75 batch loss 0.335011959\n",
      "Validated batch 76 batch loss 0.337150067\n",
      "Validated batch 77 batch loss 0.314510852\n",
      "Validated batch 78 batch loss 0.320020258\n",
      "Validated batch 79 batch loss 0.328523546\n",
      "Validated batch 80 batch loss 0.339755058\n",
      "Validated batch 81 batch loss 0.33825776\n",
      "Validated batch 82 batch loss 0.315226614\n",
      "Validated batch 83 batch loss 0.316975385\n",
      "Validated batch 84 batch loss 0.338251889\n",
      "Validated batch 85 batch loss 0.318929762\n",
      "Validated batch 86 batch loss 0.362308383\n",
      "Validated batch 87 batch loss 0.327558786\n",
      "Validated batch 88 batch loss 0.317436546\n",
      "Validated batch 89 batch loss 0.333254874\n",
      "Validated batch 90 batch loss 0.336461782\n",
      "Validated batch 91 batch loss 0.318381965\n",
      "Validated batch 92 batch loss 0.337803662\n",
      "Validated batch 93 batch loss 0.368035853\n",
      "Validated batch 94 batch loss 0.329290748\n",
      "Validated batch 95 batch loss 0.330424815\n",
      "Validated batch 96 batch loss 0.326183885\n",
      "Validated batch 97 batch loss 0.314228058\n",
      "Validated batch 98 batch loss 0.339898139\n",
      "Validated batch 99 batch loss 0.319292486\n",
      "Validated batch 100 batch loss 0.308342278\n",
      "Validated batch 101 batch loss 0.305545151\n",
      "Validated batch 102 batch loss 0.323198736\n",
      "Validated batch 103 batch loss 0.340544343\n",
      "Validated batch 104 batch loss 0.34548229\n",
      "Validated batch 105 batch loss 0.31705907\n",
      "Validated batch 106 batch loss 0.296894\n",
      "Validated batch 107 batch loss 0.318947345\n",
      "Validated batch 108 batch loss 0.340709537\n",
      "Validated batch 109 batch loss 0.318382263\n",
      "Validated batch 110 batch loss 0.335522711\n",
      "Validated batch 111 batch loss 0.343070686\n",
      "Validated batch 112 batch loss 0.383714855\n",
      "Validated batch 113 batch loss 0.368068188\n",
      "Validated batch 114 batch loss 0.330132693\n",
      "Validated batch 115 batch loss 0.2962749\n",
      "Validated batch 116 batch loss 0.32227084\n",
      "Validated batch 117 batch loss 0.354574\n",
      "Validated batch 118 batch loss 0.305570215\n",
      "Validated batch 119 batch loss 0.311012298\n",
      "Validated batch 120 batch loss 0.327949494\n",
      "Validated batch 121 batch loss 0.320881933\n",
      "Validated batch 122 batch loss 0.323965102\n",
      "Validated batch 123 batch loss 0.312856108\n",
      "Validated batch 124 batch loss 0.309886277\n",
      "Validated batch 125 batch loss 0.35282284\n",
      "Validated batch 126 batch loss 0.322325915\n",
      "Validated batch 127 batch loss 0.303607613\n",
      "Validated batch 128 batch loss 0.313438267\n",
      "Validated batch 129 batch loss 0.358852684\n",
      "Validated batch 130 batch loss 0.343069613\n",
      "Validated batch 131 batch loss 0.367677569\n",
      "Validated batch 132 batch loss 0.320722044\n",
      "Validated batch 133 batch loss 0.371143103\n",
      "Validated batch 134 batch loss 0.321989536\n",
      "Validated batch 135 batch loss 0.348288476\n",
      "Validated batch 136 batch loss 0.337576866\n",
      "Validated batch 137 batch loss 0.266964704\n",
      "Validated batch 138 batch loss 0.32749933\n",
      "Validated batch 139 batch loss 0.363373756\n",
      "Validated batch 140 batch loss 0.33833313\n",
      "Validated batch 141 batch loss 0.328849941\n",
      "Validated batch 142 batch loss 0.326913625\n",
      "Validated batch 143 batch loss 0.337107539\n",
      "Validated batch 144 batch loss 0.366653085\n",
      "Validated batch 145 batch loss 0.309762239\n",
      "Validated batch 146 batch loss 0.335181415\n",
      "Validated batch 147 batch loss 0.313012064\n",
      "Validated batch 148 batch loss 0.350476772\n",
      "Validated batch 149 batch loss 0.330016553\n",
      "Validated batch 150 batch loss 0.307567567\n",
      "Validated batch 151 batch loss 0.337689638\n",
      "Validated batch 152 batch loss 0.344021261\n",
      "Validated batch 153 batch loss 0.343757868\n",
      "Validated batch 154 batch loss 0.32908994\n",
      "Validated batch 155 batch loss 0.331796557\n",
      "Validated batch 156 batch loss 0.310870767\n",
      "Validated batch 157 batch loss 0.308898151\n",
      "Validated batch 158 batch loss 0.332121879\n",
      "Validated batch 159 batch loss 0.315529108\n",
      "Validated batch 160 batch loss 0.327856511\n",
      "Validated batch 161 batch loss 0.337121785\n",
      "Validated batch 162 batch loss 0.348635644\n",
      "Validated batch 163 batch loss 0.288587183\n",
      "Validated batch 164 batch loss 0.318378836\n",
      "Validated batch 165 batch loss 0.302311122\n",
      "Validated batch 166 batch loss 0.3087596\n",
      "Validated batch 167 batch loss 0.337793022\n",
      "Validated batch 168 batch loss 0.332111359\n",
      "Validated batch 169 batch loss 0.323618948\n",
      "Validated batch 170 batch loss 0.360688\n",
      "Validated batch 171 batch loss 0.344974816\n",
      "Validated batch 172 batch loss 0.333040565\n",
      "Validated batch 173 batch loss 0.361606538\n",
      "Validated batch 174 batch loss 0.347504914\n",
      "Validated batch 175 batch loss 0.346670181\n",
      "Validated batch 176 batch loss 0.354026556\n",
      "Validated batch 177 batch loss 0.353657097\n",
      "Validated batch 178 batch loss 0.338642269\n",
      "Validated batch 179 batch loss 0.30105117\n",
      "Validated batch 180 batch loss 0.312434703\n",
      "Validated batch 181 batch loss 0.332938731\n",
      "Validated batch 182 batch loss 0.345890343\n",
      "Validated batch 183 batch loss 0.321679652\n",
      "Validated batch 184 batch loss 0.334982812\n",
      "Validated batch 185 batch loss 0.319611728\n",
      "Epoch 1 val loss 0.3281919062137604\n",
      "Model /aiffel/aiffel/mpii/my_models/model-epoch-1-loss-0.3282.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.32594797 epoch total loss 0.32594797\n",
      "Trained batch 2 batch loss 0.276812375 epoch total loss 0.301380157\n",
      "Trained batch 3 batch loss 0.277672648 epoch total loss 0.293477654\n",
      "Trained batch 4 batch loss 0.331430286 epoch total loss 0.30296582\n",
      "Trained batch 5 batch loss 0.339319 epoch total loss 0.310236454\n",
      "Trained batch 6 batch loss 0.32931751 epoch total loss 0.31341663\n",
      "Trained batch 7 batch loss 0.32775268 epoch total loss 0.315464646\n",
      "Trained batch 8 batch loss 0.321910083 epoch total loss 0.316270322\n",
      "Trained batch 9 batch loss 0.325349927 epoch total loss 0.31727919\n",
      "Trained batch 10 batch loss 0.334613323 epoch total loss 0.319012582\n",
      "Trained batch 11 batch loss 0.32689485 epoch total loss 0.319729149\n",
      "Trained batch 12 batch loss 0.293624789 epoch total loss 0.317553788\n",
      "Trained batch 13 batch loss 0.306578279 epoch total loss 0.316709518\n",
      "Trained batch 14 batch loss 0.320554763 epoch total loss 0.316984177\n",
      "Trained batch 15 batch loss 0.304014415 epoch total loss 0.316119522\n",
      "Trained batch 16 batch loss 0.275635481 epoch total loss 0.313589275\n",
      "Trained batch 17 batch loss 0.312609494 epoch total loss 0.313531637\n",
      "Trained batch 18 batch loss 0.303639591 epoch total loss 0.312982082\n",
      "Trained batch 19 batch loss 0.298205912 epoch total loss 0.312204391\n",
      "Trained batch 20 batch loss 0.294253618 epoch total loss 0.311306864\n",
      "Trained batch 21 batch loss 0.287108094 epoch total loss 0.310154527\n",
      "Trained batch 22 batch loss 0.31545189 epoch total loss 0.31039533\n",
      "Trained batch 23 batch loss 0.300351351 epoch total loss 0.309958637\n",
      "Trained batch 24 batch loss 0.310706735 epoch total loss 0.30998978\n",
      "Trained batch 25 batch loss 0.327714175 epoch total loss 0.310698748\n",
      "Trained batch 26 batch loss 0.289576679 epoch total loss 0.309886396\n",
      "Trained batch 27 batch loss 0.315973163 epoch total loss 0.310111821\n",
      "Trained batch 28 batch loss 0.314513713 epoch total loss 0.310269058\n",
      "Trained batch 29 batch loss 0.320239156 epoch total loss 0.310612857\n",
      "Trained batch 30 batch loss 0.277934402 epoch total loss 0.309523553\n",
      "Trained batch 31 batch loss 0.297792852 epoch total loss 0.309145123\n",
      "Trained batch 32 batch loss 0.317378432 epoch total loss 0.309402406\n",
      "Trained batch 33 batch loss 0.299602449 epoch total loss 0.309105426\n",
      "Trained batch 34 batch loss 0.290319532 epoch total loss 0.308552921\n",
      "Trained batch 35 batch loss 0.32330811 epoch total loss 0.308974475\n",
      "Trained batch 36 batch loss 0.302540749 epoch total loss 0.30879578\n",
      "Trained batch 37 batch loss 0.311128289 epoch total loss 0.308858812\n",
      "Trained batch 38 batch loss 0.316745698 epoch total loss 0.309066385\n",
      "Trained batch 39 batch loss 0.31067577 epoch total loss 0.309107631\n",
      "Trained batch 40 batch loss 0.312796503 epoch total loss 0.30919987\n",
      "Trained batch 41 batch loss 0.300750762 epoch total loss 0.308993787\n",
      "Trained batch 42 batch loss 0.305878252 epoch total loss 0.308919609\n",
      "Trained batch 43 batch loss 0.331617236 epoch total loss 0.309447467\n",
      "Trained batch 44 batch loss 0.338118672 epoch total loss 0.310099095\n",
      "Trained batch 45 batch loss 0.315348566 epoch total loss 0.310215741\n",
      "Trained batch 46 batch loss 0.300275654 epoch total loss 0.309999645\n",
      "Trained batch 47 batch loss 0.31108892 epoch total loss 0.310022831\n",
      "Trained batch 48 batch loss 0.308714092 epoch total loss 0.309995562\n",
      "Trained batch 49 batch loss 0.319849044 epoch total loss 0.310196638\n",
      "Trained batch 50 batch loss 0.340510786 epoch total loss 0.310802907\n",
      "Trained batch 51 batch loss 0.324478775 epoch total loss 0.311071068\n",
      "Trained batch 52 batch loss 0.3090294 epoch total loss 0.311031818\n",
      "Trained batch 53 batch loss 0.306663513 epoch total loss 0.310949385\n",
      "Trained batch 54 batch loss 0.328262985 epoch total loss 0.31127\n",
      "Trained batch 55 batch loss 0.309374 epoch total loss 0.311235517\n",
      "Trained batch 56 batch loss 0.311330557 epoch total loss 0.311237246\n",
      "Trained batch 57 batch loss 0.293262839 epoch total loss 0.310921878\n",
      "Trained batch 58 batch loss 0.322373748 epoch total loss 0.311119348\n",
      "Trained batch 59 batch loss 0.33117342 epoch total loss 0.311459243\n",
      "Trained batch 60 batch loss 0.332423031 epoch total loss 0.311808616\n",
      "Trained batch 61 batch loss 0.334715962 epoch total loss 0.312184155\n",
      "Trained batch 62 batch loss 0.369733661 epoch total loss 0.313112378\n",
      "Trained batch 63 batch loss 0.313838333 epoch total loss 0.313123912\n",
      "Trained batch 64 batch loss 0.323756605 epoch total loss 0.31329006\n",
      "Trained batch 65 batch loss 0.307160646 epoch total loss 0.313195765\n",
      "Trained batch 66 batch loss 0.306716293 epoch total loss 0.313097596\n",
      "Trained batch 67 batch loss 0.321667373 epoch total loss 0.313225508\n",
      "Trained batch 68 batch loss 0.3322227 epoch total loss 0.313504875\n",
      "Trained batch 69 batch loss 0.309872985 epoch total loss 0.313452244\n",
      "Trained batch 70 batch loss 0.314412683 epoch total loss 0.313465953\n",
      "Trained batch 71 batch loss 0.295191109 epoch total loss 0.31320858\n",
      "Trained batch 72 batch loss 0.280025184 epoch total loss 0.312747687\n",
      "Trained batch 73 batch loss 0.303238839 epoch total loss 0.312617421\n",
      "Trained batch 74 batch loss 0.299588263 epoch total loss 0.312441349\n",
      "Trained batch 75 batch loss 0.302207917 epoch total loss 0.312304914\n",
      "Trained batch 76 batch loss 0.304013401 epoch total loss 0.312195837\n",
      "Trained batch 77 batch loss 0.299286366 epoch total loss 0.31202817\n",
      "Trained batch 78 batch loss 0.301802546 epoch total loss 0.311897069\n",
      "Trained batch 79 batch loss 0.292064846 epoch total loss 0.311646\n",
      "Trained batch 80 batch loss 0.308104038 epoch total loss 0.311601728\n",
      "Trained batch 81 batch loss 0.298303068 epoch total loss 0.311437547\n",
      "Trained batch 82 batch loss 0.308017582 epoch total loss 0.311395854\n",
      "Trained batch 83 batch loss 0.297548443 epoch total loss 0.31122902\n",
      "Trained batch 84 batch loss 0.280822039 epoch total loss 0.310867041\n",
      "Trained batch 85 batch loss 0.299370676 epoch total loss 0.310731769\n",
      "Trained batch 86 batch loss 0.314216018 epoch total loss 0.3107723\n",
      "Trained batch 87 batch loss 0.312765419 epoch total loss 0.310795218\n",
      "Trained batch 88 batch loss 0.301012456 epoch total loss 0.310684025\n",
      "Trained batch 89 batch loss 0.348023951 epoch total loss 0.311103582\n",
      "Trained batch 90 batch loss 0.357791454 epoch total loss 0.311622351\n",
      "Trained batch 91 batch loss 0.3784298 epoch total loss 0.312356502\n",
      "Trained batch 92 batch loss 0.37136817 epoch total loss 0.312997907\n",
      "Trained batch 93 batch loss 0.332520097 epoch total loss 0.313207835\n",
      "Trained batch 94 batch loss 0.363955468 epoch total loss 0.313747704\n",
      "Trained batch 95 batch loss 0.327984452 epoch total loss 0.31389755\n",
      "Trained batch 96 batch loss 0.285359621 epoch total loss 0.313600272\n",
      "Trained batch 97 batch loss 0.297289729 epoch total loss 0.313432127\n",
      "Trained batch 98 batch loss 0.324964941 epoch total loss 0.313549787\n",
      "Trained batch 99 batch loss 0.328320861 epoch total loss 0.313699\n",
      "Trained batch 100 batch loss 0.332866639 epoch total loss 0.313890696\n",
      "Trained batch 101 batch loss 0.297611833 epoch total loss 0.313729495\n",
      "Trained batch 102 batch loss 0.315962255 epoch total loss 0.3137514\n",
      "Trained batch 103 batch loss 0.275813341 epoch total loss 0.313383073\n",
      "Trained batch 104 batch loss 0.27717784 epoch total loss 0.313034981\n",
      "Trained batch 105 batch loss 0.307929724 epoch total loss 0.312986344\n",
      "Trained batch 106 batch loss 0.295582 epoch total loss 0.312822163\n",
      "Trained batch 107 batch loss 0.303568929 epoch total loss 0.312735677\n",
      "Trained batch 108 batch loss 0.320640922 epoch total loss 0.312808871\n",
      "Trained batch 109 batch loss 0.305202603 epoch total loss 0.312739104\n",
      "Trained batch 110 batch loss 0.29456684 epoch total loss 0.31257391\n",
      "Trained batch 111 batch loss 0.312513649 epoch total loss 0.312573373\n",
      "Trained batch 112 batch loss 0.289474666 epoch total loss 0.312367141\n",
      "Trained batch 113 batch loss 0.303172 epoch total loss 0.312285781\n",
      "Trained batch 114 batch loss 0.310975403 epoch total loss 0.312274277\n",
      "Trained batch 115 batch loss 0.287010849 epoch total loss 0.312054574\n",
      "Trained batch 116 batch loss 0.27556929 epoch total loss 0.311740041\n",
      "Trained batch 117 batch loss 0.302078128 epoch total loss 0.311657488\n",
      "Trained batch 118 batch loss 0.314133525 epoch total loss 0.311678439\n",
      "Trained batch 119 batch loss 0.255871326 epoch total loss 0.31120947\n",
      "Trained batch 120 batch loss 0.290091276 epoch total loss 0.311033517\n",
      "Trained batch 121 batch loss 0.271533072 epoch total loss 0.310707062\n",
      "Trained batch 122 batch loss 0.293278873 epoch total loss 0.31056419\n",
      "Trained batch 123 batch loss 0.334957331 epoch total loss 0.310762525\n",
      "Trained batch 124 batch loss 0.291429847 epoch total loss 0.310606629\n",
      "Trained batch 125 batch loss 0.294425219 epoch total loss 0.310477167\n",
      "Trained batch 126 batch loss 0.326980293 epoch total loss 0.310608149\n",
      "Trained batch 127 batch loss 0.331712097 epoch total loss 0.310774326\n",
      "Trained batch 128 batch loss 0.331633031 epoch total loss 0.310937285\n",
      "Trained batch 129 batch loss 0.327012509 epoch total loss 0.311061889\n",
      "Trained batch 130 batch loss 0.326058179 epoch total loss 0.311177254\n",
      "Trained batch 131 batch loss 0.342099071 epoch total loss 0.311413288\n",
      "Trained batch 132 batch loss 0.322470903 epoch total loss 0.311497062\n",
      "Trained batch 133 batch loss 0.31627056 epoch total loss 0.311532944\n",
      "Trained batch 134 batch loss 0.317669421 epoch total loss 0.311578721\n",
      "Trained batch 135 batch loss 0.324648261 epoch total loss 0.311675549\n",
      "Trained batch 136 batch loss 0.328361392 epoch total loss 0.311798245\n",
      "Trained batch 137 batch loss 0.303963661 epoch total loss 0.311741054\n",
      "Trained batch 138 batch loss 0.298436671 epoch total loss 0.311644614\n",
      "Trained batch 139 batch loss 0.26888153 epoch total loss 0.311337\n",
      "Trained batch 140 batch loss 0.290040523 epoch total loss 0.311184853\n",
      "Trained batch 141 batch loss 0.277727962 epoch total loss 0.310947567\n",
      "Trained batch 142 batch loss 0.292932719 epoch total loss 0.310820729\n",
      "Trained batch 143 batch loss 0.254315794 epoch total loss 0.31042558\n",
      "Trained batch 144 batch loss 0.245643973 epoch total loss 0.309975713\n",
      "Trained batch 145 batch loss 0.246929884 epoch total loss 0.309540898\n",
      "Trained batch 146 batch loss 0.248295 epoch total loss 0.3091214\n",
      "Trained batch 147 batch loss 0.269058436 epoch total loss 0.308848858\n",
      "Trained batch 148 batch loss 0.28596136 epoch total loss 0.308694214\n",
      "Trained batch 149 batch loss 0.300034046 epoch total loss 0.308636099\n",
      "Trained batch 150 batch loss 0.314823121 epoch total loss 0.308677346\n",
      "Trained batch 151 batch loss 0.28783977 epoch total loss 0.308539331\n",
      "Trained batch 152 batch loss 0.269229382 epoch total loss 0.308280706\n",
      "Trained batch 153 batch loss 0.258394092 epoch total loss 0.307954639\n",
      "Trained batch 154 batch loss 0.355626643 epoch total loss 0.308264196\n",
      "Trained batch 155 batch loss 0.304923 epoch total loss 0.308242649\n",
      "Trained batch 156 batch loss 0.323394358 epoch total loss 0.308339775\n",
      "Trained batch 157 batch loss 0.356338501 epoch total loss 0.308645517\n",
      "Trained batch 158 batch loss 0.31777 epoch total loss 0.308703274\n",
      "Trained batch 159 batch loss 0.303158253 epoch total loss 0.308668375\n",
      "Trained batch 160 batch loss 0.294002563 epoch total loss 0.308576733\n",
      "Trained batch 161 batch loss 0.314101756 epoch total loss 0.308611035\n",
      "Trained batch 162 batch loss 0.307482421 epoch total loss 0.308604091\n",
      "Trained batch 163 batch loss 0.287432373 epoch total loss 0.308474213\n",
      "Trained batch 164 batch loss 0.329660535 epoch total loss 0.308603406\n",
      "Trained batch 165 batch loss 0.327528059 epoch total loss 0.308718115\n",
      "Trained batch 166 batch loss 0.323996067 epoch total loss 0.308810145\n",
      "Trained batch 167 batch loss 0.314481705 epoch total loss 0.30884409\n",
      "Trained batch 168 batch loss 0.333105743 epoch total loss 0.308988512\n",
      "Trained batch 169 batch loss 0.322002739 epoch total loss 0.309065521\n",
      "Trained batch 170 batch loss 0.322127 epoch total loss 0.309142381\n",
      "Trained batch 171 batch loss 0.349225879 epoch total loss 0.309376776\n",
      "Trained batch 172 batch loss 0.352781832 epoch total loss 0.309629142\n",
      "Trained batch 173 batch loss 0.328415364 epoch total loss 0.309737712\n",
      "Trained batch 174 batch loss 0.327650905 epoch total loss 0.309840679\n",
      "Trained batch 175 batch loss 0.325818717 epoch total loss 0.309931964\n",
      "Trained batch 176 batch loss 0.312798977 epoch total loss 0.309948236\n",
      "Trained batch 177 batch loss 0.32159102 epoch total loss 0.310014\n",
      "Trained batch 178 batch loss 0.309101 epoch total loss 0.310008883\n",
      "Trained batch 179 batch loss 0.293562651 epoch total loss 0.309917033\n",
      "Trained batch 180 batch loss 0.318450212 epoch total loss 0.309964448\n",
      "Trained batch 181 batch loss 0.287900031 epoch total loss 0.309842527\n",
      "Trained batch 182 batch loss 0.283492029 epoch total loss 0.309697747\n",
      "Trained batch 183 batch loss 0.280588239 epoch total loss 0.309538692\n",
      "Trained batch 184 batch loss 0.273926198 epoch total loss 0.309345156\n",
      "Trained batch 185 batch loss 0.281903684 epoch total loss 0.3091968\n",
      "Trained batch 186 batch loss 0.299328923 epoch total loss 0.309143752\n",
      "Trained batch 187 batch loss 0.271956325 epoch total loss 0.308944881\n",
      "Trained batch 188 batch loss 0.286735117 epoch total loss 0.308826745\n",
      "Trained batch 189 batch loss 0.306364685 epoch total loss 0.308813721\n",
      "Trained batch 190 batch loss 0.299995273 epoch total loss 0.308767319\n",
      "Trained batch 191 batch loss 0.347946644 epoch total loss 0.308972448\n",
      "Trained batch 192 batch loss 0.293397903 epoch total loss 0.308891326\n",
      "Trained batch 193 batch loss 0.32782954 epoch total loss 0.308989465\n",
      "Trained batch 194 batch loss 0.326637268 epoch total loss 0.309080452\n",
      "Trained batch 195 batch loss 0.320724785 epoch total loss 0.309140146\n",
      "Trained batch 196 batch loss 0.284070671 epoch total loss 0.309012234\n",
      "Trained batch 197 batch loss 0.274913043 epoch total loss 0.308839142\n",
      "Trained batch 198 batch loss 0.324836195 epoch total loss 0.308919936\n",
      "Trained batch 199 batch loss 0.325108588 epoch total loss 0.309001297\n",
      "Trained batch 200 batch loss 0.321578979 epoch total loss 0.30906418\n",
      "Trained batch 201 batch loss 0.323269725 epoch total loss 0.309134841\n",
      "Trained batch 202 batch loss 0.336985677 epoch total loss 0.309272736\n",
      "Trained batch 203 batch loss 0.344109893 epoch total loss 0.309444338\n",
      "Trained batch 204 batch loss 0.318885446 epoch total loss 0.309490621\n",
      "Trained batch 205 batch loss 0.336996913 epoch total loss 0.309624791\n",
      "Trained batch 206 batch loss 0.353181303 epoch total loss 0.309836239\n",
      "Trained batch 207 batch loss 0.322400451 epoch total loss 0.309896946\n",
      "Trained batch 208 batch loss 0.293814 epoch total loss 0.309819639\n",
      "Trained batch 209 batch loss 0.293731838 epoch total loss 0.309742659\n",
      "Trained batch 210 batch loss 0.253080368 epoch total loss 0.309472829\n",
      "Trained batch 211 batch loss 0.28282246 epoch total loss 0.309346527\n",
      "Trained batch 212 batch loss 0.319515586 epoch total loss 0.309394509\n",
      "Trained batch 213 batch loss 0.306787103 epoch total loss 0.30938226\n",
      "Trained batch 214 batch loss 0.30821988 epoch total loss 0.309376836\n",
      "Trained batch 215 batch loss 0.352021039 epoch total loss 0.30957517\n",
      "Trained batch 216 batch loss 0.299253345 epoch total loss 0.309527397\n",
      "Trained batch 217 batch loss 0.349093765 epoch total loss 0.309709728\n",
      "Trained batch 218 batch loss 0.287864923 epoch total loss 0.309609503\n",
      "Trained batch 219 batch loss 0.365572125 epoch total loss 0.309865028\n",
      "Trained batch 220 batch loss 0.305257887 epoch total loss 0.309844106\n",
      "Trained batch 221 batch loss 0.330265373 epoch total loss 0.309936523\n",
      "Trained batch 222 batch loss 0.360427052 epoch total loss 0.310163975\n",
      "Trained batch 223 batch loss 0.359318644 epoch total loss 0.310384393\n",
      "Trained batch 224 batch loss 0.307932079 epoch total loss 0.310373455\n",
      "Trained batch 225 batch loss 0.314089239 epoch total loss 0.310389936\n",
      "Trained batch 226 batch loss 0.321073741 epoch total loss 0.310437232\n",
      "Trained batch 227 batch loss 0.31460309 epoch total loss 0.31045559\n",
      "Trained batch 228 batch loss 0.319969 epoch total loss 0.310497314\n",
      "Trained batch 229 batch loss 0.330205739 epoch total loss 0.310583383\n",
      "Trained batch 230 batch loss 0.332441866 epoch total loss 0.310678422\n",
      "Trained batch 231 batch loss 0.352455169 epoch total loss 0.310859293\n",
      "Trained batch 232 batch loss 0.342652112 epoch total loss 0.310996324\n",
      "Trained batch 233 batch loss 0.359536409 epoch total loss 0.311204642\n",
      "Trained batch 234 batch loss 0.350382 epoch total loss 0.311372042\n",
      "Trained batch 235 batch loss 0.349087179 epoch total loss 0.311532557\n",
      "Trained batch 236 batch loss 0.343276739 epoch total loss 0.311667085\n",
      "Trained batch 237 batch loss 0.34415409 epoch total loss 0.311804146\n",
      "Trained batch 238 batch loss 0.340466857 epoch total loss 0.311924577\n",
      "Trained batch 239 batch loss 0.388792068 epoch total loss 0.312246233\n",
      "Trained batch 240 batch loss 0.342544973 epoch total loss 0.312372476\n",
      "Trained batch 241 batch loss 0.321145833 epoch total loss 0.312408864\n",
      "Trained batch 242 batch loss 0.323626816 epoch total loss 0.312455207\n",
      "Trained batch 243 batch loss 0.318406761 epoch total loss 0.312479675\n",
      "Trained batch 244 batch loss 0.333401799 epoch total loss 0.312565446\n",
      "Trained batch 245 batch loss 0.323981673 epoch total loss 0.312612057\n",
      "Trained batch 246 batch loss 0.297403663 epoch total loss 0.312550217\n",
      "Trained batch 247 batch loss 0.292015016 epoch total loss 0.312467068\n",
      "Trained batch 248 batch loss 0.323335111 epoch total loss 0.312510878\n",
      "Trained batch 249 batch loss 0.319272548 epoch total loss 0.312538058\n",
      "Trained batch 250 batch loss 0.300622046 epoch total loss 0.312490374\n",
      "Trained batch 251 batch loss 0.346271366 epoch total loss 0.312624961\n",
      "Trained batch 252 batch loss 0.341049105 epoch total loss 0.312737763\n",
      "Trained batch 253 batch loss 0.334239185 epoch total loss 0.312822729\n",
      "Trained batch 254 batch loss 0.330749303 epoch total loss 0.312893301\n",
      "Trained batch 255 batch loss 0.275078177 epoch total loss 0.312745\n",
      "Trained batch 256 batch loss 0.287244499 epoch total loss 0.312645406\n",
      "Trained batch 257 batch loss 0.284884334 epoch total loss 0.312537372\n",
      "Trained batch 258 batch loss 0.305885851 epoch total loss 0.312511593\n",
      "Trained batch 259 batch loss 0.320798874 epoch total loss 0.312543601\n",
      "Trained batch 260 batch loss 0.280645102 epoch total loss 0.312420905\n",
      "Trained batch 261 batch loss 0.342516512 epoch total loss 0.31253621\n",
      "Trained batch 262 batch loss 0.317202479 epoch total loss 0.312554032\n",
      "Trained batch 263 batch loss 0.271850646 epoch total loss 0.312399238\n",
      "Trained batch 264 batch loss 0.257158756 epoch total loss 0.31219\n",
      "Trained batch 265 batch loss 0.270356655 epoch total loss 0.312032133\n",
      "Trained batch 266 batch loss 0.279446661 epoch total loss 0.311909646\n",
      "Trained batch 267 batch loss 0.300067097 epoch total loss 0.31186527\n",
      "Trained batch 268 batch loss 0.302898586 epoch total loss 0.311831832\n",
      "Trained batch 269 batch loss 0.253762573 epoch total loss 0.311615944\n",
      "Trained batch 270 batch loss 0.334895253 epoch total loss 0.311702162\n",
      "Trained batch 271 batch loss 0.323852 epoch total loss 0.311746985\n",
      "Trained batch 272 batch loss 0.313676149 epoch total loss 0.311754078\n",
      "Trained batch 273 batch loss 0.351182461 epoch total loss 0.3118985\n",
      "Trained batch 274 batch loss 0.281036615 epoch total loss 0.311785877\n",
      "Trained batch 275 batch loss 0.28039974 epoch total loss 0.311671764\n",
      "Trained batch 276 batch loss 0.296781242 epoch total loss 0.311617821\n",
      "Trained batch 277 batch loss 0.314129144 epoch total loss 0.311626881\n",
      "Trained batch 278 batch loss 0.354274809 epoch total loss 0.311780304\n",
      "Trained batch 279 batch loss 0.302660048 epoch total loss 0.311747611\n",
      "Trained batch 280 batch loss 0.324029535 epoch total loss 0.31179148\n",
      "Trained batch 281 batch loss 0.292338133 epoch total loss 0.311722249\n",
      "Trained batch 282 batch loss 0.30216527 epoch total loss 0.311688334\n",
      "Trained batch 283 batch loss 0.318427026 epoch total loss 0.311712146\n",
      "Trained batch 284 batch loss 0.298904061 epoch total loss 0.311667055\n",
      "Trained batch 285 batch loss 0.322375476 epoch total loss 0.311704606\n",
      "Trained batch 286 batch loss 0.307350934 epoch total loss 0.311689377\n",
      "Trained batch 287 batch loss 0.327393889 epoch total loss 0.311744094\n",
      "Trained batch 288 batch loss 0.324131161 epoch total loss 0.311787128\n",
      "Trained batch 289 batch loss 0.313319743 epoch total loss 0.311792433\n",
      "Trained batch 290 batch loss 0.29100275 epoch total loss 0.311720729\n",
      "Trained batch 291 batch loss 0.281400442 epoch total loss 0.31161654\n",
      "Trained batch 292 batch loss 0.298639596 epoch total loss 0.311572075\n",
      "Trained batch 293 batch loss 0.314198017 epoch total loss 0.311581075\n",
      "Trained batch 294 batch loss 0.339252442 epoch total loss 0.311675161\n",
      "Trained batch 295 batch loss 0.319435298 epoch total loss 0.311701477\n",
      "Trained batch 296 batch loss 0.299829185 epoch total loss 0.311661363\n",
      "Trained batch 297 batch loss 0.304500699 epoch total loss 0.311637253\n",
      "Trained batch 298 batch loss 0.318596244 epoch total loss 0.311660618\n",
      "Trained batch 299 batch loss 0.290584207 epoch total loss 0.311590105\n",
      "Trained batch 300 batch loss 0.297098756 epoch total loss 0.311541796\n",
      "Trained batch 301 batch loss 0.307538182 epoch total loss 0.311528504\n",
      "Trained batch 302 batch loss 0.311678499 epoch total loss 0.311528981\n",
      "Trained batch 303 batch loss 0.330067068 epoch total loss 0.311590195\n",
      "Trained batch 304 batch loss 0.319562942 epoch total loss 0.311616421\n",
      "Trained batch 305 batch loss 0.362080961 epoch total loss 0.311781883\n",
      "Trained batch 306 batch loss 0.335950315 epoch total loss 0.311860889\n",
      "Trained batch 307 batch loss 0.321326464 epoch total loss 0.311891705\n",
      "Trained batch 308 batch loss 0.303907365 epoch total loss 0.311865807\n",
      "Trained batch 309 batch loss 0.316160023 epoch total loss 0.311879694\n",
      "Trained batch 310 batch loss 0.314068317 epoch total loss 0.311886758\n",
      "Trained batch 311 batch loss 0.298191249 epoch total loss 0.31184274\n",
      "Trained batch 312 batch loss 0.303313404 epoch total loss 0.311815411\n",
      "Trained batch 313 batch loss 0.314518809 epoch total loss 0.311824054\n",
      "Trained batch 314 batch loss 0.324312538 epoch total loss 0.31186381\n",
      "Trained batch 315 batch loss 0.306214154 epoch total loss 0.311845869\n",
      "Trained batch 316 batch loss 0.324770063 epoch total loss 0.311886787\n",
      "Trained batch 317 batch loss 0.30035919 epoch total loss 0.311850429\n",
      "Trained batch 318 batch loss 0.266713858 epoch total loss 0.31170848\n",
      "Trained batch 319 batch loss 0.325427115 epoch total loss 0.311751485\n",
      "Trained batch 320 batch loss 0.301242054 epoch total loss 0.311718643\n",
      "Trained batch 321 batch loss 0.289444 epoch total loss 0.311649233\n",
      "Trained batch 322 batch loss 0.291725069 epoch total loss 0.311587363\n",
      "Trained batch 323 batch loss 0.289134502 epoch total loss 0.311517835\n",
      "Trained batch 324 batch loss 0.294781804 epoch total loss 0.311466187\n",
      "Trained batch 325 batch loss 0.323520035 epoch total loss 0.311503261\n",
      "Trained batch 326 batch loss 0.313715369 epoch total loss 0.311510056\n",
      "Trained batch 327 batch loss 0.303126 epoch total loss 0.311484396\n",
      "Trained batch 328 batch loss 0.339360774 epoch total loss 0.311569393\n",
      "Trained batch 329 batch loss 0.320025802 epoch total loss 0.311595082\n",
      "Trained batch 330 batch loss 0.333895922 epoch total loss 0.311662674\n",
      "Trained batch 331 batch loss 0.367979914 epoch total loss 0.311832815\n",
      "Trained batch 332 batch loss 0.36619404 epoch total loss 0.311996549\n",
      "Trained batch 333 batch loss 0.326126754 epoch total loss 0.312039\n",
      "Trained batch 334 batch loss 0.283977777 epoch total loss 0.311954975\n",
      "Trained batch 335 batch loss 0.307202518 epoch total loss 0.311940789\n",
      "Trained batch 336 batch loss 0.308186799 epoch total loss 0.311929643\n",
      "Trained batch 337 batch loss 0.288918108 epoch total loss 0.311861336\n",
      "Trained batch 338 batch loss 0.288228035 epoch total loss 0.31179145\n",
      "Trained batch 339 batch loss 0.288526177 epoch total loss 0.311722815\n",
      "Trained batch 340 batch loss 0.291776359 epoch total loss 0.311664164\n",
      "Trained batch 341 batch loss 0.263118327 epoch total loss 0.311521769\n",
      "Trained batch 342 batch loss 0.273040354 epoch total loss 0.311409265\n",
      "Trained batch 343 batch loss 0.305757403 epoch total loss 0.311392784\n",
      "Trained batch 344 batch loss 0.322412252 epoch total loss 0.311424822\n",
      "Trained batch 345 batch loss 0.293444246 epoch total loss 0.311372697\n",
      "Trained batch 346 batch loss 0.284193367 epoch total loss 0.311294138\n",
      "Trained batch 347 batch loss 0.263899446 epoch total loss 0.311157554\n",
      "Trained batch 348 batch loss 0.290308148 epoch total loss 0.311097652\n",
      "Trained batch 349 batch loss 0.33244127 epoch total loss 0.311158806\n",
      "Trained batch 350 batch loss 0.350442678 epoch total loss 0.311271042\n",
      "Trained batch 351 batch loss 0.328708977 epoch total loss 0.311320722\n",
      "Trained batch 352 batch loss 0.319112241 epoch total loss 0.311342865\n",
      "Trained batch 353 batch loss 0.34724918 epoch total loss 0.311444581\n",
      "Trained batch 354 batch loss 0.340400606 epoch total loss 0.311526388\n",
      "Trained batch 355 batch loss 0.311181575 epoch total loss 0.311525404\n",
      "Trained batch 356 batch loss 0.321549475 epoch total loss 0.311553568\n",
      "Trained batch 357 batch loss 0.315112591 epoch total loss 0.311563522\n",
      "Trained batch 358 batch loss 0.3226403 epoch total loss 0.311594456\n",
      "Trained batch 359 batch loss 0.33398217 epoch total loss 0.311656833\n",
      "Trained batch 360 batch loss 0.340750962 epoch total loss 0.311737657\n",
      "Trained batch 361 batch loss 0.305793136 epoch total loss 0.311721176\n",
      "Trained batch 362 batch loss 0.313441902 epoch total loss 0.311725944\n",
      "Trained batch 363 batch loss 0.277445495 epoch total loss 0.311631501\n",
      "Trained batch 364 batch loss 0.278829217 epoch total loss 0.311541378\n",
      "Trained batch 365 batch loss 0.302464157 epoch total loss 0.311516523\n",
      "Trained batch 366 batch loss 0.306974798 epoch total loss 0.311504126\n",
      "Trained batch 367 batch loss 0.326636404 epoch total loss 0.311545342\n",
      "Trained batch 368 batch loss 0.31028071 epoch total loss 0.311541915\n",
      "Trained batch 369 batch loss 0.332722276 epoch total loss 0.311599314\n",
      "Trained batch 370 batch loss 0.335285097 epoch total loss 0.31166333\n",
      "Trained batch 371 batch loss 0.324553549 epoch total loss 0.311698079\n",
      "Trained batch 372 batch loss 0.308761686 epoch total loss 0.311690181\n",
      "Trained batch 373 batch loss 0.318368495 epoch total loss 0.311708063\n",
      "Trained batch 374 batch loss 0.278512359 epoch total loss 0.311619312\n",
      "Trained batch 375 batch loss 0.299983531 epoch total loss 0.311588287\n",
      "Trained batch 376 batch loss 0.293178767 epoch total loss 0.311539322\n",
      "Trained batch 377 batch loss 0.284073979 epoch total loss 0.311466455\n",
      "Trained batch 378 batch loss 0.291695982 epoch total loss 0.311414152\n",
      "Trained batch 379 batch loss 0.271807253 epoch total loss 0.311309665\n",
      "Trained batch 380 batch loss 0.296601593 epoch total loss 0.311270952\n",
      "Trained batch 381 batch loss 0.278960943 epoch total loss 0.311186135\n",
      "Trained batch 382 batch loss 0.323384017 epoch total loss 0.311218083\n",
      "Trained batch 383 batch loss 0.33692798 epoch total loss 0.311285228\n",
      "Trained batch 384 batch loss 0.311987 epoch total loss 0.311287045\n",
      "Trained batch 385 batch loss 0.304588199 epoch total loss 0.311269641\n",
      "Trained batch 386 batch loss 0.320979536 epoch total loss 0.311294794\n",
      "Trained batch 387 batch loss 0.313214391 epoch total loss 0.311299771\n",
      "Trained batch 388 batch loss 0.320911348 epoch total loss 0.311324537\n",
      "Trained batch 389 batch loss 0.319623113 epoch total loss 0.311345875\n",
      "Trained batch 390 batch loss 0.302518636 epoch total loss 0.311323225\n",
      "Trained batch 391 batch loss 0.28455776 epoch total loss 0.311254799\n",
      "Trained batch 392 batch loss 0.258134842 epoch total loss 0.311119288\n",
      "Trained batch 393 batch loss 0.280200541 epoch total loss 0.31104058\n",
      "Trained batch 394 batch loss 0.321068853 epoch total loss 0.311066031\n",
      "Trained batch 395 batch loss 0.330668151 epoch total loss 0.311115652\n",
      "Trained batch 396 batch loss 0.360690713 epoch total loss 0.311240852\n",
      "Trained batch 397 batch loss 0.32237196 epoch total loss 0.311268866\n",
      "Trained batch 398 batch loss 0.292847216 epoch total loss 0.311222583\n",
      "Trained batch 399 batch loss 0.315323621 epoch total loss 0.311232865\n",
      "Trained batch 400 batch loss 0.301918924 epoch total loss 0.311209589\n",
      "Trained batch 401 batch loss 0.318537533 epoch total loss 0.311227858\n",
      "Trained batch 402 batch loss 0.320515901 epoch total loss 0.311250955\n",
      "Trained batch 403 batch loss 0.313039064 epoch total loss 0.311255395\n",
      "Trained batch 404 batch loss 0.333337307 epoch total loss 0.311310053\n",
      "Trained batch 405 batch loss 0.30571124 epoch total loss 0.311296225\n",
      "Trained batch 406 batch loss 0.31312108 epoch total loss 0.311300725\n",
      "Trained batch 407 batch loss 0.291009068 epoch total loss 0.311250865\n",
      "Trained batch 408 batch loss 0.307608098 epoch total loss 0.311241925\n",
      "Trained batch 409 batch loss 0.299077928 epoch total loss 0.311212212\n",
      "Trained batch 410 batch loss 0.308630824 epoch total loss 0.311205894\n",
      "Trained batch 411 batch loss 0.265291154 epoch total loss 0.311094195\n",
      "Trained batch 412 batch loss 0.271967471 epoch total loss 0.310999244\n",
      "Trained batch 413 batch loss 0.300777495 epoch total loss 0.310974479\n",
      "Trained batch 414 batch loss 0.333894193 epoch total loss 0.311029851\n",
      "Trained batch 415 batch loss 0.33771053 epoch total loss 0.311094135\n",
      "Trained batch 416 batch loss 0.325217903 epoch total loss 0.31112808\n",
      "Trained batch 417 batch loss 0.287625343 epoch total loss 0.311071724\n",
      "Trained batch 418 batch loss 0.266397268 epoch total loss 0.310964853\n",
      "Trained batch 419 batch loss 0.289687216 epoch total loss 0.310914069\n",
      "Trained batch 420 batch loss 0.312754035 epoch total loss 0.31091845\n",
      "Trained batch 421 batch loss 0.332671165 epoch total loss 0.310970128\n",
      "Trained batch 422 batch loss 0.327620029 epoch total loss 0.311009586\n",
      "Trained batch 423 batch loss 0.334534049 epoch total loss 0.311065197\n",
      "Trained batch 424 batch loss 0.316030234 epoch total loss 0.311076909\n",
      "Trained batch 425 batch loss 0.327011555 epoch total loss 0.311114401\n",
      "Trained batch 426 batch loss 0.336494774 epoch total loss 0.311173975\n",
      "Trained batch 427 batch loss 0.332685411 epoch total loss 0.311224371\n",
      "Trained batch 428 batch loss 0.327079147 epoch total loss 0.311261386\n",
      "Trained batch 429 batch loss 0.323624194 epoch total loss 0.311290205\n",
      "Trained batch 430 batch loss 0.315061539 epoch total loss 0.311299\n",
      "Trained batch 431 batch loss 0.324570835 epoch total loss 0.311329782\n",
      "Trained batch 432 batch loss 0.309938 epoch total loss 0.311326563\n",
      "Trained batch 433 batch loss 0.31055963 epoch total loss 0.311324805\n",
      "Trained batch 434 batch loss 0.324916422 epoch total loss 0.311356127\n",
      "Trained batch 435 batch loss 0.299909145 epoch total loss 0.311329812\n",
      "Trained batch 436 batch loss 0.335719258 epoch total loss 0.311385751\n",
      "Trained batch 437 batch loss 0.305747092 epoch total loss 0.311372846\n",
      "Trained batch 438 batch loss 0.315767586 epoch total loss 0.31138286\n",
      "Trained batch 439 batch loss 0.331640929 epoch total loss 0.311429\n",
      "Trained batch 440 batch loss 0.308712602 epoch total loss 0.311422825\n",
      "Trained batch 441 batch loss 0.2910088 epoch total loss 0.311376572\n",
      "Trained batch 442 batch loss 0.297095567 epoch total loss 0.311344236\n",
      "Trained batch 443 batch loss 0.278913349 epoch total loss 0.311271042\n",
      "Trained batch 444 batch loss 0.282963783 epoch total loss 0.311207265\n",
      "Trained batch 445 batch loss 0.294439673 epoch total loss 0.311169565\n",
      "Trained batch 446 batch loss 0.29018271 epoch total loss 0.311122507\n",
      "Trained batch 447 batch loss 0.295288801 epoch total loss 0.311087072\n",
      "Trained batch 448 batch loss 0.278001815 epoch total loss 0.311013222\n",
      "Trained batch 449 batch loss 0.295157105 epoch total loss 0.310977906\n",
      "Trained batch 450 batch loss 0.278044194 epoch total loss 0.310904711\n",
      "Trained batch 451 batch loss 0.283840507 epoch total loss 0.310844719\n",
      "Trained batch 452 batch loss 0.319718 epoch total loss 0.310864329\n",
      "Trained batch 453 batch loss 0.328452 epoch total loss 0.310903162\n",
      "Trained batch 454 batch loss 0.315070271 epoch total loss 0.310912311\n",
      "Trained batch 455 batch loss 0.316346973 epoch total loss 0.310924262\n",
      "Trained batch 456 batch loss 0.304666787 epoch total loss 0.310910553\n",
      "Trained batch 457 batch loss 0.30368647 epoch total loss 0.310894728\n",
      "Trained batch 458 batch loss 0.263371974 epoch total loss 0.310790956\n",
      "Trained batch 459 batch loss 0.295146435 epoch total loss 0.310756862\n",
      "Trained batch 460 batch loss 0.305798858 epoch total loss 0.310746104\n",
      "Trained batch 461 batch loss 0.318032295 epoch total loss 0.310761929\n",
      "Trained batch 462 batch loss 0.351437241 epoch total loss 0.310849965\n",
      "Trained batch 463 batch loss 0.337794602 epoch total loss 0.310908169\n",
      "Trained batch 464 batch loss 0.325945139 epoch total loss 0.310940564\n",
      "Trained batch 465 batch loss 0.331725448 epoch total loss 0.310985267\n",
      "Trained batch 466 batch loss 0.335980505 epoch total loss 0.311038911\n",
      "Trained batch 467 batch loss 0.316302419 epoch total loss 0.311050177\n",
      "Trained batch 468 batch loss 0.305195183 epoch total loss 0.31103766\n",
      "Trained batch 469 batch loss 0.325503647 epoch total loss 0.311068505\n",
      "Trained batch 470 batch loss 0.291457236 epoch total loss 0.311026782\n",
      "Trained batch 471 batch loss 0.30543673 epoch total loss 0.31101492\n",
      "Trained batch 472 batch loss 0.314993471 epoch total loss 0.311023325\n",
      "Trained batch 473 batch loss 0.311873347 epoch total loss 0.311025113\n",
      "Trained batch 474 batch loss 0.302406877 epoch total loss 0.311006963\n",
      "Trained batch 475 batch loss 0.288764685 epoch total loss 0.310960114\n",
      "Trained batch 476 batch loss 0.314558059 epoch total loss 0.310967684\n",
      "Trained batch 477 batch loss 0.280589044 epoch total loss 0.310904\n",
      "Trained batch 478 batch loss 0.326074064 epoch total loss 0.310935766\n",
      "Trained batch 479 batch loss 0.270211101 epoch total loss 0.310850739\n",
      "Trained batch 480 batch loss 0.25938496 epoch total loss 0.310743511\n",
      "Trained batch 481 batch loss 0.25355652 epoch total loss 0.310624629\n",
      "Trained batch 482 batch loss 0.24293986 epoch total loss 0.310484201\n",
      "Trained batch 483 batch loss 0.2939955 epoch total loss 0.310450047\n",
      "Trained batch 484 batch loss 0.35245654 epoch total loss 0.310536861\n",
      "Trained batch 485 batch loss 0.332523584 epoch total loss 0.310582161\n",
      "Trained batch 486 batch loss 0.344600886 epoch total loss 0.310652167\n",
      "Trained batch 487 batch loss 0.275724888 epoch total loss 0.310580462\n",
      "Trained batch 488 batch loss 0.316230923 epoch total loss 0.310592055\n",
      "Trained batch 489 batch loss 0.305669278 epoch total loss 0.310581982\n",
      "Trained batch 490 batch loss 0.305412829 epoch total loss 0.310571432\n",
      "Trained batch 491 batch loss 0.318686217 epoch total loss 0.310587972\n",
      "Trained batch 492 batch loss 0.312495828 epoch total loss 0.310591847\n",
      "Trained batch 493 batch loss 0.335384399 epoch total loss 0.310642153\n",
      "Trained batch 494 batch loss 0.353454143 epoch total loss 0.310728818\n",
      "Trained batch 495 batch loss 0.335109413 epoch total loss 0.310778081\n",
      "Trained batch 496 batch loss 0.316637 epoch total loss 0.310789883\n",
      "Trained batch 497 batch loss 0.299690366 epoch total loss 0.310767561\n",
      "Trained batch 498 batch loss 0.28349036 epoch total loss 0.310712785\n",
      "Trained batch 499 batch loss 0.30351615 epoch total loss 0.31069836\n",
      "Trained batch 500 batch loss 0.298450619 epoch total loss 0.310673863\n",
      "Trained batch 501 batch loss 0.305530608 epoch total loss 0.310663581\n",
      "Trained batch 502 batch loss 0.327196807 epoch total loss 0.310696512\n",
      "Trained batch 503 batch loss 0.288371056 epoch total loss 0.310652137\n",
      "Trained batch 504 batch loss 0.315412283 epoch total loss 0.310661584\n",
      "Trained batch 505 batch loss 0.324049354 epoch total loss 0.310688108\n",
      "Trained batch 506 batch loss 0.318003923 epoch total loss 0.310702562\n",
      "Trained batch 507 batch loss 0.344145536 epoch total loss 0.310768545\n",
      "Trained batch 508 batch loss 0.284421086 epoch total loss 0.310716689\n",
      "Trained batch 509 batch loss 0.263282299 epoch total loss 0.310623467\n",
      "Trained batch 510 batch loss 0.279353708 epoch total loss 0.310562164\n",
      "Trained batch 511 batch loss 0.285712421 epoch total loss 0.310513526\n",
      "Trained batch 512 batch loss 0.293152779 epoch total loss 0.310479611\n",
      "Trained batch 513 batch loss 0.304355919 epoch total loss 0.31046766\n",
      "Trained batch 514 batch loss 0.294440359 epoch total loss 0.310436457\n",
      "Trained batch 515 batch loss 0.321619362 epoch total loss 0.310458183\n",
      "Trained batch 516 batch loss 0.332182884 epoch total loss 0.310500294\n",
      "Trained batch 517 batch loss 0.306980759 epoch total loss 0.310493469\n",
      "Trained batch 518 batch loss 0.309580922 epoch total loss 0.310491741\n",
      "Trained batch 519 batch loss 0.349692434 epoch total loss 0.31056726\n",
      "Trained batch 520 batch loss 0.365834862 epoch total loss 0.310673535\n",
      "Trained batch 521 batch loss 0.331425458 epoch total loss 0.310713351\n",
      "Trained batch 522 batch loss 0.295536041 epoch total loss 0.310684264\n",
      "Trained batch 523 batch loss 0.303906649 epoch total loss 0.3106713\n",
      "Trained batch 524 batch loss 0.317728519 epoch total loss 0.3106848\n",
      "Trained batch 525 batch loss 0.286114663 epoch total loss 0.310637981\n",
      "Trained batch 526 batch loss 0.290080965 epoch total loss 0.31059891\n",
      "Trained batch 527 batch loss 0.313505977 epoch total loss 0.310604423\n",
      "Trained batch 528 batch loss 0.286467433 epoch total loss 0.310558736\n",
      "Trained batch 529 batch loss 0.263804525 epoch total loss 0.310470343\n",
      "Trained batch 530 batch loss 0.233786196 epoch total loss 0.310325652\n",
      "Trained batch 531 batch loss 0.270303488 epoch total loss 0.310250282\n",
      "Trained batch 532 batch loss 0.318657488 epoch total loss 0.310266107\n",
      "Trained batch 533 batch loss 0.32431066 epoch total loss 0.310292453\n",
      "Trained batch 534 batch loss 0.311531723 epoch total loss 0.310294777\n",
      "Trained batch 535 batch loss 0.313398123 epoch total loss 0.310300589\n",
      "Trained batch 536 batch loss 0.296848685 epoch total loss 0.310275495\n",
      "Trained batch 537 batch loss 0.270153105 epoch total loss 0.310200781\n",
      "Trained batch 538 batch loss 0.272580385 epoch total loss 0.310130864\n",
      "Trained batch 539 batch loss 0.296500742 epoch total loss 0.310105562\n",
      "Trained batch 540 batch loss 0.298011869 epoch total loss 0.310083181\n",
      "Trained batch 541 batch loss 0.347649574 epoch total loss 0.31015262\n",
      "Trained batch 542 batch loss 0.306158334 epoch total loss 0.310145259\n",
      "Trained batch 543 batch loss 0.30992341 epoch total loss 0.310144842\n",
      "Trained batch 544 batch loss 0.280195 epoch total loss 0.310089797\n",
      "Trained batch 545 batch loss 0.274016112 epoch total loss 0.310023606\n",
      "Trained batch 546 batch loss 0.279984295 epoch total loss 0.309968591\n",
      "Trained batch 547 batch loss 0.281911403 epoch total loss 0.309917271\n",
      "Trained batch 548 batch loss 0.257390022 epoch total loss 0.309821427\n",
      "Trained batch 549 batch loss 0.300223112 epoch total loss 0.309803933\n",
      "Trained batch 550 batch loss 0.287841439 epoch total loss 0.309764\n",
      "Trained batch 551 batch loss 0.284850568 epoch total loss 0.309718788\n",
      "Trained batch 552 batch loss 0.293411672 epoch total loss 0.309689224\n",
      "Trained batch 553 batch loss 0.276612222 epoch total loss 0.309629411\n",
      "Trained batch 554 batch loss 0.280924499 epoch total loss 0.309577614\n",
      "Trained batch 555 batch loss 0.281937689 epoch total loss 0.309527814\n",
      "Trained batch 556 batch loss 0.349473208 epoch total loss 0.309599638\n",
      "Trained batch 557 batch loss 0.344061494 epoch total loss 0.309661508\n",
      "Trained batch 558 batch loss 0.322138578 epoch total loss 0.309683889\n",
      "Trained batch 559 batch loss 0.32432434 epoch total loss 0.309710085\n",
      "Trained batch 560 batch loss 0.339623183 epoch total loss 0.309763491\n",
      "Trained batch 561 batch loss 0.307532907 epoch total loss 0.309759498\n",
      "Trained batch 562 batch loss 0.325557 epoch total loss 0.309787631\n",
      "Trained batch 563 batch loss 0.278014839 epoch total loss 0.309731185\n",
      "Trained batch 564 batch loss 0.310151726 epoch total loss 0.30973193\n",
      "Trained batch 565 batch loss 0.317693859 epoch total loss 0.309746027\n",
      "Trained batch 566 batch loss 0.308805376 epoch total loss 0.309744358\n",
      "Trained batch 567 batch loss 0.303068668 epoch total loss 0.309732586\n",
      "Trained batch 568 batch loss 0.337516 epoch total loss 0.309781492\n",
      "Trained batch 569 batch loss 0.353596777 epoch total loss 0.309858501\n",
      "Trained batch 570 batch loss 0.350794315 epoch total loss 0.309930325\n",
      "Trained batch 571 batch loss 0.334006876 epoch total loss 0.309972465\n",
      "Trained batch 572 batch loss 0.315166563 epoch total loss 0.309981555\n",
      "Trained batch 573 batch loss 0.303440839 epoch total loss 0.30997014\n",
      "Trained batch 574 batch loss 0.311960399 epoch total loss 0.309973598\n",
      "Trained batch 575 batch loss 0.333457649 epoch total loss 0.310014427\n",
      "Trained batch 576 batch loss 0.328618497 epoch total loss 0.310046732\n",
      "Trained batch 577 batch loss 0.366345406 epoch total loss 0.310144305\n",
      "Trained batch 578 batch loss 0.333197474 epoch total loss 0.310184181\n",
      "Trained batch 579 batch loss 0.331870675 epoch total loss 0.310221612\n",
      "Trained batch 580 batch loss 0.310336024 epoch total loss 0.310221821\n",
      "Trained batch 581 batch loss 0.304476112 epoch total loss 0.310211927\n",
      "Trained batch 582 batch loss 0.351702273 epoch total loss 0.310283214\n",
      "Trained batch 583 batch loss 0.353798807 epoch total loss 0.310357869\n",
      "Trained batch 584 batch loss 0.305715472 epoch total loss 0.310349882\n",
      "Trained batch 585 batch loss 0.26833877 epoch total loss 0.310278088\n",
      "Trained batch 586 batch loss 0.332254916 epoch total loss 0.310315609\n",
      "Trained batch 587 batch loss 0.315049469 epoch total loss 0.310323656\n",
      "Trained batch 588 batch loss 0.323220611 epoch total loss 0.31034559\n",
      "Trained batch 589 batch loss 0.326970667 epoch total loss 0.310373813\n",
      "Trained batch 590 batch loss 0.330816627 epoch total loss 0.310408473\n",
      "Trained batch 591 batch loss 0.348803937 epoch total loss 0.310473412\n",
      "Trained batch 592 batch loss 0.321451455 epoch total loss 0.310491979\n",
      "Trained batch 593 batch loss 0.306050122 epoch total loss 0.310484469\n",
      "Trained batch 594 batch loss 0.337106943 epoch total loss 0.310529292\n",
      "Trained batch 595 batch loss 0.364329845 epoch total loss 0.310619742\n",
      "Trained batch 596 batch loss 0.330816388 epoch total loss 0.310653597\n",
      "Trained batch 597 batch loss 0.306692958 epoch total loss 0.310646951\n",
      "Trained batch 598 batch loss 0.292706668 epoch total loss 0.31061697\n",
      "Trained batch 599 batch loss 0.298747361 epoch total loss 0.310597152\n",
      "Trained batch 600 batch loss 0.306017488 epoch total loss 0.310589522\n",
      "Trained batch 601 batch loss 0.308883727 epoch total loss 0.310586691\n",
      "Trained batch 602 batch loss 0.309989095 epoch total loss 0.310585678\n",
      "Trained batch 603 batch loss 0.296155542 epoch total loss 0.310561746\n",
      "Trained batch 604 batch loss 0.304889768 epoch total loss 0.310552359\n",
      "Trained batch 605 batch loss 0.302523971 epoch total loss 0.310539067\n",
      "Trained batch 606 batch loss 0.297578424 epoch total loss 0.310517699\n",
      "Trained batch 607 batch loss 0.296782881 epoch total loss 0.310495079\n",
      "Trained batch 608 batch loss 0.259134471 epoch total loss 0.310410589\n",
      "Trained batch 609 batch loss 0.273020744 epoch total loss 0.310349226\n",
      "Trained batch 610 batch loss 0.28460741 epoch total loss 0.310307026\n",
      "Trained batch 611 batch loss 0.27594018 epoch total loss 0.310250759\n",
      "Trained batch 612 batch loss 0.29088378 epoch total loss 0.310219109\n",
      "Trained batch 613 batch loss 0.288610339 epoch total loss 0.310183853\n",
      "Trained batch 614 batch loss 0.28229028 epoch total loss 0.310138404\n",
      "Trained batch 615 batch loss 0.274229825 epoch total loss 0.310080022\n",
      "Trained batch 616 batch loss 0.311037719 epoch total loss 0.310081571\n",
      "Trained batch 617 batch loss 0.288027674 epoch total loss 0.310045838\n",
      "Trained batch 618 batch loss 0.294862539 epoch total loss 0.310021251\n",
      "Trained batch 619 batch loss 0.282549202 epoch total loss 0.309976876\n",
      "Trained batch 620 batch loss 0.295057535 epoch total loss 0.309952825\n",
      "Trained batch 621 batch loss 0.306516796 epoch total loss 0.309947282\n",
      "Trained batch 622 batch loss 0.304105759 epoch total loss 0.309937894\n",
      "Trained batch 623 batch loss 0.305206358 epoch total loss 0.309930295\n",
      "Trained batch 624 batch loss 0.359247863 epoch total loss 0.310009331\n",
      "Trained batch 625 batch loss 0.34469679 epoch total loss 0.310064852\n",
      "Trained batch 626 batch loss 0.317673087 epoch total loss 0.310077\n",
      "Trained batch 627 batch loss 0.30745 epoch total loss 0.310072809\n",
      "Trained batch 628 batch loss 0.28212595 epoch total loss 0.310028285\n",
      "Trained batch 629 batch loss 0.305240154 epoch total loss 0.310020685\n",
      "Trained batch 630 batch loss 0.298037529 epoch total loss 0.310001642\n",
      "Trained batch 631 batch loss 0.309573263 epoch total loss 0.310000956\n",
      "Trained batch 632 batch loss 0.29720512 epoch total loss 0.30998072\n",
      "Trained batch 633 batch loss 0.310677886 epoch total loss 0.309981853\n",
      "Trained batch 634 batch loss 0.321498901 epoch total loss 0.31\n",
      "Trained batch 635 batch loss 0.297860265 epoch total loss 0.309980899\n",
      "Trained batch 636 batch loss 0.288162947 epoch total loss 0.309946597\n",
      "Trained batch 637 batch loss 0.302551985 epoch total loss 0.309935\n",
      "Trained batch 638 batch loss 0.302791834 epoch total loss 0.309923798\n",
      "Trained batch 639 batch loss 0.346140087 epoch total loss 0.309980482\n",
      "Trained batch 640 batch loss 0.318106413 epoch total loss 0.309993178\n",
      "Trained batch 641 batch loss 0.282690614 epoch total loss 0.30995056\n",
      "Trained batch 642 batch loss 0.296848834 epoch total loss 0.309930146\n",
      "Trained batch 643 batch loss 0.297612339 epoch total loss 0.309910983\n",
      "Trained batch 644 batch loss 0.304690182 epoch total loss 0.309902877\n",
      "Trained batch 645 batch loss 0.318541706 epoch total loss 0.309916288\n",
      "Trained batch 646 batch loss 0.300908834 epoch total loss 0.30990231\n",
      "Trained batch 647 batch loss 0.298933566 epoch total loss 0.309885383\n",
      "Trained batch 648 batch loss 0.286978215 epoch total loss 0.30985\n",
      "Trained batch 649 batch loss 0.300377935 epoch total loss 0.309835434\n",
      "Trained batch 650 batch loss 0.28074652 epoch total loss 0.309790671\n",
      "Trained batch 651 batch loss 0.295249939 epoch total loss 0.309768319\n",
      "Trained batch 652 batch loss 0.301036835 epoch total loss 0.309754938\n",
      "Trained batch 653 batch loss 0.296274066 epoch total loss 0.309734315\n",
      "Trained batch 654 batch loss 0.315009236 epoch total loss 0.309742361\n",
      "Trained batch 655 batch loss 0.319046259 epoch total loss 0.309756577\n",
      "Trained batch 656 batch loss 0.267156243 epoch total loss 0.309691608\n",
      "Trained batch 657 batch loss 0.316428781 epoch total loss 0.30970186\n",
      "Trained batch 658 batch loss 0.32686758 epoch total loss 0.309727967\n",
      "Trained batch 659 batch loss 0.326364368 epoch total loss 0.309753209\n",
      "Trained batch 660 batch loss 0.297178149 epoch total loss 0.309734166\n",
      "Trained batch 661 batch loss 0.26167962 epoch total loss 0.309661448\n",
      "Trained batch 662 batch loss 0.273216724 epoch total loss 0.309606403\n",
      "Trained batch 663 batch loss 0.284472913 epoch total loss 0.309568495\n",
      "Trained batch 664 batch loss 0.260733217 epoch total loss 0.309494942\n",
      "Trained batch 665 batch loss 0.264428198 epoch total loss 0.309427172\n",
      "Trained batch 666 batch loss 0.276171744 epoch total loss 0.309377253\n",
      "Trained batch 667 batch loss 0.287253559 epoch total loss 0.309344053\n",
      "Trained batch 668 batch loss 0.29210943 epoch total loss 0.309318274\n",
      "Trained batch 669 batch loss 0.30241698 epoch total loss 0.309307963\n",
      "Trained batch 670 batch loss 0.297341317 epoch total loss 0.309290111\n",
      "Trained batch 671 batch loss 0.312739968 epoch total loss 0.309295237\n",
      "Trained batch 672 batch loss 0.308394372 epoch total loss 0.309293896\n",
      "Trained batch 673 batch loss 0.316991806 epoch total loss 0.30930534\n",
      "Trained batch 674 batch loss 0.306882083 epoch total loss 0.309301734\n",
      "Trained batch 675 batch loss 0.325962543 epoch total loss 0.30932641\n",
      "Trained batch 676 batch loss 0.325396925 epoch total loss 0.309350193\n",
      "Trained batch 677 batch loss 0.350006 epoch total loss 0.309410244\n",
      "Trained batch 678 batch loss 0.341528952 epoch total loss 0.3094576\n",
      "Trained batch 679 batch loss 0.330731511 epoch total loss 0.309488952\n",
      "Trained batch 680 batch loss 0.300763428 epoch total loss 0.309476107\n",
      "Trained batch 681 batch loss 0.267866105 epoch total loss 0.309415\n",
      "Trained batch 682 batch loss 0.260317266 epoch total loss 0.309343\n",
      "Trained batch 683 batch loss 0.282260418 epoch total loss 0.309303373\n",
      "Trained batch 684 batch loss 0.284913599 epoch total loss 0.3092677\n",
      "Trained batch 685 batch loss 0.244491667 epoch total loss 0.309173137\n",
      "Trained batch 686 batch loss 0.23477751 epoch total loss 0.309064686\n",
      "Trained batch 687 batch loss 0.255118817 epoch total loss 0.308986157\n",
      "Trained batch 688 batch loss 0.259968281 epoch total loss 0.3089149\n",
      "Trained batch 689 batch loss 0.294465601 epoch total loss 0.308893919\n",
      "Trained batch 690 batch loss 0.312085092 epoch total loss 0.308898538\n",
      "Trained batch 691 batch loss 0.32980898 epoch total loss 0.308928818\n",
      "Trained batch 692 batch loss 0.314184695 epoch total loss 0.308936387\n",
      "Trained batch 693 batch loss 0.30730015 epoch total loss 0.308934033\n",
      "Trained batch 694 batch loss 0.323807865 epoch total loss 0.308955461\n",
      "Trained batch 695 batch loss 0.325075209 epoch total loss 0.308978647\n",
      "Trained batch 696 batch loss 0.307215869 epoch total loss 0.308976114\n",
      "Trained batch 697 batch loss 0.337702751 epoch total loss 0.30901733\n",
      "Trained batch 698 batch loss 0.295584917 epoch total loss 0.308998078\n",
      "Trained batch 699 batch loss 0.330738962 epoch total loss 0.309029192\n",
      "Trained batch 700 batch loss 0.311371744 epoch total loss 0.30903253\n",
      "Trained batch 701 batch loss 0.274615258 epoch total loss 0.308983415\n",
      "Trained batch 702 batch loss 0.306876123 epoch total loss 0.308980405\n",
      "Trained batch 703 batch loss 0.314555258 epoch total loss 0.308988363\n",
      "Trained batch 704 batch loss 0.329963863 epoch total loss 0.309018165\n",
      "Trained batch 705 batch loss 0.326932251 epoch total loss 0.309043556\n",
      "Trained batch 706 batch loss 0.334353149 epoch total loss 0.309079409\n",
      "Trained batch 707 batch loss 0.323618144 epoch total loss 0.3091\n",
      "Trained batch 708 batch loss 0.327832788 epoch total loss 0.309126437\n",
      "Trained batch 709 batch loss 0.339356661 epoch total loss 0.309169084\n",
      "Trained batch 710 batch loss 0.29621762 epoch total loss 0.309150845\n",
      "Trained batch 711 batch loss 0.327899098 epoch total loss 0.30917722\n",
      "Trained batch 712 batch loss 0.298511833 epoch total loss 0.309162229\n",
      "Trained batch 713 batch loss 0.290121853 epoch total loss 0.309135526\n",
      "Trained batch 714 batch loss 0.29625687 epoch total loss 0.309117466\n",
      "Trained batch 715 batch loss 0.292073786 epoch total loss 0.309093624\n",
      "Trained batch 716 batch loss 0.298795849 epoch total loss 0.30907923\n",
      "Trained batch 717 batch loss 0.295711666 epoch total loss 0.309060603\n",
      "Trained batch 718 batch loss 0.309878498 epoch total loss 0.309061736\n",
      "Trained batch 719 batch loss 0.305793107 epoch total loss 0.309057176\n",
      "Trained batch 720 batch loss 0.312810332 epoch total loss 0.309062392\n",
      "Trained batch 721 batch loss 0.305532783 epoch total loss 0.309057474\n",
      "Trained batch 722 batch loss 0.280701548 epoch total loss 0.309018195\n",
      "Trained batch 723 batch loss 0.300524056 epoch total loss 0.309006453\n",
      "Trained batch 724 batch loss 0.290654 epoch total loss 0.308981091\n",
      "Trained batch 725 batch loss 0.287532806 epoch total loss 0.308951527\n",
      "Trained batch 726 batch loss 0.293661594 epoch total loss 0.308930457\n",
      "Trained batch 727 batch loss 0.326015949 epoch total loss 0.308953971\n",
      "Trained batch 728 batch loss 0.322608858 epoch total loss 0.308972716\n",
      "Trained batch 729 batch loss 0.308407217 epoch total loss 0.308971941\n",
      "Trained batch 730 batch loss 0.324947894 epoch total loss 0.308993816\n",
      "Trained batch 731 batch loss 0.312125981 epoch total loss 0.308998108\n",
      "Trained batch 732 batch loss 0.322595716 epoch total loss 0.309016675\n",
      "Trained batch 733 batch loss 0.296103448 epoch total loss 0.308999062\n",
      "Trained batch 734 batch loss 0.289534807 epoch total loss 0.308972538\n",
      "Trained batch 735 batch loss 0.29444775 epoch total loss 0.308952779\n",
      "Trained batch 736 batch loss 0.28091222 epoch total loss 0.308914691\n",
      "Trained batch 737 batch loss 0.309197575 epoch total loss 0.308915079\n",
      "Trained batch 738 batch loss 0.331924886 epoch total loss 0.308946252\n",
      "Trained batch 739 batch loss 0.32333988 epoch total loss 0.308965713\n",
      "Trained batch 740 batch loss 0.291456044 epoch total loss 0.30894205\n",
      "Trained batch 741 batch loss 0.284979045 epoch total loss 0.308909714\n",
      "Trained batch 742 batch loss 0.299343407 epoch total loss 0.30889684\n",
      "Trained batch 743 batch loss 0.278446436 epoch total loss 0.308855832\n",
      "Trained batch 744 batch loss 0.282042086 epoch total loss 0.308819801\n",
      "Trained batch 745 batch loss 0.275986314 epoch total loss 0.308775723\n",
      "Trained batch 746 batch loss 0.320566 epoch total loss 0.308791548\n",
      "Trained batch 747 batch loss 0.305175185 epoch total loss 0.30878669\n",
      "Trained batch 748 batch loss 0.285843 epoch total loss 0.308756024\n",
      "Trained batch 749 batch loss 0.327142388 epoch total loss 0.308780581\n",
      "Trained batch 750 batch loss 0.329111 epoch total loss 0.308807701\n",
      "Trained batch 751 batch loss 0.370145023 epoch total loss 0.308889389\n",
      "Trained batch 752 batch loss 0.400346637 epoch total loss 0.309010983\n",
      "Trained batch 753 batch loss 0.355126858 epoch total loss 0.309072256\n",
      "Trained batch 754 batch loss 0.313163966 epoch total loss 0.30907768\n",
      "Trained batch 755 batch loss 0.294819593 epoch total loss 0.309058785\n",
      "Trained batch 756 batch loss 0.323371321 epoch total loss 0.30907771\n",
      "Trained batch 757 batch loss 0.332799345 epoch total loss 0.309109032\n",
      "Trained batch 758 batch loss 0.330016702 epoch total loss 0.309136629\n",
      "Trained batch 759 batch loss 0.303113878 epoch total loss 0.309128702\n",
      "Trained batch 760 batch loss 0.357832253 epoch total loss 0.309192777\n",
      "Trained batch 761 batch loss 0.294601113 epoch total loss 0.309173614\n",
      "Trained batch 762 batch loss 0.299896657 epoch total loss 0.309161425\n",
      "Trained batch 763 batch loss 0.273357213 epoch total loss 0.309114516\n",
      "Trained batch 764 batch loss 0.273956597 epoch total loss 0.309068501\n",
      "Trained batch 765 batch loss 0.369763225 epoch total loss 0.309147835\n",
      "Trained batch 766 batch loss 0.340127796 epoch total loss 0.309188277\n",
      "Trained batch 767 batch loss 0.311207116 epoch total loss 0.309190899\n",
      "Trained batch 768 batch loss 0.312253296 epoch total loss 0.309194893\n",
      "Trained batch 769 batch loss 0.319696873 epoch total loss 0.309208572\n",
      "Trained batch 770 batch loss 0.292602271 epoch total loss 0.309187\n",
      "Trained batch 771 batch loss 0.296865135 epoch total loss 0.309171021\n",
      "Trained batch 772 batch loss 0.286176175 epoch total loss 0.309141219\n",
      "Trained batch 773 batch loss 0.311683893 epoch total loss 0.309144527\n",
      "Trained batch 774 batch loss 0.317586958 epoch total loss 0.309155434\n",
      "Trained batch 775 batch loss 0.307910711 epoch total loss 0.309153825\n",
      "Trained batch 776 batch loss 0.305564165 epoch total loss 0.309149176\n",
      "Trained batch 777 batch loss 0.331184417 epoch total loss 0.309177548\n",
      "Trained batch 778 batch loss 0.28885603 epoch total loss 0.309151411\n",
      "Trained batch 779 batch loss 0.283213794 epoch total loss 0.309118122\n",
      "Trained batch 780 batch loss 0.293194294 epoch total loss 0.309097707\n",
      "Trained batch 781 batch loss 0.306039393 epoch total loss 0.309093803\n",
      "Trained batch 782 batch loss 0.277997583 epoch total loss 0.309054047\n",
      "Trained batch 783 batch loss 0.288982838 epoch total loss 0.309028417\n",
      "Trained batch 784 batch loss 0.284644425 epoch total loss 0.308997303\n",
      "Trained batch 785 batch loss 0.295842916 epoch total loss 0.308980554\n",
      "Trained batch 786 batch loss 0.271967918 epoch total loss 0.308933467\n",
      "Trained batch 787 batch loss 0.311710387 epoch total loss 0.308936983\n",
      "Trained batch 788 batch loss 0.329187393 epoch total loss 0.308962703\n",
      "Trained batch 789 batch loss 0.333730459 epoch total loss 0.308994085\n",
      "Trained batch 790 batch loss 0.318499774 epoch total loss 0.309006095\n",
      "Trained batch 791 batch loss 0.308622062 epoch total loss 0.309005618\n",
      "Trained batch 792 batch loss 0.300736219 epoch total loss 0.308995187\n",
      "Trained batch 793 batch loss 0.273042858 epoch total loss 0.308949828\n",
      "Trained batch 794 batch loss 0.271915972 epoch total loss 0.308903188\n",
      "Trained batch 795 batch loss 0.308398515 epoch total loss 0.308902562\n",
      "Trained batch 796 batch loss 0.318789065 epoch total loss 0.308914959\n",
      "Trained batch 797 batch loss 0.315183192 epoch total loss 0.308922827\n",
      "Trained batch 798 batch loss 0.3126688 epoch total loss 0.308927536\n",
      "Trained batch 799 batch loss 0.299819589 epoch total loss 0.308916122\n",
      "Trained batch 800 batch loss 0.316818804 epoch total loss 0.308926016\n",
      "Trained batch 801 batch loss 0.28885746 epoch total loss 0.308900952\n",
      "Trained batch 802 batch loss 0.282566607 epoch total loss 0.30886811\n",
      "Trained batch 803 batch loss 0.281654984 epoch total loss 0.308834255\n",
      "Trained batch 804 batch loss 0.32052353 epoch total loss 0.308848798\n",
      "Trained batch 805 batch loss 0.306302845 epoch total loss 0.308845609\n",
      "Trained batch 806 batch loss 0.273356527 epoch total loss 0.308801591\n",
      "Trained batch 807 batch loss 0.281971276 epoch total loss 0.308768332\n",
      "Trained batch 808 batch loss 0.282159388 epoch total loss 0.30873543\n",
      "Trained batch 809 batch loss 0.279852718 epoch total loss 0.308699727\n",
      "Trained batch 810 batch loss 0.33455047 epoch total loss 0.308731616\n",
      "Trained batch 811 batch loss 0.34597522 epoch total loss 0.308777541\n",
      "Trained batch 812 batch loss 0.356545895 epoch total loss 0.308836401\n",
      "Trained batch 813 batch loss 0.335092127 epoch total loss 0.308868676\n",
      "Trained batch 814 batch loss 0.343544632 epoch total loss 0.308911294\n",
      "Trained batch 815 batch loss 0.322104484 epoch total loss 0.308927476\n",
      "Trained batch 816 batch loss 0.287582815 epoch total loss 0.30890131\n",
      "Trained batch 817 batch loss 0.295819849 epoch total loss 0.308885306\n",
      "Trained batch 818 batch loss 0.30412066 epoch total loss 0.308879495\n",
      "Trained batch 819 batch loss 0.320854 epoch total loss 0.308894098\n",
      "Trained batch 820 batch loss 0.290065855 epoch total loss 0.30887115\n",
      "Trained batch 821 batch loss 0.312570959 epoch total loss 0.30887565\n",
      "Trained batch 822 batch loss 0.313610971 epoch total loss 0.308881432\n",
      "Trained batch 823 batch loss 0.299232811 epoch total loss 0.30886972\n",
      "Trained batch 824 batch loss 0.31391421 epoch total loss 0.308875829\n",
      "Trained batch 825 batch loss 0.292588234 epoch total loss 0.3088561\n",
      "Trained batch 826 batch loss 0.319871157 epoch total loss 0.308869421\n",
      "Trained batch 827 batch loss 0.314854026 epoch total loss 0.308876663\n",
      "Trained batch 828 batch loss 0.307578623 epoch total loss 0.308875084\n",
      "Trained batch 829 batch loss 0.337108821 epoch total loss 0.308909118\n",
      "Trained batch 830 batch loss 0.285931587 epoch total loss 0.308881432\n",
      "Trained batch 831 batch loss 0.265514553 epoch total loss 0.308829218\n",
      "Trained batch 832 batch loss 0.28759563 epoch total loss 0.308803707\n",
      "Trained batch 833 batch loss 0.285295814 epoch total loss 0.308775514\n",
      "Trained batch 834 batch loss 0.324353844 epoch total loss 0.308794171\n",
      "Trained batch 835 batch loss 0.306756258 epoch total loss 0.308791727\n",
      "Trained batch 836 batch loss 0.279389948 epoch total loss 0.30875656\n",
      "Trained batch 837 batch loss 0.281560123 epoch total loss 0.308724046\n",
      "Trained batch 838 batch loss 0.283863187 epoch total loss 0.308694392\n",
      "Trained batch 839 batch loss 0.322614849 epoch total loss 0.308711\n",
      "Trained batch 840 batch loss 0.320581079 epoch total loss 0.308725119\n",
      "Trained batch 841 batch loss 0.323791593 epoch total loss 0.30874303\n",
      "Trained batch 842 batch loss 0.296176195 epoch total loss 0.308728099\n",
      "Trained batch 843 batch loss 0.252936542 epoch total loss 0.308661908\n",
      "Trained batch 844 batch loss 0.233858198 epoch total loss 0.308573276\n",
      "Trained batch 845 batch loss 0.242209956 epoch total loss 0.308494747\n",
      "Trained batch 846 batch loss 0.268368542 epoch total loss 0.308447331\n",
      "Trained batch 847 batch loss 0.36356616 epoch total loss 0.30851239\n",
      "Trained batch 848 batch loss 0.353744686 epoch total loss 0.308565766\n",
      "Trained batch 849 batch loss 0.283970624 epoch total loss 0.308536768\n",
      "Trained batch 850 batch loss 0.287991166 epoch total loss 0.308512598\n",
      "Trained batch 851 batch loss 0.297860295 epoch total loss 0.308500081\n",
      "Trained batch 852 batch loss 0.310281932 epoch total loss 0.308502167\n",
      "Trained batch 853 batch loss 0.289225072 epoch total loss 0.308479548\n",
      "Trained batch 854 batch loss 0.257224023 epoch total loss 0.308419555\n",
      "Trained batch 855 batch loss 0.254491836 epoch total loss 0.308356464\n",
      "Trained batch 856 batch loss 0.301769942 epoch total loss 0.308348745\n",
      "Trained batch 857 batch loss 0.269265622 epoch total loss 0.308303148\n",
      "Trained batch 858 batch loss 0.274099827 epoch total loss 0.308263272\n",
      "Trained batch 859 batch loss 0.280926913 epoch total loss 0.308231443\n",
      "Trained batch 860 batch loss 0.288597941 epoch total loss 0.308208615\n",
      "Trained batch 861 batch loss 0.322070539 epoch total loss 0.308224738\n",
      "Trained batch 862 batch loss 0.288371742 epoch total loss 0.3082017\n",
      "Trained batch 863 batch loss 0.279060811 epoch total loss 0.308167905\n",
      "Trained batch 864 batch loss 0.302161574 epoch total loss 0.308160961\n",
      "Trained batch 865 batch loss 0.343694597 epoch total loss 0.308202028\n",
      "Trained batch 866 batch loss 0.302622437 epoch total loss 0.308195591\n",
      "Trained batch 867 batch loss 0.319203138 epoch total loss 0.308208287\n",
      "Trained batch 868 batch loss 0.366977721 epoch total loss 0.308276\n",
      "Trained batch 869 batch loss 0.369281054 epoch total loss 0.308346212\n",
      "Trained batch 870 batch loss 0.344333172 epoch total loss 0.308387578\n",
      "Trained batch 871 batch loss 0.282125622 epoch total loss 0.308357418\n",
      "Trained batch 872 batch loss 0.321864188 epoch total loss 0.308372915\n",
      "Trained batch 873 batch loss 0.295707881 epoch total loss 0.308358431\n",
      "Trained batch 874 batch loss 0.29292345 epoch total loss 0.308340788\n",
      "Trained batch 875 batch loss 0.305364668 epoch total loss 0.308337361\n",
      "Trained batch 876 batch loss 0.310114324 epoch total loss 0.308339387\n",
      "Trained batch 877 batch loss 0.331716508 epoch total loss 0.30836606\n",
      "Trained batch 878 batch loss 0.324032545 epoch total loss 0.308383912\n",
      "Trained batch 879 batch loss 0.35552755 epoch total loss 0.308437556\n",
      "Trained batch 880 batch loss 0.340698063 epoch total loss 0.308474213\n",
      "Trained batch 881 batch loss 0.308839172 epoch total loss 0.30847463\n",
      "Trained batch 882 batch loss 0.292883337 epoch total loss 0.308456928\n",
      "Trained batch 883 batch loss 0.30696255 epoch total loss 0.308455259\n",
      "Trained batch 884 batch loss 0.309656024 epoch total loss 0.30845663\n",
      "Trained batch 885 batch loss 0.309715778 epoch total loss 0.30845806\n",
      "Trained batch 886 batch loss 0.287281871 epoch total loss 0.308434159\n",
      "Trained batch 887 batch loss 0.271633089 epoch total loss 0.308392674\n",
      "Trained batch 888 batch loss 0.308903247 epoch total loss 0.30839327\n",
      "Trained batch 889 batch loss 0.284617454 epoch total loss 0.308366507\n",
      "Trained batch 890 batch loss 0.305420935 epoch total loss 0.308363199\n",
      "Trained batch 891 batch loss 0.319919109 epoch total loss 0.308376163\n",
      "Trained batch 892 batch loss 0.284570962 epoch total loss 0.30834946\n",
      "Trained batch 893 batch loss 0.287170291 epoch total loss 0.308325768\n",
      "Trained batch 894 batch loss 0.268131405 epoch total loss 0.308280796\n",
      "Trained batch 895 batch loss 0.300736636 epoch total loss 0.308272392\n",
      "Trained batch 896 batch loss 0.287754416 epoch total loss 0.308249474\n",
      "Trained batch 897 batch loss 0.278977245 epoch total loss 0.30821687\n",
      "Trained batch 898 batch loss 0.279634833 epoch total loss 0.308185\n",
      "Trained batch 899 batch loss 0.314062327 epoch total loss 0.308191568\n",
      "Trained batch 900 batch loss 0.289503068 epoch total loss 0.308170766\n",
      "Trained batch 901 batch loss 0.264909714 epoch total loss 0.308122784\n",
      "Trained batch 902 batch loss 0.314597219 epoch total loss 0.308129966\n",
      "Trained batch 903 batch loss 0.300865799 epoch total loss 0.30812192\n",
      "Trained batch 904 batch loss 0.315228283 epoch total loss 0.308129787\n",
      "Trained batch 905 batch loss 0.323143482 epoch total loss 0.308146358\n",
      "Trained batch 906 batch loss 0.308981448 epoch total loss 0.308147311\n",
      "Trained batch 907 batch loss 0.305000812 epoch total loss 0.308143824\n",
      "Trained batch 908 batch loss 0.291141 epoch total loss 0.308125108\n",
      "Trained batch 909 batch loss 0.313583851 epoch total loss 0.308131129\n",
      "Trained batch 910 batch loss 0.284177959 epoch total loss 0.308104783\n",
      "Trained batch 911 batch loss 0.28274408 epoch total loss 0.308076948\n",
      "Trained batch 912 batch loss 0.27759257 epoch total loss 0.30804354\n",
      "Trained batch 913 batch loss 0.273082614 epoch total loss 0.308005214\n",
      "Trained batch 914 batch loss 0.300907165 epoch total loss 0.307997465\n",
      "Trained batch 915 batch loss 0.28669 epoch total loss 0.30797416\n",
      "Trained batch 916 batch loss 0.312012136 epoch total loss 0.30797857\n",
      "Trained batch 917 batch loss 0.293202132 epoch total loss 0.307962477\n",
      "Trained batch 918 batch loss 0.277841449 epoch total loss 0.307929635\n",
      "Trained batch 919 batch loss 0.301824391 epoch total loss 0.307923\n",
      "Trained batch 920 batch loss 0.325109392 epoch total loss 0.307941675\n",
      "Trained batch 921 batch loss 0.283323377 epoch total loss 0.307914943\n",
      "Trained batch 922 batch loss 0.276009321 epoch total loss 0.307880342\n",
      "Trained batch 923 batch loss 0.296657503 epoch total loss 0.307868183\n",
      "Trained batch 924 batch loss 0.287105292 epoch total loss 0.307845712\n",
      "Trained batch 925 batch loss 0.29872337 epoch total loss 0.307835847\n",
      "Trained batch 926 batch loss 0.331193447 epoch total loss 0.30786109\n",
      "Trained batch 927 batch loss 0.3000108 epoch total loss 0.307852626\n",
      "Trained batch 928 batch loss 0.268439591 epoch total loss 0.307810158\n",
      "Trained batch 929 batch loss 0.274122089 epoch total loss 0.307773888\n",
      "Trained batch 930 batch loss 0.244630009 epoch total loss 0.307706\n",
      "Trained batch 931 batch loss 0.233134478 epoch total loss 0.30762586\n",
      "Trained batch 932 batch loss 0.250457674 epoch total loss 0.307564527\n",
      "Trained batch 933 batch loss 0.272918969 epoch total loss 0.307527393\n",
      "Trained batch 934 batch loss 0.319500059 epoch total loss 0.307540208\n",
      "Trained batch 935 batch loss 0.308618486 epoch total loss 0.30754137\n",
      "Trained batch 936 batch loss 0.311445355 epoch total loss 0.307545513\n",
      "Trained batch 937 batch loss 0.326646924 epoch total loss 0.307565928\n",
      "Trained batch 938 batch loss 0.326503545 epoch total loss 0.307586104\n",
      "Trained batch 939 batch loss 0.312346727 epoch total loss 0.3075912\n",
      "Trained batch 940 batch loss 0.335426509 epoch total loss 0.307620794\n",
      "Trained batch 941 batch loss 0.340246528 epoch total loss 0.307655454\n",
      "Trained batch 942 batch loss 0.340429515 epoch total loss 0.307690233\n",
      "Trained batch 943 batch loss 0.296829224 epoch total loss 0.307678729\n",
      "Trained batch 944 batch loss 0.325209796 epoch total loss 0.307697296\n",
      "Trained batch 945 batch loss 0.345374286 epoch total loss 0.307737172\n",
      "Trained batch 946 batch loss 0.301297128 epoch total loss 0.307730347\n",
      "Trained batch 947 batch loss 0.320597112 epoch total loss 0.307743937\n",
      "Trained batch 948 batch loss 0.31029886 epoch total loss 0.307746619\n",
      "Trained batch 949 batch loss 0.292101026 epoch total loss 0.307730168\n",
      "Trained batch 950 batch loss 0.330577314 epoch total loss 0.307754189\n",
      "Trained batch 951 batch loss 0.297449738 epoch total loss 0.307743371\n",
      "Trained batch 952 batch loss 0.321301043 epoch total loss 0.307757586\n",
      "Trained batch 953 batch loss 0.336660683 epoch total loss 0.307787925\n",
      "Trained batch 954 batch loss 0.299996197 epoch total loss 0.307779759\n",
      "Trained batch 955 batch loss 0.296383 epoch total loss 0.307767838\n",
      "Trained batch 956 batch loss 0.302811056 epoch total loss 0.307762653\n",
      "Trained batch 957 batch loss 0.334513 epoch total loss 0.307790607\n",
      "Trained batch 958 batch loss 0.309185117 epoch total loss 0.307792038\n",
      "Trained batch 959 batch loss 0.323501587 epoch total loss 0.307808429\n",
      "Trained batch 960 batch loss 0.361101955 epoch total loss 0.307863981\n",
      "Trained batch 961 batch loss 0.301906 epoch total loss 0.307857782\n",
      "Trained batch 962 batch loss 0.295854241 epoch total loss 0.307845294\n",
      "Trained batch 963 batch loss 0.284957141 epoch total loss 0.307821512\n",
      "Trained batch 964 batch loss 0.325279653 epoch total loss 0.307839632\n",
      "Trained batch 965 batch loss 0.34288913 epoch total loss 0.307875961\n",
      "Trained batch 966 batch loss 0.31073603 epoch total loss 0.307878911\n",
      "Trained batch 967 batch loss 0.320155144 epoch total loss 0.307891637\n",
      "Trained batch 968 batch loss 0.317822039 epoch total loss 0.307901859\n",
      "Trained batch 969 batch loss 0.305879 epoch total loss 0.307899773\n",
      "Trained batch 970 batch loss 0.311520278 epoch total loss 0.307903528\n",
      "Trained batch 971 batch loss 0.343526095 epoch total loss 0.307940215\n",
      "Trained batch 972 batch loss 0.338998973 epoch total loss 0.307972163\n",
      "Trained batch 973 batch loss 0.334461391 epoch total loss 0.307999402\n",
      "Trained batch 974 batch loss 0.286638796 epoch total loss 0.307977468\n",
      "Trained batch 975 batch loss 0.296122491 epoch total loss 0.307965308\n",
      "Trained batch 976 batch loss 0.285302162 epoch total loss 0.307942092\n",
      "Trained batch 977 batch loss 0.301891774 epoch total loss 0.307935894\n",
      "Trained batch 978 batch loss 0.329241663 epoch total loss 0.307957679\n",
      "Trained batch 979 batch loss 0.314500064 epoch total loss 0.307964385\n",
      "Trained batch 980 batch loss 0.313258231 epoch total loss 0.307969779\n",
      "Trained batch 981 batch loss 0.315180957 epoch total loss 0.30797714\n",
      "Trained batch 982 batch loss 0.296587735 epoch total loss 0.307965547\n",
      "Trained batch 983 batch loss 0.308212042 epoch total loss 0.307965785\n",
      "Trained batch 984 batch loss 0.287090421 epoch total loss 0.307944566\n",
      "Trained batch 985 batch loss 0.309469491 epoch total loss 0.307946116\n",
      "Trained batch 986 batch loss 0.255687714 epoch total loss 0.307893127\n",
      "Trained batch 987 batch loss 0.286579251 epoch total loss 0.307871521\n",
      "Trained batch 988 batch loss 0.304929256 epoch total loss 0.30786857\n",
      "Trained batch 989 batch loss 0.326403767 epoch total loss 0.307887316\n",
      "Trained batch 990 batch loss 0.351334184 epoch total loss 0.307931215\n",
      "Trained batch 991 batch loss 0.347965807 epoch total loss 0.307971597\n",
      "Trained batch 992 batch loss 0.344429523 epoch total loss 0.308008343\n",
      "Trained batch 993 batch loss 0.298739254 epoch total loss 0.307999\n",
      "Trained batch 994 batch loss 0.275168151 epoch total loss 0.307966\n",
      "Trained batch 995 batch loss 0.266843319 epoch total loss 0.307924658\n",
      "Trained batch 996 batch loss 0.242516056 epoch total loss 0.307859\n",
      "Trained batch 997 batch loss 0.2492553 epoch total loss 0.307800233\n",
      "Trained batch 998 batch loss 0.270828664 epoch total loss 0.307763189\n",
      "Trained batch 999 batch loss 0.324909955 epoch total loss 0.307780385\n",
      "Trained batch 1000 batch loss 0.325012833 epoch total loss 0.307797611\n",
      "Trained batch 1001 batch loss 0.279784262 epoch total loss 0.307769626\n",
      "Trained batch 1002 batch loss 0.294575334 epoch total loss 0.307756454\n",
      "Trained batch 1003 batch loss 0.296421051 epoch total loss 0.307745159\n",
      "Trained batch 1004 batch loss 0.301943839 epoch total loss 0.307739377\n",
      "Trained batch 1005 batch loss 0.297712475 epoch total loss 0.307729393\n",
      "Trained batch 1006 batch loss 0.294829547 epoch total loss 0.307716578\n",
      "Trained batch 1007 batch loss 0.308317721 epoch total loss 0.307717174\n",
      "Trained batch 1008 batch loss 0.349876761 epoch total loss 0.307759\n",
      "Trained batch 1009 batch loss 0.303220391 epoch total loss 0.307754487\n",
      "Trained batch 1010 batch loss 0.339349687 epoch total loss 0.307785779\n",
      "Trained batch 1011 batch loss 0.343405783 epoch total loss 0.307821035\n",
      "Trained batch 1012 batch loss 0.331982046 epoch total loss 0.307844907\n",
      "Trained batch 1013 batch loss 0.309506536 epoch total loss 0.307846546\n",
      "Trained batch 1014 batch loss 0.281832695 epoch total loss 0.307820886\n",
      "Trained batch 1015 batch loss 0.31431067 epoch total loss 0.307827264\n",
      "Trained batch 1016 batch loss 0.298598766 epoch total loss 0.307818174\n",
      "Trained batch 1017 batch loss 0.265693486 epoch total loss 0.307776749\n",
      "Trained batch 1018 batch loss 0.292062193 epoch total loss 0.307761282\n",
      "Trained batch 1019 batch loss 0.281130493 epoch total loss 0.307735145\n",
      "Trained batch 1020 batch loss 0.297911048 epoch total loss 0.307725519\n",
      "Trained batch 1021 batch loss 0.298172235 epoch total loss 0.307716191\n",
      "Trained batch 1022 batch loss 0.253076017 epoch total loss 0.307662725\n",
      "Trained batch 1023 batch loss 0.265338242 epoch total loss 0.30762136\n",
      "Trained batch 1024 batch loss 0.301443696 epoch total loss 0.30761534\n",
      "Trained batch 1025 batch loss 0.295905411 epoch total loss 0.307603896\n",
      "Trained batch 1026 batch loss 0.287424713 epoch total loss 0.307584226\n",
      "Trained batch 1027 batch loss 0.32203567 epoch total loss 0.307598293\n",
      "Trained batch 1028 batch loss 0.335126102 epoch total loss 0.307625055\n",
      "Trained batch 1029 batch loss 0.344693542 epoch total loss 0.307661086\n",
      "Trained batch 1030 batch loss 0.293764293 epoch total loss 0.307647586\n",
      "Trained batch 1031 batch loss 0.29533866 epoch total loss 0.307635665\n",
      "Trained batch 1032 batch loss 0.277134329 epoch total loss 0.307606101\n",
      "Trained batch 1033 batch loss 0.322936416 epoch total loss 0.307620943\n",
      "Trained batch 1034 batch loss 0.283298016 epoch total loss 0.307597399\n",
      "Trained batch 1035 batch loss 0.247811615 epoch total loss 0.307539642\n",
      "Trained batch 1036 batch loss 0.224658877 epoch total loss 0.307459652\n",
      "Trained batch 1037 batch loss 0.292836756 epoch total loss 0.307445556\n",
      "Trained batch 1038 batch loss 0.297379911 epoch total loss 0.30743587\n",
      "Trained batch 1039 batch loss 0.354648411 epoch total loss 0.307481319\n",
      "Trained batch 1040 batch loss 0.326633871 epoch total loss 0.307499737\n",
      "Trained batch 1041 batch loss 0.319137931 epoch total loss 0.307510912\n",
      "Trained batch 1042 batch loss 0.297974527 epoch total loss 0.307501763\n",
      "Trained batch 1043 batch loss 0.286854684 epoch total loss 0.307481974\n",
      "Trained batch 1044 batch loss 0.286686659 epoch total loss 0.307462066\n",
      "Trained batch 1045 batch loss 0.31208849 epoch total loss 0.307466507\n",
      "Trained batch 1046 batch loss 0.320330054 epoch total loss 0.307478815\n",
      "Trained batch 1047 batch loss 0.333806813 epoch total loss 0.307503939\n",
      "Trained batch 1048 batch loss 0.321440965 epoch total loss 0.30751726\n",
      "Trained batch 1049 batch loss 0.34526363 epoch total loss 0.307553232\n",
      "Trained batch 1050 batch loss 0.347220838 epoch total loss 0.307591021\n",
      "Trained batch 1051 batch loss 0.358777344 epoch total loss 0.307639718\n",
      "Trained batch 1052 batch loss 0.31816572 epoch total loss 0.307649732\n",
      "Trained batch 1053 batch loss 0.303537667 epoch total loss 0.307645828\n",
      "Trained batch 1054 batch loss 0.326676 epoch total loss 0.307663888\n",
      "Trained batch 1055 batch loss 0.314391851 epoch total loss 0.307670265\n",
      "Trained batch 1056 batch loss 0.314785451 epoch total loss 0.307677\n",
      "Trained batch 1057 batch loss 0.288141429 epoch total loss 0.307658523\n",
      "Trained batch 1058 batch loss 0.28778407 epoch total loss 0.307639748\n",
      "Trained batch 1059 batch loss 0.279698968 epoch total loss 0.307613343\n",
      "Trained batch 1060 batch loss 0.262877882 epoch total loss 0.307571143\n",
      "Trained batch 1061 batch loss 0.303263098 epoch total loss 0.30756709\n",
      "Trained batch 1062 batch loss 0.298494279 epoch total loss 0.307558537\n",
      "Trained batch 1063 batch loss 0.331546545 epoch total loss 0.307581097\n",
      "Trained batch 1064 batch loss 0.294775546 epoch total loss 0.307569057\n",
      "Trained batch 1065 batch loss 0.329948276 epoch total loss 0.307590067\n",
      "Trained batch 1066 batch loss 0.322123528 epoch total loss 0.307603717\n",
      "Trained batch 1067 batch loss 0.312081903 epoch total loss 0.307607889\n",
      "Trained batch 1068 batch loss 0.28813231 epoch total loss 0.30758968\n",
      "Trained batch 1069 batch loss 0.288396031 epoch total loss 0.307571709\n",
      "Trained batch 1070 batch loss 0.328602791 epoch total loss 0.307591379\n",
      "Trained batch 1071 batch loss 0.296839833 epoch total loss 0.307581335\n",
      "Trained batch 1072 batch loss 0.297941208 epoch total loss 0.307572365\n",
      "Trained batch 1073 batch loss 0.330499738 epoch total loss 0.307593733\n",
      "Trained batch 1074 batch loss 0.274524689 epoch total loss 0.307562947\n",
      "Trained batch 1075 batch loss 0.320489436 epoch total loss 0.307575\n",
      "Trained batch 1076 batch loss 0.272816479 epoch total loss 0.307542682\n",
      "Trained batch 1077 batch loss 0.300875872 epoch total loss 0.307536483\n",
      "Trained batch 1078 batch loss 0.317613751 epoch total loss 0.307545841\n",
      "Trained batch 1079 batch loss 0.302955925 epoch total loss 0.307541579\n",
      "Trained batch 1080 batch loss 0.32097736 epoch total loss 0.307554036\n",
      "Trained batch 1081 batch loss 0.30504331 epoch total loss 0.307551712\n",
      "Trained batch 1082 batch loss 0.287350804 epoch total loss 0.307533056\n",
      "Trained batch 1083 batch loss 0.288332224 epoch total loss 0.307515323\n",
      "Trained batch 1084 batch loss 0.288042724 epoch total loss 0.307497382\n",
      "Trained batch 1085 batch loss 0.293287784 epoch total loss 0.307484269\n",
      "Trained batch 1086 batch loss 0.338115513 epoch total loss 0.307512462\n",
      "Trained batch 1087 batch loss 0.335497588 epoch total loss 0.307538211\n",
      "Trained batch 1088 batch loss 0.323621035 epoch total loss 0.307553\n",
      "Trained batch 1089 batch loss 0.337678015 epoch total loss 0.30758065\n",
      "Trained batch 1090 batch loss 0.321314 epoch total loss 0.307593256\n",
      "Trained batch 1091 batch loss 0.324198842 epoch total loss 0.307608455\n",
      "Trained batch 1092 batch loss 0.287877411 epoch total loss 0.307590395\n",
      "Trained batch 1093 batch loss 0.293493956 epoch total loss 0.307577491\n",
      "Trained batch 1094 batch loss 0.315134645 epoch total loss 0.307584375\n",
      "Trained batch 1095 batch loss 0.297504425 epoch total loss 0.307575196\n",
      "Trained batch 1096 batch loss 0.312369049 epoch total loss 0.307579577\n",
      "Trained batch 1097 batch loss 0.306506455 epoch total loss 0.307578593\n",
      "Trained batch 1098 batch loss 0.288577437 epoch total loss 0.307561308\n",
      "Trained batch 1099 batch loss 0.276264668 epoch total loss 0.307532817\n",
      "Trained batch 1100 batch loss 0.264816135 epoch total loss 0.307493985\n",
      "Trained batch 1101 batch loss 0.263692886 epoch total loss 0.307454199\n",
      "Trained batch 1102 batch loss 0.304491967 epoch total loss 0.307451516\n",
      "Trained batch 1103 batch loss 0.296316564 epoch total loss 0.307441443\n",
      "Trained batch 1104 batch loss 0.316010654 epoch total loss 0.307449192\n",
      "Trained batch 1105 batch loss 0.32412535 epoch total loss 0.307464302\n",
      "Trained batch 1106 batch loss 0.329556614 epoch total loss 0.307484269\n",
      "Trained batch 1107 batch loss 0.311892 epoch total loss 0.307488263\n",
      "Trained batch 1108 batch loss 0.310664594 epoch total loss 0.307491124\n",
      "Trained batch 1109 batch loss 0.330939293 epoch total loss 0.307512254\n",
      "Trained batch 1110 batch loss 0.319271535 epoch total loss 0.307522863\n",
      "Trained batch 1111 batch loss 0.315918028 epoch total loss 0.307530403\n",
      "Trained batch 1112 batch loss 0.321697 epoch total loss 0.307543159\n",
      "Trained batch 1113 batch loss 0.313096821 epoch total loss 0.307548136\n",
      "Trained batch 1114 batch loss 0.317367345 epoch total loss 0.307556957\n",
      "Trained batch 1115 batch loss 0.298315853 epoch total loss 0.307548642\n",
      "Trained batch 1116 batch loss 0.301513433 epoch total loss 0.307543248\n",
      "Trained batch 1117 batch loss 0.290448129 epoch total loss 0.30752793\n",
      "Trained batch 1118 batch loss 0.299554616 epoch total loss 0.307520807\n",
      "Trained batch 1119 batch loss 0.284227252 epoch total loss 0.3075\n",
      "Trained batch 1120 batch loss 0.293608069 epoch total loss 0.307487607\n",
      "Trained batch 1121 batch loss 0.271642506 epoch total loss 0.307455629\n",
      "Trained batch 1122 batch loss 0.265787661 epoch total loss 0.307418466\n",
      "Trained batch 1123 batch loss 0.296791136 epoch total loss 0.307409\n",
      "Trained batch 1124 batch loss 0.319187015 epoch total loss 0.307419479\n",
      "Trained batch 1125 batch loss 0.315143079 epoch total loss 0.307426363\n",
      "Trained batch 1126 batch loss 0.311465 epoch total loss 0.30742994\n",
      "Trained batch 1127 batch loss 0.320621252 epoch total loss 0.307441652\n",
      "Trained batch 1128 batch loss 0.301386863 epoch total loss 0.307436287\n",
      "Trained batch 1129 batch loss 0.276425481 epoch total loss 0.30740881\n",
      "Trained batch 1130 batch loss 0.294765711 epoch total loss 0.307397634\n",
      "Trained batch 1131 batch loss 0.301589251 epoch total loss 0.307392478\n",
      "Trained batch 1132 batch loss 0.292211056 epoch total loss 0.307379067\n",
      "Trained batch 1133 batch loss 0.289725572 epoch total loss 0.30736348\n",
      "Trained batch 1134 batch loss 0.289205283 epoch total loss 0.307347476\n",
      "Trained batch 1135 batch loss 0.309774667 epoch total loss 0.307349622\n",
      "Trained batch 1136 batch loss 0.308059275 epoch total loss 0.307350248\n",
      "Trained batch 1137 batch loss 0.31885159 epoch total loss 0.307360351\n",
      "Trained batch 1138 batch loss 0.316485703 epoch total loss 0.307368368\n",
      "Trained batch 1139 batch loss 0.284745246 epoch total loss 0.30734852\n",
      "Trained batch 1140 batch loss 0.297731489 epoch total loss 0.307340086\n",
      "Trained batch 1141 batch loss 0.287528127 epoch total loss 0.307322741\n",
      "Trained batch 1142 batch loss 0.29755345 epoch total loss 0.307314187\n",
      "Trained batch 1143 batch loss 0.327079207 epoch total loss 0.307331473\n",
      "Trained batch 1144 batch loss 0.324806213 epoch total loss 0.307346731\n",
      "Trained batch 1145 batch loss 0.340072811 epoch total loss 0.307375342\n",
      "Trained batch 1146 batch loss 0.328273594 epoch total loss 0.307393581\n",
      "Trained batch 1147 batch loss 0.348412037 epoch total loss 0.307429343\n",
      "Trained batch 1148 batch loss 0.307463109 epoch total loss 0.307429373\n",
      "Trained batch 1149 batch loss 0.280835032 epoch total loss 0.307406217\n",
      "Trained batch 1150 batch loss 0.300594807 epoch total loss 0.307400286\n",
      "Trained batch 1151 batch loss 0.301314503 epoch total loss 0.307395\n",
      "Trained batch 1152 batch loss 0.343299478 epoch total loss 0.307426155\n",
      "Trained batch 1153 batch loss 0.326876104 epoch total loss 0.307443023\n",
      "Trained batch 1154 batch loss 0.344495952 epoch total loss 0.30747512\n",
      "Trained batch 1155 batch loss 0.324003905 epoch total loss 0.307489425\n",
      "Trained batch 1156 batch loss 0.30280894 epoch total loss 0.307485372\n",
      "Trained batch 1157 batch loss 0.316559583 epoch total loss 0.30749321\n",
      "Trained batch 1158 batch loss 0.290442258 epoch total loss 0.307478487\n",
      "Trained batch 1159 batch loss 0.331215203 epoch total loss 0.307498962\n",
      "Trained batch 1160 batch loss 0.289401829 epoch total loss 0.307483345\n",
      "Trained batch 1161 batch loss 0.292689741 epoch total loss 0.30747062\n",
      "Trained batch 1162 batch loss 0.266436577 epoch total loss 0.307435304\n",
      "Trained batch 1163 batch loss 0.273268312 epoch total loss 0.307405919\n",
      "Trained batch 1164 batch loss 0.245989949 epoch total loss 0.307353169\n",
      "Trained batch 1165 batch loss 0.324665189 epoch total loss 0.30736804\n",
      "Trained batch 1166 batch loss 0.369550228 epoch total loss 0.307421356\n",
      "Trained batch 1167 batch loss 0.362793773 epoch total loss 0.307468802\n",
      "Trained batch 1168 batch loss 0.357786119 epoch total loss 0.307511896\n",
      "Trained batch 1169 batch loss 0.324629188 epoch total loss 0.307526529\n",
      "Trained batch 1170 batch loss 0.327453494 epoch total loss 0.307543546\n",
      "Trained batch 1171 batch loss 0.348486125 epoch total loss 0.307578504\n",
      "Trained batch 1172 batch loss 0.322634608 epoch total loss 0.307591349\n",
      "Trained batch 1173 batch loss 0.332244456 epoch total loss 0.30761236\n",
      "Trained batch 1174 batch loss 0.363003492 epoch total loss 0.307659566\n",
      "Trained batch 1175 batch loss 0.299981117 epoch total loss 0.30765304\n",
      "Trained batch 1176 batch loss 0.296041101 epoch total loss 0.307643175\n",
      "Trained batch 1177 batch loss 0.258806318 epoch total loss 0.30760169\n",
      "Trained batch 1178 batch loss 0.306377441 epoch total loss 0.307600647\n",
      "Trained batch 1179 batch loss 0.29952544 epoch total loss 0.307593793\n",
      "Trained batch 1180 batch loss 0.336897969 epoch total loss 0.307618618\n",
      "Trained batch 1181 batch loss 0.309962451 epoch total loss 0.307620585\n",
      "Trained batch 1182 batch loss 0.315764904 epoch total loss 0.307627499\n",
      "Trained batch 1183 batch loss 0.293629408 epoch total loss 0.307615668\n",
      "Trained batch 1184 batch loss 0.318117321 epoch total loss 0.307624519\n",
      "Trained batch 1185 batch loss 0.310658664 epoch total loss 0.307627112\n",
      "Trained batch 1186 batch loss 0.288834542 epoch total loss 0.307611257\n",
      "Trained batch 1187 batch loss 0.342112571 epoch total loss 0.307640314\n",
      "Trained batch 1188 batch loss 0.337459177 epoch total loss 0.307665437\n",
      "Trained batch 1189 batch loss 0.292842329 epoch total loss 0.30765298\n",
      "Trained batch 1190 batch loss 0.268712848 epoch total loss 0.307620227\n",
      "Trained batch 1191 batch loss 0.280996203 epoch total loss 0.307597905\n",
      "Trained batch 1192 batch loss 0.310993433 epoch total loss 0.307600737\n",
      "Trained batch 1193 batch loss 0.324901551 epoch total loss 0.30761525\n",
      "Trained batch 1194 batch loss 0.327843547 epoch total loss 0.307632178\n",
      "Trained batch 1195 batch loss 0.298858702 epoch total loss 0.307624847\n",
      "Trained batch 1196 batch loss 0.299207866 epoch total loss 0.307617813\n",
      "Trained batch 1197 batch loss 0.28211087 epoch total loss 0.307596475\n",
      "Trained batch 1198 batch loss 0.279865742 epoch total loss 0.307573348\n",
      "Trained batch 1199 batch loss 0.296521246 epoch total loss 0.30756411\n",
      "Trained batch 1200 batch loss 0.307022333 epoch total loss 0.307563692\n",
      "Trained batch 1201 batch loss 0.321760118 epoch total loss 0.307575494\n",
      "Trained batch 1202 batch loss 0.317099154 epoch total loss 0.307583421\n",
      "Trained batch 1203 batch loss 0.299691141 epoch total loss 0.307576865\n",
      "Trained batch 1204 batch loss 0.305446982 epoch total loss 0.307575077\n",
      "Trained batch 1205 batch loss 0.303043097 epoch total loss 0.307571322\n",
      "Trained batch 1206 batch loss 0.313929647 epoch total loss 0.307576597\n",
      "Trained batch 1207 batch loss 0.323175132 epoch total loss 0.307589531\n",
      "Trained batch 1208 batch loss 0.305869251 epoch total loss 0.3075881\n",
      "Trained batch 1209 batch loss 0.306119919 epoch total loss 0.307586908\n",
      "Trained batch 1210 batch loss 0.265612632 epoch total loss 0.307552218\n",
      "Trained batch 1211 batch loss 0.28986007 epoch total loss 0.307537615\n",
      "Trained batch 1212 batch loss 0.29627952 epoch total loss 0.307528317\n",
      "Trained batch 1213 batch loss 0.318404257 epoch total loss 0.307537258\n",
      "Trained batch 1214 batch loss 0.298118174 epoch total loss 0.307529509\n",
      "Trained batch 1215 batch loss 0.313115805 epoch total loss 0.307534099\n",
      "Trained batch 1216 batch loss 0.318162948 epoch total loss 0.307542861\n",
      "Trained batch 1217 batch loss 0.29103294 epoch total loss 0.3075293\n",
      "Trained batch 1218 batch loss 0.318858236 epoch total loss 0.307538599\n",
      "Trained batch 1219 batch loss 0.33031714 epoch total loss 0.307557285\n",
      "Trained batch 1220 batch loss 0.302374125 epoch total loss 0.307553023\n",
      "Trained batch 1221 batch loss 0.303479284 epoch total loss 0.307549685\n",
      "Trained batch 1222 batch loss 0.318085968 epoch total loss 0.307558298\n",
      "Trained batch 1223 batch loss 0.322609872 epoch total loss 0.307570606\n",
      "Trained batch 1224 batch loss 0.290003181 epoch total loss 0.307556242\n",
      "Trained batch 1225 batch loss 0.305472076 epoch total loss 0.307554573\n",
      "Trained batch 1226 batch loss 0.308841676 epoch total loss 0.307555616\n",
      "Trained batch 1227 batch loss 0.314324856 epoch total loss 0.307561129\n",
      "Trained batch 1228 batch loss 0.315705717 epoch total loss 0.307567745\n",
      "Trained batch 1229 batch loss 0.264992893 epoch total loss 0.307533115\n",
      "Trained batch 1230 batch loss 0.272199422 epoch total loss 0.307504386\n",
      "Trained batch 1231 batch loss 0.260694802 epoch total loss 0.307466328\n",
      "Trained batch 1232 batch loss 0.271123976 epoch total loss 0.307436824\n",
      "Trained batch 1233 batch loss 0.261810124 epoch total loss 0.307399839\n",
      "Trained batch 1234 batch loss 0.290426522 epoch total loss 0.30738607\n",
      "Trained batch 1235 batch loss 0.264602453 epoch total loss 0.30735141\n",
      "Trained batch 1236 batch loss 0.273856819 epoch total loss 0.30732432\n",
      "Trained batch 1237 batch loss 0.266726613 epoch total loss 0.307291508\n",
      "Trained batch 1238 batch loss 0.259568483 epoch total loss 0.307252973\n",
      "Trained batch 1239 batch loss 0.288692415 epoch total loss 0.307237983\n",
      "Trained batch 1240 batch loss 0.28700158 epoch total loss 0.307221651\n",
      "Trained batch 1241 batch loss 0.298416257 epoch total loss 0.307214588\n",
      "Trained batch 1242 batch loss 0.308776677 epoch total loss 0.30721584\n",
      "Trained batch 1243 batch loss 0.305681348 epoch total loss 0.307214618\n",
      "Trained batch 1244 batch loss 0.298468471 epoch total loss 0.307207584\n",
      "Trained batch 1245 batch loss 0.340446264 epoch total loss 0.307234287\n",
      "Trained batch 1246 batch loss 0.319786221 epoch total loss 0.30724436\n",
      "Trained batch 1247 batch loss 0.332450867 epoch total loss 0.307264596\n",
      "Trained batch 1248 batch loss 0.299965262 epoch total loss 0.307258725\n",
      "Trained batch 1249 batch loss 0.283128 epoch total loss 0.307239413\n",
      "Trained batch 1250 batch loss 0.278216958 epoch total loss 0.307216197\n",
      "Trained batch 1251 batch loss 0.288601547 epoch total loss 0.307201326\n",
      "Trained batch 1252 batch loss 0.317557693 epoch total loss 0.307209611\n",
      "Trained batch 1253 batch loss 0.317563683 epoch total loss 0.307217866\n",
      "Trained batch 1254 batch loss 0.33762145 epoch total loss 0.307242125\n",
      "Trained batch 1255 batch loss 0.323688239 epoch total loss 0.307255238\n",
      "Trained batch 1256 batch loss 0.27358 epoch total loss 0.307228416\n",
      "Trained batch 1257 batch loss 0.299654126 epoch total loss 0.307222396\n",
      "Trained batch 1258 batch loss 0.286324561 epoch total loss 0.307205796\n",
      "Trained batch 1259 batch loss 0.277649 epoch total loss 0.307182312\n",
      "Trained batch 1260 batch loss 0.271133542 epoch total loss 0.307153702\n",
      "Trained batch 1261 batch loss 0.271365434 epoch total loss 0.30712533\n",
      "Trained batch 1262 batch loss 0.276701897 epoch total loss 0.30710122\n",
      "Trained batch 1263 batch loss 0.323114455 epoch total loss 0.307113916\n",
      "Trained batch 1264 batch loss 0.324272454 epoch total loss 0.307127476\n",
      "Trained batch 1265 batch loss 0.315318167 epoch total loss 0.307133943\n",
      "Trained batch 1266 batch loss 0.304399848 epoch total loss 0.307131797\n",
      "Trained batch 1267 batch loss 0.318398923 epoch total loss 0.307140678\n",
      "Trained batch 1268 batch loss 0.317896068 epoch total loss 0.307149172\n",
      "Trained batch 1269 batch loss 0.291937292 epoch total loss 0.307137191\n",
      "Trained batch 1270 batch loss 0.28796804 epoch total loss 0.307122082\n",
      "Trained batch 1271 batch loss 0.319275 epoch total loss 0.307131648\n",
      "Trained batch 1272 batch loss 0.322403163 epoch total loss 0.307143658\n",
      "Trained batch 1273 batch loss 0.327010125 epoch total loss 0.307159245\n",
      "Trained batch 1274 batch loss 0.294590354 epoch total loss 0.30714938\n",
      "Trained batch 1275 batch loss 0.322238952 epoch total loss 0.307161212\n",
      "Trained batch 1276 batch loss 0.335215479 epoch total loss 0.307183206\n",
      "Trained batch 1277 batch loss 0.33628279 epoch total loss 0.307205975\n",
      "Trained batch 1278 batch loss 0.328782141 epoch total loss 0.307222873\n",
      "Trained batch 1279 batch loss 0.298518091 epoch total loss 0.307216078\n",
      "Trained batch 1280 batch loss 0.344868392 epoch total loss 0.307245493\n",
      "Trained batch 1281 batch loss 0.28952837 epoch total loss 0.307231665\n",
      "Trained batch 1282 batch loss 0.302119732 epoch total loss 0.307227671\n",
      "Trained batch 1283 batch loss 0.313369393 epoch total loss 0.30723244\n",
      "Trained batch 1284 batch loss 0.313706398 epoch total loss 0.307237506\n",
      "Trained batch 1285 batch loss 0.295181632 epoch total loss 0.307228118\n",
      "Trained batch 1286 batch loss 0.31329 epoch total loss 0.307232857\n",
      "Trained batch 1287 batch loss 0.311014622 epoch total loss 0.307235777\n",
      "Trained batch 1288 batch loss 0.320664793 epoch total loss 0.307246208\n",
      "Trained batch 1289 batch loss 0.333720595 epoch total loss 0.307266742\n",
      "Trained batch 1290 batch loss 0.335943639 epoch total loss 0.307288975\n",
      "Trained batch 1291 batch loss 0.321703106 epoch total loss 0.30730015\n",
      "Trained batch 1292 batch loss 0.320913553 epoch total loss 0.3073107\n",
      "Trained batch 1293 batch loss 0.303874493 epoch total loss 0.307308018\n",
      "Trained batch 1294 batch loss 0.29891935 epoch total loss 0.307301551\n",
      "Trained batch 1295 batch loss 0.313782573 epoch total loss 0.307306558\n",
      "Trained batch 1296 batch loss 0.302878976 epoch total loss 0.307303131\n",
      "Trained batch 1297 batch loss 0.305188119 epoch total loss 0.307301491\n",
      "Trained batch 1298 batch loss 0.313587338 epoch total loss 0.307306349\n",
      "Trained batch 1299 batch loss 0.30672285 epoch total loss 0.307305902\n",
      "Trained batch 1300 batch loss 0.289049089 epoch total loss 0.307291865\n",
      "Trained batch 1301 batch loss 0.323536098 epoch total loss 0.307304353\n",
      "Trained batch 1302 batch loss 0.351715505 epoch total loss 0.307338476\n",
      "Trained batch 1303 batch loss 0.3196069 epoch total loss 0.307347894\n",
      "Trained batch 1304 batch loss 0.274188787 epoch total loss 0.307322472\n",
      "Trained batch 1305 batch loss 0.248799607 epoch total loss 0.30727765\n",
      "Trained batch 1306 batch loss 0.302170783 epoch total loss 0.307273746\n",
      "Trained batch 1307 batch loss 0.322616875 epoch total loss 0.307285488\n",
      "Trained batch 1308 batch loss 0.322512716 epoch total loss 0.30729714\n",
      "Trained batch 1309 batch loss 0.304840058 epoch total loss 0.307295263\n",
      "Trained batch 1310 batch loss 0.317263246 epoch total loss 0.307302862\n",
      "Trained batch 1311 batch loss 0.31136328 epoch total loss 0.307305962\n",
      "Trained batch 1312 batch loss 0.304929823 epoch total loss 0.307304144\n",
      "Trained batch 1313 batch loss 0.297303796 epoch total loss 0.307296544\n",
      "Trained batch 1314 batch loss 0.305765629 epoch total loss 0.307295352\n",
      "Trained batch 1315 batch loss 0.301052094 epoch total loss 0.307290614\n",
      "Trained batch 1316 batch loss 0.291106641 epoch total loss 0.307278305\n",
      "Trained batch 1317 batch loss 0.298485607 epoch total loss 0.307271659\n",
      "Trained batch 1318 batch loss 0.306635082 epoch total loss 0.307271153\n",
      "Trained batch 1319 batch loss 0.307205409 epoch total loss 0.307271123\n",
      "Trained batch 1320 batch loss 0.313470066 epoch total loss 0.307275832\n",
      "Trained batch 1321 batch loss 0.316449583 epoch total loss 0.307282776\n",
      "Trained batch 1322 batch loss 0.307957 epoch total loss 0.307283282\n",
      "Trained batch 1323 batch loss 0.336105376 epoch total loss 0.307305068\n",
      "Trained batch 1324 batch loss 0.329039723 epoch total loss 0.307321489\n",
      "Trained batch 1325 batch loss 0.32635659 epoch total loss 0.307335854\n",
      "Trained batch 1326 batch loss 0.29605931 epoch total loss 0.30732733\n",
      "Trained batch 1327 batch loss 0.281947404 epoch total loss 0.307308227\n",
      "Trained batch 1328 batch loss 0.279495686 epoch total loss 0.307287276\n",
      "Trained batch 1329 batch loss 0.288082331 epoch total loss 0.307272851\n",
      "Trained batch 1330 batch loss 0.289393306 epoch total loss 0.307259411\n",
      "Trained batch 1331 batch loss 0.283007622 epoch total loss 0.307241172\n",
      "Trained batch 1332 batch loss 0.312269956 epoch total loss 0.307244956\n",
      "Trained batch 1333 batch loss 0.304625511 epoch total loss 0.307243\n",
      "Trained batch 1334 batch loss 0.290892929 epoch total loss 0.307230741\n",
      "Trained batch 1335 batch loss 0.29107672 epoch total loss 0.307218641\n",
      "Trained batch 1336 batch loss 0.287522644 epoch total loss 0.307203889\n",
      "Trained batch 1337 batch loss 0.299570143 epoch total loss 0.307198167\n",
      "Trained batch 1338 batch loss 0.270815492 epoch total loss 0.307171\n",
      "Trained batch 1339 batch loss 0.307075143 epoch total loss 0.307170898\n",
      "Trained batch 1340 batch loss 0.286051363 epoch total loss 0.307155132\n",
      "Trained batch 1341 batch loss 0.316972584 epoch total loss 0.307162464\n",
      "Trained batch 1342 batch loss 0.286890447 epoch total loss 0.307147384\n",
      "Trained batch 1343 batch loss 0.289245129 epoch total loss 0.307134032\n",
      "Trained batch 1344 batch loss 0.283391327 epoch total loss 0.307116359\n",
      "Trained batch 1345 batch loss 0.292702496 epoch total loss 0.30710566\n",
      "Trained batch 1346 batch loss 0.271476865 epoch total loss 0.307079196\n",
      "Trained batch 1347 batch loss 0.326579571 epoch total loss 0.30709365\n",
      "Trained batch 1348 batch loss 0.331322193 epoch total loss 0.307111621\n",
      "Trained batch 1349 batch loss 0.326132834 epoch total loss 0.307125747\n",
      "Trained batch 1350 batch loss 0.302932382 epoch total loss 0.307122618\n",
      "Trained batch 1351 batch loss 0.317510366 epoch total loss 0.307130307\n",
      "Trained batch 1352 batch loss 0.315391809 epoch total loss 0.307136416\n",
      "Trained batch 1353 batch loss 0.330798417 epoch total loss 0.30715391\n",
      "Trained batch 1354 batch loss 0.3203848 epoch total loss 0.307163686\n",
      "Trained batch 1355 batch loss 0.310161412 epoch total loss 0.307165891\n",
      "Trained batch 1356 batch loss 0.317269087 epoch total loss 0.307173342\n",
      "Trained batch 1357 batch loss 0.317816138 epoch total loss 0.30718118\n",
      "Trained batch 1358 batch loss 0.326780975 epoch total loss 0.307195604\n",
      "Trained batch 1359 batch loss 0.313478142 epoch total loss 0.307200223\n",
      "Trained batch 1360 batch loss 0.310861796 epoch total loss 0.307202905\n",
      "Trained batch 1361 batch loss 0.33106333 epoch total loss 0.307220429\n",
      "Trained batch 1362 batch loss 0.310162932 epoch total loss 0.307222575\n",
      "Trained batch 1363 batch loss 0.301098168 epoch total loss 0.307218075\n",
      "Trained batch 1364 batch loss 0.315178633 epoch total loss 0.307223916\n",
      "Trained batch 1365 batch loss 0.306380808 epoch total loss 0.30722329\n",
      "Trained batch 1366 batch loss 0.25937891 epoch total loss 0.307188272\n",
      "Trained batch 1367 batch loss 0.279426306 epoch total loss 0.307167947\n",
      "Trained batch 1368 batch loss 0.263771206 epoch total loss 0.307136238\n",
      "Trained batch 1369 batch loss 0.275693387 epoch total loss 0.30711326\n",
      "Trained batch 1370 batch loss 0.294930816 epoch total loss 0.307104349\n",
      "Trained batch 1371 batch loss 0.301518053 epoch total loss 0.307100266\n",
      "Trained batch 1372 batch loss 0.326764286 epoch total loss 0.307114601\n",
      "Trained batch 1373 batch loss 0.311332136 epoch total loss 0.307117671\n",
      "Trained batch 1374 batch loss 0.298901379 epoch total loss 0.307111681\n",
      "Trained batch 1375 batch loss 0.263126224 epoch total loss 0.307079703\n",
      "Trained batch 1376 batch loss 0.249381915 epoch total loss 0.307037771\n",
      "Trained batch 1377 batch loss 0.281615436 epoch total loss 0.307019323\n",
      "Trained batch 1378 batch loss 0.307623148 epoch total loss 0.30701974\n",
      "Trained batch 1379 batch loss 0.305698723 epoch total loss 0.307018787\n",
      "Trained batch 1380 batch loss 0.326200843 epoch total loss 0.307032675\n",
      "Trained batch 1381 batch loss 0.325262368 epoch total loss 0.307045877\n",
      "Trained batch 1382 batch loss 0.273948461 epoch total loss 0.307021946\n",
      "Trained batch 1383 batch loss 0.313109577 epoch total loss 0.307026356\n",
      "Trained batch 1384 batch loss 0.300506949 epoch total loss 0.307021618\n",
      "Trained batch 1385 batch loss 0.309232444 epoch total loss 0.307023227\n",
      "Trained batch 1386 batch loss 0.312513232 epoch total loss 0.307027191\n",
      "Trained batch 1387 batch loss 0.307692111 epoch total loss 0.307027638\n",
      "Trained batch 1388 batch loss 0.290413022 epoch total loss 0.307015687\n",
      "Epoch 2 train loss 0.30701568722724915\n",
      "train time : 673.722740650177\n",
      "Validated batch 1 batch loss 0.332223266\n",
      "Validated batch 2 batch loss 0.316616148\n",
      "Validated batch 3 batch loss 0.293415368\n",
      "Validated batch 4 batch loss 0.303951114\n",
      "Validated batch 5 batch loss 0.274335861\n",
      "Validated batch 6 batch loss 0.319829822\n",
      "Validated batch 7 batch loss 0.318012625\n",
      "Validated batch 8 batch loss 0.27760607\n",
      "Validated batch 9 batch loss 0.319259048\n",
      "Validated batch 10 batch loss 0.300885975\n",
      "Validated batch 11 batch loss 0.318837523\n",
      "Validated batch 12 batch loss 0.30396387\n",
      "Validated batch 13 batch loss 0.30806905\n",
      "Validated batch 14 batch loss 0.285875916\n",
      "Validated batch 15 batch loss 0.285932541\n",
      "Validated batch 16 batch loss 0.313511699\n",
      "Validated batch 17 batch loss 0.29541418\n",
      "Validated batch 18 batch loss 0.312090039\n",
      "Validated batch 19 batch loss 0.326885253\n",
      "Validated batch 20 batch loss 0.365417659\n",
      "Validated batch 21 batch loss 0.319979161\n",
      "Validated batch 22 batch loss 0.30328998\n",
      "Validated batch 23 batch loss 0.27208519\n",
      "Validated batch 24 batch loss 0.308163524\n",
      "Validated batch 25 batch loss 0.30955714\n",
      "Validated batch 26 batch loss 0.293929935\n",
      "Validated batch 27 batch loss 0.282985091\n",
      "Validated batch 28 batch loss 0.293516457\n",
      "Validated batch 29 batch loss 0.314757556\n",
      "Validated batch 30 batch loss 0.294095159\n",
      "Validated batch 31 batch loss 0.292256504\n",
      "Validated batch 32 batch loss 0.311877698\n",
      "Validated batch 33 batch loss 0.314932019\n",
      "Validated batch 34 batch loss 0.28515172\n",
      "Validated batch 35 batch loss 0.269949377\n",
      "Validated batch 36 batch loss 0.302421272\n",
      "Validated batch 37 batch loss 0.310996652\n",
      "Validated batch 38 batch loss 0.341413558\n",
      "Validated batch 39 batch loss 0.329959\n",
      "Validated batch 40 batch loss 0.299975634\n",
      "Validated batch 41 batch loss 0.334057719\n",
      "Validated batch 42 batch loss 0.302228272\n",
      "Validated batch 43 batch loss 0.302941561\n",
      "Validated batch 44 batch loss 0.300200045\n",
      "Validated batch 45 batch loss 0.240618\n",
      "Validated batch 46 batch loss 0.305725157\n",
      "Validated batch 47 batch loss 0.276164979\n",
      "Validated batch 48 batch loss 0.29854998\n",
      "Validated batch 49 batch loss 0.311342388\n",
      "Validated batch 50 batch loss 0.287176639\n",
      "Validated batch 51 batch loss 0.318751752\n",
      "Validated batch 52 batch loss 0.353452623\n",
      "Validated batch 53 batch loss 0.271955281\n",
      "Validated batch 54 batch loss 0.31329006\n",
      "Validated batch 55 batch loss 0.297218025\n",
      "Validated batch 56 batch loss 0.322725505\n",
      "Validated batch 57 batch loss 0.314351171\n",
      "Validated batch 58 batch loss 0.264558464\n",
      "Validated batch 59 batch loss 0.262890041\n",
      "Validated batch 60 batch loss 0.292751729\n",
      "Validated batch 61 batch loss 0.303117484\n",
      "Validated batch 62 batch loss 0.287165165\n",
      "Validated batch 63 batch loss 0.301698536\n",
      "Validated batch 64 batch loss 0.267426968\n",
      "Validated batch 65 batch loss 0.316371679\n",
      "Validated batch 66 batch loss 0.329242826\n",
      "Validated batch 67 batch loss 0.319476157\n",
      "Validated batch 68 batch loss 0.303455025\n",
      "Validated batch 69 batch loss 0.277261257\n",
      "Validated batch 70 batch loss 0.294845402\n",
      "Validated batch 71 batch loss 0.285411149\n",
      "Validated batch 72 batch loss 0.296840727\n",
      "Validated batch 73 batch loss 0.275451481\n",
      "Validated batch 74 batch loss 0.297499895\n",
      "Validated batch 75 batch loss 0.320405126\n",
      "Validated batch 76 batch loss 0.297936261\n",
      "Validated batch 77 batch loss 0.328860492\n",
      "Validated batch 78 batch loss 0.319674402\n",
      "Validated batch 79 batch loss 0.31047225\n",
      "Validated batch 80 batch loss 0.301850319\n",
      "Validated batch 81 batch loss 0.335445076\n",
      "Validated batch 82 batch loss 0.318061531\n",
      "Validated batch 83 batch loss 0.316994846\n",
      "Validated batch 84 batch loss 0.323965311\n",
      "Validated batch 85 batch loss 0.329239666\n",
      "Validated batch 86 batch loss 0.324388325\n",
      "Validated batch 87 batch loss 0.280314386\n",
      "Validated batch 88 batch loss 0.304860234\n",
      "Validated batch 89 batch loss 0.3281838\n",
      "Validated batch 90 batch loss 0.315537751\n",
      "Validated batch 91 batch loss 0.298181\n",
      "Validated batch 92 batch loss 0.306846678\n",
      "Validated batch 93 batch loss 0.304196805\n",
      "Validated batch 94 batch loss 0.314833432\n",
      "Validated batch 95 batch loss 0.3000606\n",
      "Validated batch 96 batch loss 0.293132722\n",
      "Validated batch 97 batch loss 0.309757411\n",
      "Validated batch 98 batch loss 0.31777671\n",
      "Validated batch 99 batch loss 0.321541816\n",
      "Validated batch 100 batch loss 0.31919536\n",
      "Validated batch 101 batch loss 0.307535827\n",
      "Validated batch 102 batch loss 0.289961964\n",
      "Validated batch 103 batch loss 0.311941326\n",
      "Validated batch 104 batch loss 0.316967577\n",
      "Validated batch 105 batch loss 0.31153062\n",
      "Validated batch 106 batch loss 0.329411507\n",
      "Validated batch 107 batch loss 0.31903404\n",
      "Validated batch 108 batch loss 0.316553622\n",
      "Validated batch 109 batch loss 0.336897105\n",
      "Validated batch 110 batch loss 0.268441975\n",
      "Validated batch 111 batch loss 0.304619819\n",
      "Validated batch 112 batch loss 0.287184417\n",
      "Validated batch 113 batch loss 0.272291422\n",
      "Validated batch 114 batch loss 0.333796859\n",
      "Validated batch 115 batch loss 0.282583\n",
      "Validated batch 116 batch loss 0.292770952\n",
      "Validated batch 117 batch loss 0.293176115\n",
      "Validated batch 118 batch loss 0.284809828\n",
      "Validated batch 119 batch loss 0.273881555\n",
      "Validated batch 120 batch loss 0.280426681\n",
      "Validated batch 121 batch loss 0.333636671\n",
      "Validated batch 122 batch loss 0.275730908\n",
      "Validated batch 123 batch loss 0.271741301\n",
      "Validated batch 124 batch loss 0.284128278\n",
      "Validated batch 125 batch loss 0.300077558\n",
      "Validated batch 126 batch loss 0.273449779\n",
      "Validated batch 127 batch loss 0.302966535\n",
      "Validated batch 128 batch loss 0.286358804\n",
      "Validated batch 129 batch loss 0.279902458\n",
      "Validated batch 130 batch loss 0.30625543\n",
      "Validated batch 131 batch loss 0.334185421\n",
      "Validated batch 132 batch loss 0.277410716\n",
      "Validated batch 133 batch loss 0.328404\n",
      "Validated batch 134 batch loss 0.261527508\n",
      "Validated batch 135 batch loss 0.272619426\n",
      "Validated batch 136 batch loss 0.279667854\n",
      "Validated batch 137 batch loss 0.297901571\n",
      "Validated batch 138 batch loss 0.332320929\n",
      "Validated batch 139 batch loss 0.304589301\n",
      "Validated batch 140 batch loss 0.274398655\n",
      "Validated batch 141 batch loss 0.307762951\n",
      "Validated batch 142 batch loss 0.293650359\n",
      "Validated batch 143 batch loss 0.286283076\n",
      "Validated batch 144 batch loss 0.303085864\n",
      "Validated batch 145 batch loss 0.294231862\n",
      "Validated batch 146 batch loss 0.308251232\n",
      "Validated batch 147 batch loss 0.319325417\n",
      "Validated batch 148 batch loss 0.269394785\n",
      "Validated batch 149 batch loss 0.327001512\n",
      "Validated batch 150 batch loss 0.308979303\n",
      "Validated batch 151 batch loss 0.268499106\n",
      "Validated batch 152 batch loss 0.316930979\n",
      "Validated batch 153 batch loss 0.292428285\n",
      "Validated batch 154 batch loss 0.275170326\n",
      "Validated batch 155 batch loss 0.338053942\n",
      "Validated batch 156 batch loss 0.300984174\n",
      "Validated batch 157 batch loss 0.307930738\n",
      "Validated batch 158 batch loss 0.290599942\n",
      "Validated batch 159 batch loss 0.302930653\n",
      "Validated batch 160 batch loss 0.299291164\n",
      "Validated batch 161 batch loss 0.289153785\n",
      "Validated batch 162 batch loss 0.307833374\n",
      "Validated batch 163 batch loss 0.305959076\n",
      "Validated batch 164 batch loss 0.311766952\n",
      "Validated batch 165 batch loss 0.294936121\n",
      "Validated batch 166 batch loss 0.306081504\n",
      "Validated batch 167 batch loss 0.343696594\n",
      "Validated batch 168 batch loss 0.287238747\n",
      "Validated batch 169 batch loss 0.311150819\n",
      "Validated batch 170 batch loss 0.285417\n",
      "Validated batch 171 batch loss 0.323452711\n",
      "Validated batch 172 batch loss 0.305555522\n",
      "Validated batch 173 batch loss 0.295447171\n",
      "Validated batch 174 batch loss 0.300970823\n",
      "Validated batch 175 batch loss 0.324635178\n",
      "Validated batch 176 batch loss 0.306162268\n",
      "Validated batch 177 batch loss 0.317562133\n",
      "Validated batch 178 batch loss 0.314412087\n",
      "Validated batch 179 batch loss 0.302515715\n",
      "Validated batch 180 batch loss 0.288555682\n",
      "Validated batch 181 batch loss 0.31989485\n",
      "Validated batch 182 batch loss 0.290550917\n",
      "Validated batch 183 batch loss 0.291297197\n",
      "Validated batch 184 batch loss 0.314386904\n",
      "Validated batch 185 batch loss 0.348943472\n",
      "Epoch 2 val loss 0.30313944816589355\n",
      "Model /aiffel/aiffel/mpii/my_models/model-epoch-2-loss-0.3031.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "simplebaseline_model_file = train(epochs, learning_rate, num_heatmap, \n",
    "                                  batch_size, train_tfrecords, val_tfrecords, is_baseline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **예측 엔진 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourglass_WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "\n",
    "hourglass_model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "hourglass_model = hourglass_model.load_weights(hourglass_WEIGHTS_PATH)\n",
    "\n",
    "# # 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
    "# hourglass_model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "# hourglass_model = hourglass_model.load_weights(hourglass_model_file)\n",
    "\n",
    "simple_model = Simplebaseline(IMAGE_SHAPE)\n",
    "simple_model = simple_model.load_weights(simplebaseline_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap에서 최대값을 찾는 함수\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.float32)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생\n",
    "# 실제 계산에서는 3x3 필터를 이용해서 근사치 구함\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Hourglass Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4819/3864241112.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_image.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhourglass_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdraw_keypoints_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdraw_skeleton_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4819/1447485813.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, image_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(hourglass_model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[SimpleBaseline Model]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4819/1483416431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_image.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdraw_keypoints_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdraw_skeleton_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4819/1447485813.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, image_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(simple_model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
